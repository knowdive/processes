<?xml version="1.0" encoding="UTF-8"?><process version="9.2.001">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="9.2.001" expanded="true" name="Process">
    <parameter key="logverbosity" value="init"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read Knowledge" width="90" x="380" y="289">
        <parameter key="excel_file" value="/home/marco/Desktop/KLCis.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="prefix.true.polynominal.attribute"/>
          <parameter key="1" value="URI.true.polynominal.attribute"/>
          <parameter key="2" value="Title.true.polynominal.attribute"/>
          <parameter key="3" value="Languages.true.polynominal.attribute"/>
          <parameter key="4" value="VersionName.true.polynominal.attribute"/>
          <parameter key="5" value="VersionDate.true.polynominal.attribute"/>
          <parameter key="6" value="Link.true.polynominal.attribute"/>
          <parameter key="7" value="Folder.true.polynominal.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
      </operator>
      <operator activated="true" class="multiply" compatibility="9.2.001" expanded="true" height="103" name="Multiply" width="90" x="514" y="289"/>
      <operator activated="true" class="set_macros" compatibility="9.2.001" expanded="true" height="166" name="Set Macros" width="90" x="112" y="442">
        <list key="macros">
          <parameter key="folderDestination" value="~/Desktop/K-Files/"/>
          <parameter key="Predicates" value=" "/>
          <parameter key="fileName" value="/home/marco/Desktop/K-Files/LOV_Latest/Resources/nt/2019-05-16_owl_v2009-11-15_2009-11-15.nt"/>
          <parameter key="Formats" value="nt ttl rdf n3 json"/>
        </list>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="103" name="KnowledgeSerializer" width="90" x="313" y="799">
        <parameter key="script" value="# Import libraries&#10;import rdflib&#10;from rdflib import Graph, Namespace&#10;from rdflib.util import guess_format&#10;from rdflib.plugins.parsers.notation3 import N3Parser&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main():&#10;    # Get the name of the file to serialize&#10;    fileName = &quot;%{fileName}&quot;&#10;&#10;    # Try to create the graph to analyze the vocabulary&#10;    try:&#10;        g = Graph()&#10;        format_ = fileName.split(&quot;.&quot;)[-1]&#10;        if(format_ == &quot;txt&quot;):&#10;            format_ = fileName.split(&quot;.&quot;)[-2]&#10;        format_ = format_.split(&quot;?&quot;)[0]&#10;        result = g.parse(fileName, format=guess_format(format_))&#10;    except Exception as e:&#10;        # In case of an error during the graph's initiation, print the error&#10;        print(str(e) + &quot;\n&quot;)    &#10;&#10;    # Get the formats that will be used for serialization&#10;    strFormats = &quot;%{Formats}&quot;&#10;    dest = fileName.split(&quot;.&quot;)[0]&#10;&#10;    # Serialize the vocabulary in multiple formats&#10;    if(&quot;n3&quot; in strFormats.split()):&#10;        g.serialize(destination=dest + &quot;.n3&quot;, format=&quot;n3&quot;)&#10;    if(&quot;nt&quot; in strFormats.split()):&#10;        g.serialize(destination=dest + &quot;.nt&quot;, format=&quot;nt&quot;)&#10;    if(&quot;rdf&quot; in strFormats.split()):&#10;        g.serialize(destination=dest + &quot;.rdf&quot;, format=&quot;pretty-xml&quot;)&#10;    if(&quot;ttl&quot; in strFormats.split()):&#10;        g.serialize(destination=dest + &quot;.ttl&quot;, format=&quot;turtle&quot;)&#10;    if(&quot;json&quot; in strFormats.split()):&#10;        g.serialize(destination=dest + &quot;.json-ld&quot;, format=&quot;json-ld&quot;)"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="124" name="KnowledgeParser" width="90" x="849" y="340">
        <parameter key="script" value="# Import libraries&#10;import rdflib&#10;from rdflib import Graph, Namespace&#10;from rdflib.util import guess_format&#10;from rdflib.plugins.parsers.notation3 import N3Parser&#10;import pandas as pd&#10;import os&#10;import time&#10;&#10;# Class to handle the Excel file and relative indexes&#10;class ExcelFile:&#10;    def __init__(self, writer, num = 0):&#10;        self.writer = writer&#10;        self.index = 1&#10;        self.num = num + 1&#10;&#10;# Parse the given file and add its information to the file Excel given as third parameter&#10;def parse(vocabFolder, date, row, totalExcel, list_):&#10;    # Try to create the graph to analyze the vocabulary&#10;    try:&#10;        g = Graph()&#10;        format_ = row[&quot;Link&quot;].split(&quot;.&quot;)[-1]&#10;        if(format_ == &quot;txt&quot;):&#10;            format_ = row[&quot;Link&quot;].split(&quot;.&quot;)[-2]&#10;        format_ = format_.split(&quot;?&quot;)[0]&#10;        result = g.parse(row[&quot;Link&quot;], format=guess_format(format_), publicID=row[&quot;prefix&quot;])&#10;    except Exception as e:&#10;        # In case of an error during the graph's initiation, print the error and return an empty list&#10;        print(str(e) + &quot;\n&quot;)    &#10;        return totalExcel, list_, 0&#10;&#10;    # Serialize the vocabulary in multiple formats&#10;    serializedName = date + &quot;_&quot; + row[&quot;prefix&quot;] + &quot;_&quot; + row[&quot;VersionName&quot;] + &quot;_&quot; + row[&quot;VersionDate&quot;] + &quot;.&quot;&#10;    serializeVoc(vocabFolder, serializedName + &quot;n3&quot;, g, &quot;n3&quot;)&#10;    serializeVoc(vocabFolder, serializedName + &quot;nt&quot;, g, &quot;nt&quot;)&#10;    serializeVoc(vocabFolder, serializedName + &quot;rdf&quot;, g, &quot;pretty-xml&quot;)&#10;    serializeVoc(vocabFolder, serializedName + &quot;ttl&quot;, g, &quot;turtle&quot;)&#10;    serializeVoc(vocabFolder, serializedName + &quot;json-ld&quot;, g, &quot;json-ld&quot;)&#10;&#10;    # Get the totalExcel file and relative worksheets&#10;    totalWorkbook = totalExcel.writer.book&#10;    totalSheet = totalWorkbook.get_worksheet_by_name(&quot;Total Parsed Triples&quot;)&#10;&#10;    # Elaborate the fileName of the vocabulary&#10;    fileName = date + &quot;_Parsed_&quot; + row[&quot;prefix&quot;] + &quot;_&quot; + row[&quot;VersionName&quot;] + &quot;_&quot; + row[&quot;VersionDate&quot;] + &quot;_&quot;&#10;    # Create a Pandas Excel writer using XlsxWriter as the engine.&#10;    singleExcel, singleWorkbook, singleSheet = newExcel(0, str(os.path.join(vocabFolder, fileName + &quot;0.xlsx&quot;)), &quot;Single Parsed Triples&quot;)&#10;&#10;    # For each statement present in the graph obtained store the triples&#10;    index = 0&#10;    for subject, predicate, object_ in g:&#10;        # Compute the filtered statement of the Triples&#10;        subjectTerm = subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(subjectTerm) and len(subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            subjectTerm = subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        predicateTerm = predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(predicateTerm) and len(predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            predicateTerm = predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        objectTerm = object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(objectTerm) and len(object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            objectTerm = object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        if(row[&quot;prefix&quot;] == &quot;FreeBase&quot;):&#10;            subjectTerm = subjectTerm.split(&quot;.&quot;)[-1]&#10;            if(not len(subjectTerm) and len(subjectTerm.split(&quot;.&quot;)) &gt; 1):&#10;                subjectTerm = subjectTerm.split(&quot;.&quot;)[-2]&#10;            predicateTerm = predicateTerm.split(&quot;.&quot;)[-1]&#10;            if(not len(objectTerm) and len(predicateTerm.split(&quot;.&quot;)) &gt; 1):&#10;                predicateTerm = predicateTerm.split(&quot;.&quot;)[-2]&#10;            objectTerm = objectTerm.split(&quot;.&quot;)[-1]&#10;            if(not len(objectTerm) and len(objectTerm.split(&quot;.&quot;)) &gt; 1):&#10;                objectTerm = objectTerm.split(&quot;.&quot;)[-2]&#10;&#10;        # Check if the triple has to be saved, if there is a predicate selection then checks if that predicate has to be saved&#10;        bool_ = False&#10;        # If there is no predicate selection then save every triple&#10;        strPredicates = &quot;%{Predicates}&quot;&#10;        if(len(strPredicates.split()) == 0):&#10;            bool_ = True&#10;        # If there is a predicate selection then check if that predicate has to be saved&#10;        else:&#10;            for pred in strPredicates.split():&#10;                if(pred == str(predicateTerm) or pred == str(predicate)):&#10;                    bool_ = True&#10;                    break&#10;        # Check if the triple has to be saved&#10;        if(bool_ == True):&#10;            # Save the statement to the List to be added to the DataFrame&#10;            list_.insert(index,{&quot;Date&quot;: date, &quot;Subject&quot;: subject, &quot;Predicate&quot;: predicate, &quot;Object&quot;: object_, &quot;SubjectTerm&quot;: subjectTerm, &quot;PredicateTerm&quot;: predicateTerm, &quot;ObjectTerm&quot;: objectTerm, &quot;Domain&quot;: row[&quot;prefix&quot;], &quot;Domain Version&quot;: row[&quot;VersionName&quot;], &quot;Domain Date&quot;: row[&quot;VersionDate&quot;], &quot;URI&quot;: row[&quot;URI&quot;], &quot;Title&quot;: row[&quot;Title&quot;], &quot;Languages&quot;: row[&quot;Languages&quot;]})&#10;            index += 1&#10;            # Save the statement to the ExcelSheet Triples&#10;            singleSheet.write_row(singleExcel.index, 0, (date, subject, predicate, object_, subjectTerm, predicateTerm, objectTerm, row[&quot;prefix&quot;], row[&quot;VersionName&quot;], row[&quot;VersionDate&quot;], row[&quot;URI&quot;], row[&quot;Title&quot;], row[&quot;Languages&quot;]))&#10;            totalSheet.write_row(totalExcel.index, 0, (date, subject, predicate, object_, subjectTerm, predicateTerm, objectTerm, row[&quot;prefix&quot;], row[&quot;VersionName&quot;], row[&quot;VersionDate&quot;], row[&quot;URI&quot;], row[&quot;Title&quot;], row[&quot;Languages&quot;]))&#10;            # Update the index of both the ExcelFiles&#10;            singleExcel.index += 1&#10;            totalExcel.index += 1&#10;            # If the rows reach the excel limit then create a new ExcelFile&#10;            if(singleExcel.index == 1048575):&#10;                #Close the ExcelFile&#10;                singleWorkbook.close()&#10;                singleExcel.writer.save()&#10;                # Create a new ExcelFile&#10;                singleExcel, singleWorkbook, singleSheet = newExcel(singleExcel.num, str(os.path.join(vocabFolder, fileName + str(singleExcel.num) + &quot;.xlsx&quot;)), &quot;Single Parsed Triples&quot;)&#10;            # If the rows reach the excel limit then create a new ExcelFile&#10;            if(totalExcel.index == 1048575):&#10;                #Close the ExcelFile&#10;                totalExcel.writer.book.close()&#10;                totalExcel.writer.save()&#10;                # Create a new ExcelFile&#10;                totalExcel, totalWorkbook, totalSheet = newExcel(totalExcel.num, str(os.path.join(os.path.dirname(vocabFolder), date + &quot;_Parsed_Knowledge-Triples_&quot; + str(totalExcel.num) + &quot;.xlsx&quot;)), &quot;Total Parsed Triples&quot;)&#10;&#10;    # Close the Excel file of the single vocabulary&#10;    singleExcel.writer.book.close()&#10;    singleExcel.writer.save()&#10;&#10;    # Return the List to be added to the DataFrame and the relative index&#10;    return totalExcel, list_, index&#10;&#10;# Serialize the vocabulary in a format saved in vocabFolder/Resources with the same name of the Excel File&#10;def serializeVoc(vocabFolder, fileName, g, format_):&#10;    try:&#10;        resourceDestination = os.path.join(vocabFolder, &quot;Resources/&quot;+format_)&#10;        if not os.path.isdir(resourceDestination):&#10;            os.makedirs(resourceDestination)&#10;        g.serialize(destination=str(os.path.join(resourceDestination, fileName)), format=format_)&#10;    except Exception as e:&#10;        # In case of an error during the graph's serialization, print the error&#10;        print(str(e) + &quot;\n&quot;)&#10;&#10;# Create a new ExcelFile&#10;def newExcel(excelNum, fileName, sheetName):&#10;    # Create a Pandas Excel writer using XlsxWriter as the engine.&#10;    writer = pd.ExcelWriter(fileName, engine='xlsxwriter', options={'strings_to_urls': False, 'constant_memory': True, 'nan_inf_to_errors': True})&#10;    excelFile_ = ExcelFile(writer, excelNum)&#10;    # Get the xlsxwriter workbook and worksheet objects.&#10;    workbook  = writer.book&#10;    # Add WorkSheet with relative titles and relative bold header &#10;    worksheet = workbook.add_worksheet(sheetName)&#10;    worksheet.write_row(0, 0, (&quot;Date&quot;, &quot;Subject&quot;, &quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;, &quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;, &quot;Domain&quot;, &quot;Domain Version&quot;, &quot;Domain Date&quot;, &quot;URI&quot;, &quot;Title&quot;, &quot;Languages&quot;), workbook.add_format({&quot;bold&quot;: True}))&#10;    worksheet.set_column(0, 8, 30)&#10;    # Return the new excelFile_, workbook, worksheet&#10;    return excelFile_, workbook, worksheet&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(vocabs):&#10;    # Create the folder used to store the results&#10;    folderDestination = &quot;%{folderDestination}&quot;&#10;    location = os.path.normpath(os.path.expanduser(folderDestination))&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;&#10;    # Get the date of the current day of the Parsing&#10;    date = time.strftime(&quot;%Y-%m-%d&quot;, time.gmtime())&#10;&#10;    # Create a Pandas Excel writer using XlsxWriter as the engine.&#10;    totalExcel, totalWorkbook, totalSheet = newExcel(0, str(os.path.join(location, date + &quot;_Parsed_Knowledge-Triples_0.xlsx&quot;)), &quot;Total Parsed Triples&quot;)&#10;&#10;    # Create the DataFrame to save the vocabs' Date of parsing, Subject, Predicate, Object, Domain, Domain Version, Domain Date, URI, Title, Languages&#10;    df = pd.DataFrame(columns=[&quot;Date&quot;, &quot;Subject&quot;, &quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;, &quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;, &quot;Domain&quot;, &quot;Domain Version&quot;, &quot;Domain Date&quot;, &quot;URI&quot;, &quot;Title&quot;, &quot;Languages&quot;])&#10;&#10;    # Iterate for every vocabulary read from the second argument&#10;    for index, row in vocabs.iterrows():&#10;        # Create the Folder &#10;        vocabFolder = str(os.path.join(location, row[&quot;Folder&quot;]))&#10;        if not os.path.isdir(vocabFolder):&#10;            os.makedirs(vocabFolder)&#10;        &#10;        # Add information for each vocabulary&#10;        totalExcel, list_, i = parse(vocabFolder, date, row, totalExcel, list())&#10;        # Save the information on the DataFrame for each vocabulary&#10;        if(i and len(list_)):&#10;            df = df.append(list_)&#10;&#10;    # Close the Excel file of the single vocabulary&#10;    totalExcel.writer.book.close()&#10;    totalExcel.writer.save()&#10;&#10;    # Return the DataFrame for RapidMiner visualization&#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_csv" compatibility="9.2.001" expanded="true" height="82" name="Write CSV Parsed" width="90" x="1251" y="340">
        <parameter key="csv_file" value="C:\Users\marco\Desktop\K-Files\FullTriples.csv"/>
        <parameter key="column_separator" value="|"/>
        <parameter key="write_attribute_names" value="true"/>
        <parameter key="quote_nominal_values" value="true"/>
        <parameter key="format_date_attributes" value="true"/>
        <parameter key="append_to_file" value="false"/>
        <parameter key="encoding" value="SYSTEM"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="124" name="KnowledgeInheritor" width="90" x="849" y="544">
        <parameter key="script" value="# Import libraries&#10;import rdflib&#10;from rdflib import Graph, RDFS&#10;from rdflib.util import guess_format&#10;from rdflib.plugins.parsers.notation3 import N3Parser&#10;import pandas as pd&#10;import os&#10;import time&#10;import networkx as nx&#10;&#10;# Class to handle the Excel file and relative indexes&#10;class ExcelFile:&#10;    def __init__(self, writer, num = 0):&#10;        self.writer = writer&#10;        self.index = 1&#10;        self.num = num + 1&#10;&#10;# Parse the given file and add its information to the file Excel given as third parameter&#10;def parse(vocabFolder, date, row, inTotalExcel, list_):&#10;    # Try to create the graph to analyze the vocabulary&#10;    try:&#10;        g = Graph()&#10;        format_ = row[&quot;Link&quot;].split(&quot;.&quot;)[-1]&#10;        if(format_ == &quot;txt&quot;):&#10;            format_ = row[&quot;Link&quot;].split(&quot;.&quot;)[-2]&#10;        format_ = format_.split(&quot;?&quot;)[0]&#10;        result = g.parse(row[&quot;Link&quot;], format=guess_format(format_), publicID=row[&quot;prefix&quot;])&#10;    except Exception as e:&#10;        # In case of an error during the graph's initiation, print the error and return an empty list&#10;        print(str(e) + &quot;\n&quot;)    &#10;        return inTotalExcel, list_, 0&#10;&#10;    # Get the inTotalExcel file and relative worksheets&#10;    inTotalWorkbook = inTotalExcel.writer.book&#10;    inTotalSheet = inTotalWorkbook.get_worksheet_by_name(&quot;Inherited Total Triples&quot;)&#10;&#10;    # Elaborate the fileName of the vocabulary&#10;    inFileName = date + &quot;_Inherited_&quot; + row[&quot;prefix&quot;] + &quot;_&quot; + row[&quot;VersionName&quot;] + &quot;_&quot; + row[&quot;VersionDate&quot;] + &quot;_&quot;&#10;    # Create a Pandas Excel writer using XlsxWriter as the engine.&#10;    inSingleExcel, inSingleWorkbook, inSingleSheet = newExcel(0, str(os.path.join(vocabFolder, inFileName + &quot;0.xlsx&quot;)), &quot;Inherited Single Triples&quot;)&#10;&#10;    # Create the DataFrame used to save the table used to handle the inheritance relations&#10;    inherit = pd.DataFrame(columns=[&quot;Subject&quot;, &quot;subOf&quot;, &quot;subbed&quot;])&#10;    # Create the set used to check if new Subject inheritance relation has to be added or if existing Subject inheritance relation has to be updated&#10;    set_ = set()&#10;    # Save inheritance relation between subject and object&#10;    for s, o in g.subject_objects(RDFS.subClassOf):&#10;        inherit, set_ = setInheritance(str(s), str(o), inherit, set_)&#10;    # Create the networkx graph used to calculate inheritance&#10;    nxG = createNXGraph(inherit)&#10;    # Calculate the transitive_closure of the networkx graph to get all the possible inheritances&#10;    nxGT = nx.transitive_closure(nxG)&#10;&#10;    # For each statement present in the graph obtained store the triples&#10;    index = 0&#10;    for subject, predicate, object_ in g:&#10;        # Compute the filtered statement of the Triples&#10;        subjectTerm = subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(subjectTerm) and len(subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            subjectTerm = subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        predicateTerm = predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(predicateTerm) and len(predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            predicateTerm = predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        objectTerm = object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(objectTerm) and len(object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            objectTerm = object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        if(row[&quot;prefix&quot;] == &quot;FreeBase&quot;):&#10;            subjectTerm = subjectTerm.split(&quot;.&quot;)[-1]&#10;            if(not len(subjectTerm) and len(subjectTerm.split(&quot;.&quot;)) &gt; 1):&#10;                subjectTerm = subjectTerm.split(&quot;.&quot;)[-2]&#10;            predicateTerm = predicateTerm.split(&quot;.&quot;)[-1]&#10;            if(not len(objectTerm) and len(predicateTerm.split(&quot;.&quot;)) &gt; 1):&#10;                predicateTerm = predicateTerm.split(&quot;.&quot;)[-2]&#10;            objectTerm = objectTerm.split(&quot;.&quot;)[-1]&#10;            if(not len(objectTerm) and len(objectTerm.split(&quot;.&quot;)) &gt; 1):&#10;                objectTerm = objectTerm.split(&quot;.&quot;)[-2]&#10;        &#10;        # Check if the triple has to be saved, if there is a predicate selection then checks if that predicate has to be saved&#10;        bool_ = False&#10;        # If there is no predicate selection then save every triple&#10;        strPredicates = &quot; &quot;&#10;        if(len(strPredicates.split()) == 0):&#10;            bool_ = True&#10;        # If there is a predicate selection then check if that predicate has to be saved&#10;        else:&#10;            for pred in strPredicates.split():&#10;                if(pred == str(predicateTerm) or pred == str(predicate)):&#10;                    bool_ = True&#10;                    break&#10;        # Check if the triple has to be saved&#10;        if(bool_ == True):&#10;            # Save the statement to the List to be added to the DataFrame&#10;            list_.insert(index,{&quot;Date&quot;: date, &quot;Subject&quot;: subject, &quot;Predicate&quot;: predicate, &quot;Object&quot;: object_, &quot;SubjectTerm&quot;: subjectTerm, &quot;PredicateTerm&quot;: predicateTerm, &quot;ObjectTerm&quot;: objectTerm, &quot;Domain&quot;: row[&quot;prefix&quot;], &quot;Domain Version&quot;: row[&quot;VersionName&quot;], &quot;Domain Date&quot;: row[&quot;VersionDate&quot;], &quot;URI&quot;: row[&quot;URI&quot;], &quot;Title&quot;: row[&quot;Title&quot;], &quot;Languages&quot;: row[&quot;Languages&quot;], &quot;Inherited&quot;: 0})&#10;            index += 1&#10;            &#10;            # Save the statement to the ExcelSheet Triples&#10;            inSingleSheet.write_row(inSingleExcel.index, 0, (date, subject, predicate, object_, subjectTerm, predicateTerm, objectTerm, row[&quot;prefix&quot;], row[&quot;VersionName&quot;], row[&quot;VersionDate&quot;], row[&quot;URI&quot;], row[&quot;Title&quot;], row[&quot;Languages&quot;], 0))&#10;            inTotalSheet.write_row(inTotalExcel.index, 0, (date, subject, predicate, object_, subjectTerm, predicateTerm, objectTerm, row[&quot;prefix&quot;], row[&quot;VersionName&quot;], row[&quot;VersionDate&quot;], row[&quot;URI&quot;], row[&quot;Title&quot;], row[&quot;Languages&quot;], 0))&#10;            # Update the index of both the ExcelSheets&#10;            inSingleExcel.index += 1&#10;            inTotalExcel.index += 1&#10;&#10;            # If the rows of inSingleExcel reach the excel limit then create a new ExcelFile&#10;            if(inSingleExcel.index == 1048575):&#10;                #Close the ExcelFile&#10;                inSingleWorkbook.close()&#10;                inSingleExcel.writer.save()&#10;                # Create a new ExcelFile&#10;                inSingleExcel, inSingleWorkbook, inSingleSheet = newExcel(inSingleExcel.num, str(os.path.join(vocabFolder, inFileName + str(inSingleExcel.num) + &quot;.xlsx&quot;)), &quot;Inherited Single Triples&quot;)&#10;                &#10;            # If the rows of totalExcel reach the excel limit then create a new ExcelFile&#10;            if(inTotalExcel.index == 1048575):&#10;                #Close the ExcelFile&#10;                inTotalWorkbook.close()&#10;                inTotalExcel.writer.save()&#10;                # Create a new ExcelFile&#10;                inTotalExcel, inTotalWorkbook, inTotalSheet = newExcel(inTotalExcel.num, str(os.path.join(os.path.dirname(vocabFolder), date + &quot;_Inherited_Knowledge-Triples_&quot; + str(inTotalExcel.num) + &quot;.xlsx&quot;)), &quot;Inherited Total Triples&quot;)&#10;&#10;            for node in nxGT:&#10;                for n in nxGT.neighbors(node):&#10;                    if(str(n) == str(object_)):&#10;                        # Compute the filtered statement of the Triples&#10;                        nodeTerm = node.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;                        if(not len(nodeTerm) and len(node.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;                            nodeTerm = node.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;                        &#10;                        # Save the statement to the List to be added to the DataFrame&#10;                        list_.insert(index,{&quot;Date&quot;: date, &quot;Subject&quot;: subject, &quot;Predicate&quot;: predicate, &quot;Object&quot;: node, &quot;SubjectTerm&quot;: subjectTerm, &quot;PredicateTerm&quot;: predicateTerm, &quot;ObjectTerm&quot;: nodeTerm, &quot;Domain&quot;: row[&quot;prefix&quot;], &quot;Domain Version&quot;: row[&quot;VersionName&quot;], &quot;Domain Date&quot;: row[&quot;VersionDate&quot;], &quot;URI&quot;: row[&quot;URI&quot;], &quot;Title&quot;: row[&quot;Title&quot;], &quot;Languages&quot;: row[&quot;Languages&quot;], &quot;Inherited&quot;: 1})&#10;                        index += 1&#10;                        &#10;                        # Save the statement to the ExcelSheet Triples&#10;                        inSingleSheet.write_row(inSingleExcel.index, 0, (date, subject, predicate, node, subjectTerm, predicateTerm, nodeTerm, row[&quot;prefix&quot;], row[&quot;VersionName&quot;], row[&quot;VersionDate&quot;], row[&quot;URI&quot;], row[&quot;Title&quot;], row[&quot;Languages&quot;], 1))&#10;                        inTotalSheet.write_row(inTotalExcel.index, 0, (date, subject, predicate, node, subjectTerm, predicateTerm, nodeTerm, row[&quot;prefix&quot;], row[&quot;VersionName&quot;], row[&quot;VersionDate&quot;], row[&quot;URI&quot;], row[&quot;Title&quot;], row[&quot;Languages&quot;], 1))&#10;                        # Update the index of both the ExcelSheets&#10;                        inSingleExcel.index += 1&#10;                        inTotalExcel.index += 1&#10;&#10;                        # If the rows of inSingleExcel reach the excel limit then create a new ExcelFile&#10;                        if(inSingleExcel.index == 1048575):&#10;                            #Close the ExcelFile&#10;                            inSingleWorkbook.close()&#10;                            inSingleExcel.writer.save()&#10;                            # Create a new ExcelFile&#10;                            inSingleExcel, inSingleWorkbook, inSingleSheet = newExcel(inSingleExcel.num, str(os.path.join(vocabFolder, inFileName + str(inSingleExcel.num) + &quot;.xlsx&quot;)), &quot;Inherited Single Triples&quot;)&#10;                            &#10;                        # If the rows of totalExcel reach the excel limit then create a new ExcelFile&#10;                        if(inTotalExcel.index == 1048575):&#10;                            #Close the ExcelFile&#10;                            inTotalWorkbook.close()&#10;                            inTotalExcel.writer.save()&#10;                            # Create a new ExcelFile&#10;                            inTotalExcel, inTotalWorkbook, inTotalSheet = newExcel(inTotalExcel.num, str(os.path.join(os.path.dirname(vocabFolder), date + &quot;_Inherited_Knowledge-Triples_&quot; + str(inTotalExcel.num) + &quot;.xlsx&quot;)), &quot;Inherited Total Triples&quot;)&#10;&#10;&#10;    # Close the Excel file of the single vocabulary&#10;    inSingleExcel.writer.book.close()&#10;    inSingleExcel.writer.save()&#10;    &#10;    # Return the List to be added to the DataFrame and the relative index&#10;    return inTotalExcel, list_, index&#10;&#10;# Create a new ExcelFile&#10;def newExcel(excelNum, fileName, sheetName):&#10;    # Create a Pandas Excel writer using XlsxWriter as the engine.&#10;    writer = pd.ExcelWriter(fileName, engine='xlsxwriter', options={'strings_to_urls': False, 'constant_memory': True, 'nan_inf_to_errors': True})&#10;    excelFile_ = ExcelFile(writer, excelNum)&#10;    # Get the xlsxwriter workbook and worksheet objects.&#10;    workbook  = writer.book&#10;    # Add WorkSheet with relative titles and relative bold header &#10;    worksheet = workbook.add_worksheet(sheetName)&#10;    worksheet.write_row(0, 0, (&quot;Date&quot;, &quot;Subject&quot;, &quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;, &quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;, &quot;Domain&quot;, &quot;Domain Version&quot;, &quot;Domain Date&quot;, &quot;URI&quot;, &quot;Title&quot;, &quot;Languages&quot;, &quot;Inherited&quot;), workbook.add_format({&quot;bold&quot;: True}))&#10;    worksheet.set_column(0, 12, 30)&#10;    # Return the new excelFile_, workbook, worksheet&#10;    return excelFile_, workbook, worksheet&#10;&#10;# Set the inheritance&#10;def setInheritance(subject, object_, inherit, set_):&#10;    # Compute the length of the subject's set&#10;    a = len(set_)&#10;    # Add the subject to the set&#10;    set_.add(subject)&#10;    # Check if the length now is bigger, i.e. a new element has been added&#10;    if(a &lt; len(set_)):&#10;        # Add the new subject and relative first inheritable object&#10;        inherit.at[subject, &quot;Subject&quot;] = subject&#10;        inherit.at[subject, &quot;subOf&quot;] = object_&#10;        inherit.at[subject, &quot;subbed&quot;] = &quot;&quot;&#10;    else:&#10;        # Add a new inheritable object&#10;        subOfs = str(inherit.at[subject, &quot;subOf&quot;])&#10;        inherit.at[subject, &quot;subOf&quot;] = subOfs + &quot; , &quot; + object_&#10;    # Return the dataframe and set relative to inheritance&#10;    return inherit, set_&#10;&#10;# Return the networkx graph used to calculate inheritance&#10;def createNXGraph(inherit):&#10;    # Create the networkx graph used to calculate inheritance&#10;    nxG = nx.DiGraph()&#10;    # Use this set to add a single node for every subject that inherit &#10;    nodes = set()&#10;&#10;    # Iterate over every element of the dataframe containing the informations about inheritance&#10;    for index, row in inherit.iterrows():&#10;        # Get the relative inheritable objects of a subject&#10;        subOfs = str(inherit.at[str(row[&quot;Subject&quot;]), &quot;subOf&quot;])&#10;        l = len(nodes)&#10;        nodes.add(str(row[&quot;Subject&quot;]))&#10;        if(len(nodes) &gt; l):&#10;            nxG.add_node(str(row[&quot;Subject&quot;]))&#10;        for sub in subOfs.split(&quot; , &quot;):&#10;            # Get the relative already inherited objects of a subject&#10;            l = len(nodes)&#10;            nodes.add(sub)&#10;            if(len(nodes) &gt; l):&#10;                nxG.add_node(sub)&#10;            nxG.add_edge(str(row[&quot;Subject&quot;]), sub)&#10;&#10;    # Return the networkx graph used to calculate inheritance&#10;    return nxG&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(vocabs):&#10;    # Create the folder used to store the results&#10;    folderDestination = &quot;~/Desktop/K-Files/&quot;&#10;    location = os.path.normpath(os.path.expanduser(folderDestination))&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;&#10;    # Get the date of the current day of the Parsing&#10;    date = time.strftime(&quot;%Y-%m-%d&quot;, time.gmtime())&#10;&#10;    # Create a Pandas Excel writer using XlsxWriter as the engine.&#10;    inTotalExcel, inTotalWorkbook, inTotalSheet = newExcel(0, str(os.path.join(location, date + &quot;_Inherited_Knowledge-Triples_0.xlsx&quot;)), &quot;Inherited Total Triples&quot;)&#10;&#10;    # Create the DataFrame to save the vocabs' Date of parsing, Subject, Predicate, Object, Domain, Domain Version, Domain Date, URI, Title, Languages&#10;    df = pd.DataFrame(columns=[&quot;Date&quot;, &quot;Subject&quot;, &quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;, &quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;, &quot;Domain&quot;, &quot;Domain Version&quot;, &quot;Domain Date&quot;, &quot;URI&quot;, &quot;Title&quot;, &quot;Languages&quot;, &quot;Inherited&quot;])&#10;&#10;    # Iterate for every vocabulary read from the second argument&#10;    for index, row in vocabs.iterrows():&#10;        # Create the Folder &#10;        vocabFolder = str(os.path.join(location, row[&quot;Folder&quot;]))&#10;        if not os.path.isdir(vocabFolder):&#10;            os.makedirs(vocabFolder)&#10;        # Add information for each vocabulary&#10;        inTotalExcel, list_, i = parse(vocabFolder, date, row, inTotalExcel, list())&#10;        # Save the information on the DataFrame for each vocabulary&#10;        if(i and len(list_)):&#10;            df = df.append(list_)&#10;&#10;    # Close the Excel file of the single vocabulary&#10;    inTotalExcel.writer.book.close()&#10;    inTotalExcel.writer.save()&#10;&#10;    # Return the DataFrame for RapidMiner visualization&#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_csv" compatibility="9.2.001" expanded="true" height="82" name="Write CSV Inherited" width="90" x="1251" y="493">
        <parameter key="csv_file" value="C:\Users\marco\Desktop\K-Files\FilteredTriples.csv"/>
        <parameter key="column_separator" value="|"/>
        <parameter key="write_attribute_names" value="true"/>
        <parameter key="quote_nominal_values" value="true"/>
        <parameter key="format_date_attributes" value="true"/>
        <parameter key="append_to_file" value="false"/>
        <parameter key="encoding" value="SYSTEM"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="103" name="MACROParser" width="90" x="849" y="748">
        <parameter key="script" value="# Import libraries&#10;import rdflib&#10;from rdflib import Graph, Namespace&#10;from rdflib.util import guess_format&#10;from rdflib.plugins.parsers.notation3 import N3Parser&#10;import pandas as pd&#10;import time&#10;&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main():&#10;    # Get the date of the current day of the Parsing&#10;    date = time.strftime(&quot;%Y-%m-%d&quot;, time.gmtime())&#10;&#10;    # Create the DataFrame to save the vocabs' Date of parsing, Subject, Predicate, Object, Domain, Domain Version, Domain Date, URI, Title, Languages&#10;    df = pd.DataFrame(columns=[&quot;Date&quot;, &quot;Subject&quot;, &quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;, &quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;, &quot;Domain&quot;])&#10;&#10;    # Get the full path name from the MACRO&#10;    fileName = &quot;%{fileName}&quot;&#10;    # Try to create the graph to analyze the vocabulary&#10;    try:&#10;        g = Graph()&#10;        format_ = fileName.split(&quot;.&quot;)[-1]&#10;        if(format_ == &quot;txt&quot;):&#10;            format_ = fileName.split(&quot;.&quot;)[-2]&#10;        format_ = format_.split(&quot;?&quot;)[0]&#10;        result = g.parse(fileName, format=guess_format(format_))&#10;    except Exception as e:&#10;        # In case of an error during the graph's initiation, print the error and return an empty list&#10;        print(str(e) + &quot;\n&quot;)    &#10;        return totalExcel, list_, 0&#10;&#10;    # Create list used to store the triples&#10;    list_ = list()&#10;    # For each statement present in the graph obtained store the triples&#10;    index = 0&#10;    for subject, predicate, object_ in g:&#10;        # Compute the filtered statement of the Triples&#10;        subjectTerm = subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(subjectTerm) and len(subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            subjectTerm = subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        predicateTerm = predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(predicateTerm) and len(predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            predicateTerm = predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        objectTerm = object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(objectTerm) and len(object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            objectTerm = object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        &#10;        # Check if the triple has to be saved, if there is a predicate selection then checks if that predicate has to be saved&#10;        bool_ = False&#10;        # If there is no predicate selection then save every triple&#10;        strPredicates = &quot;%{Predicates}&quot;&#10;        if(len(strPredicates.split()) == 0):&#10;            bool_ = True&#10;        # If there is a predicate selection then check if that predicate has to be saved&#10;        else:&#10;            for pred in strPredicates.split():&#10;                if(pred == str(predicateTerm) or pred == str(predicate)):&#10;                    bool_ = True&#10;                    break&#10;        # Check if the triple has to be saved&#10;        if(bool_ == True):&#10;            # Save the statement to the List to be added to the DataFrame&#10;            list_.insert(index,{&quot;Date&quot;: date, &quot;Subject&quot;: subject, &quot;Predicate&quot;: predicate, &quot;Object&quot;: object_, &quot;SubjectTerm&quot;: subjectTerm, &quot;PredicateTerm&quot;: predicateTerm, &quot;ObjectTerm&quot;: objectTerm, &quot;Domain&quot;: &quot;%{fileName}&quot;})&#10;            index += 1&#10;            &#10;    # Add the List to the DataFrame&#10;    df = df.append(list_)&#10;&#10;    # Return the DataFrame for RapidMiner visualization&#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_csv" compatibility="9.2.001" expanded="true" height="82" name="Write CSV MACRO Parsed" width="90" x="1184" y="748">
        <parameter key="csv_file" value="C:\Users\marco\Desktop\K-Files\FullTriples.csv"/>
        <parameter key="column_separator" value="|"/>
        <parameter key="write_attribute_names" value="true"/>
        <parameter key="quote_nominal_values" value="true"/>
        <parameter key="format_date_attributes" value="true"/>
        <parameter key="append_to_file" value="false"/>
        <parameter key="encoding" value="SYSTEM"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="103" name="MACROInheritor" width="90" x="849" y="850">
        <parameter key="script" value="# Import libraries&#10;import rdflib&#10;from rdflib import Graph, RDFS&#10;from rdflib.util import guess_format&#10;from rdflib.plugins.parsers.notation3 import N3Parser&#10;import pandas as pd&#10;import time&#10;import networkx as nx&#10;                        &#10;&#10;&#10;# Set the inheritance&#10;def setInheritance(subject, object_, inherit, set_):&#10;    # Compute the length of the subject's set&#10;    a = len(set_)&#10;    # Add the subject to the set&#10;    set_.add(subject)&#10;    # Check if the length now is bigger, i.e. a new element has been added&#10;    if(a &lt; len(set_)):&#10;        # Add the new subject and relative first inheritable object&#10;        inherit.at[subject, &quot;Subject&quot;] = subject&#10;        inherit.at[subject, &quot;subOf&quot;] = object_&#10;        inherit.at[subject, &quot;subbed&quot;] = &quot;&quot;&#10;    else:&#10;        # Add a new inheritable object&#10;        subOfs = str(inherit.at[subject, &quot;subOf&quot;])&#10;        inherit.at[subject, &quot;subOf&quot;] = subOfs + &quot; , &quot; + object_&#10;    # Return the dataframe and set relative to inheritance&#10;    return inherit, set_&#10;&#10;# Return the networkx graph used to calculate inheritance&#10;def createNXGraph(inherit):&#10;    # Create the networkx graph used to calculate inheritance&#10;    nxG = nx.DiGraph()&#10;    # Use this set to add a single node for every subject that inherit &#10;    nodes = set()&#10;&#10;    # Iterate over every element of the dataframe containing the informations about inheritance&#10;    for index, row in inherit.iterrows():&#10;        # Get the relative inheritable objects of a subject&#10;        subOfs = str(inherit.at[str(row[&quot;Subject&quot;]), &quot;subOf&quot;])&#10;        l = len(nodes)&#10;        nodes.add(str(row[&quot;Subject&quot;]))&#10;        if(len(nodes) &gt; l):&#10;            nxG.add_node(str(row[&quot;Subject&quot;]))&#10;        for sub in subOfs.split(&quot; , &quot;):&#10;            # Get the relative already inherited objects of a subject&#10;            l = len(nodes)&#10;            nodes.add(sub)&#10;            if(len(nodes) &gt; l):&#10;                nxG.add_node(sub)&#10;            nxG.add_edge(str(row[&quot;Subject&quot;]), sub)&#10;&#10;    # Return the networkx graph used to calculate inheritance&#10;    return nxG&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main():&#10;    # Get the date of the current day of the Parsing&#10;    date = time.strftime(&quot;%Y-%m-%d&quot;, time.gmtime())&#10;    &#10;    # Create the DataFrame to save the vocabs' Date of parsing, Subject, Predicate, Object, Domain, Domain Version, Domain Date, URI, Title, Languages&#10;    df = pd.DataFrame(columns=[&quot;Date&quot;, &quot;Subject&quot;, &quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;, &quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;, &quot;Domain&quot;, &quot;Inherited&quot;])&#10;&#10;    # Get the fileName from the MACRO&#10;    fileName = &quot;%{fileName}&quot;&#10;    # Try to create the graph to analyze the vocabulary&#10;    try:&#10;        g = Graph()&#10;        format_ = fileName.split(&quot;.&quot;)[-1]&#10;        if(format_ == &quot;txt&quot;):&#10;            format_ = fileName.split(&quot;.&quot;)[-2]&#10;        format_ = format_.split(&quot;?&quot;)[0]&#10;        result = g.parse(fileName, format=guess_format(format_))&#10;    except Exception as e:&#10;        # In case of an error during the graph's initiation, print the error and return an empty list&#10;        print(str(e) + &quot;\n&quot;)    &#10;        return inTotalExcel, list_, 0&#10;&#10;    # Create the DataFrame used to save the table used to handle the inheritance relations&#10;    inherit = pd.DataFrame(columns=[&quot;Subject&quot;, &quot;subOf&quot;, &quot;subbed&quot;])&#10;    # Create the set used to check if new Subject inheritance relation has to be added or if existing Subject inheritance relation has to be updated&#10;    set_ = set()&#10;    # Save inheritance relation between subject and object&#10;    for s, o in g.subject_objects(RDFS.subClassOf):&#10;        inherit, set_ = setInheritance(str(s), str(o), inherit, set_)&#10;    # Create the networkx graph used to calculate inheritance&#10;    nxG = createNXGraph(inherit)&#10;    # Calculate the transitive_closure of the networkx graph to get all the possible inheritances&#10;    nxGT = nx.transitive_closure(nxG)&#10;&#10;    # Create list used to store the triples&#10;    list_ = list()&#10;    # For each statement present in the graph obtained store the triples&#10;    index = 0&#10;    for subject, predicate, object_ in g:&#10;        # Compute the filtered statement of the Triples&#10;        subjectTerm = subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(subjectTerm) and len(subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            subjectTerm = subject.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        predicateTerm = predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(predicateTerm) and len(predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            predicateTerm = predicate.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;        objectTerm = object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;        if(not len(objectTerm) and len(object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;            objectTerm = object_.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;            &#10;        # Check if the triple has to be saved, if there is a predicate selection then checks if that predicate has to be saved&#10;        bool_ = False&#10;        # If there is no predicate selection then save every triple&#10;        strPredicates = &quot; &quot;&#10;        if(len(strPredicates.split()) == 0):&#10;            bool_ = True&#10;        # If there is a predicate selection then check if that predicate has to be saved&#10;        else:&#10;            for pred in strPredicates.split():&#10;                if(pred == str(predicateTerm) or pred == str(predicate)):&#10;                    bool_ = True&#10;                    break&#10;        # Check if the triple has to be saved&#10;        if(bool_ == True):&#10;            # Save the statement to the List to be added to the DataFrame&#10;            list_.insert(index,{&quot;Date&quot;: date, &quot;Subject&quot;: subject, &quot;Predicate&quot;: predicate, &quot;Object&quot;: object_, &quot;SubjectTerm&quot;: subjectTerm, &quot;PredicateTerm&quot;: predicateTerm, &quot;ObjectTerm&quot;: objectTerm, &quot;Domain&quot;: &quot;%{fileName}&quot;, &quot;Inherited&quot;: 0})&#10;            index += 1&#10;&#10;            # Check the inheritance graph&#10;            for node in nxGT:&#10;                for n in nxGT.neighbors(node):&#10;                    if(str(n) == str(object_)):&#10;                        # Compute the filtered statement of the Triples&#10;                        nodeTerm = node.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-1]&#10;                        if(not len(nodeTerm) and len(node.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)) &gt; 1):&#10;                            nodeTerm = node.replace(&quot;/&quot;, &quot;#&quot;).split(&quot;#&quot;)[-2]&#10;                        &#10;                        # Save the statement to the List to be added to the DataFrame&#10;                        list_.insert(index,{&quot;Date&quot;: date, &quot;Subject&quot;: subject, &quot;Predicate&quot;: predicate, &quot;Object&quot;: node, &quot;SubjectTerm&quot;: subjectTerm, &quot;PredicateTerm&quot;: predicateTerm, &quot;ObjectTerm&quot;: nodeTerm, &quot;Domain&quot;: &quot;%{fileName}&quot;, &quot;Inherited&quot;: 1})&#10;                        index += 1&#9;&#10;&#10;    # Add the List to the DataFrame&#10;    df = df.append(list_)&#10;&#10;    # Return the DataFrame for RapidMiner visualization&#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_csv" compatibility="9.2.001" expanded="true" height="82" name="Write CSV MACRO Inherited" width="90" x="1184" y="850">
        <parameter key="csv_file" value="C:\Users\marco\Desktop\K-Files\FilteredTriples.csv"/>
        <parameter key="column_separator" value="|"/>
        <parameter key="write_attribute_names" value="true"/>
        <parameter key="quote_nominal_values" value="true"/>
        <parameter key="format_date_attributes" value="true"/>
        <parameter key="append_to_file" value="false"/>
        <parameter key="encoding" value="SYSTEM"/>
      </operator>
      <connect from_op="Read Knowledge" from_port="output" to_op="Multiply" to_port="input"/>
      <connect from_op="Multiply" from_port="output 1" to_op="KnowledgeParser" to_port="input 1"/>
      <connect from_op="Multiply" from_port="output 2" to_op="KnowledgeInheritor" to_port="input 1"/>
      <connect from_op="Set Macros" from_port="through 1" to_op="KnowledgeParser" to_port="input 2"/>
      <connect from_op="Set Macros" from_port="through 2" to_op="KnowledgeInheritor" to_port="input 2"/>
      <connect from_op="Set Macros" from_port="through 3" to_op="MACROParser" to_port="input 1"/>
      <connect from_op="Set Macros" from_port="through 4" to_op="MACROInheritor" to_port="input 1"/>
      <connect from_op="Set Macros" from_port="through 5" to_op="KnowledgeSerializer" to_port="input 1"/>
      <connect from_op="KnowledgeParser" from_port="output 1" to_op="Write CSV Parsed" to_port="input"/>
      <connect from_op="Write CSV Parsed" from_port="through" to_port="result 1"/>
      <connect from_op="KnowledgeInheritor" from_port="output 1" to_op="Write CSV Inherited" to_port="input"/>
      <connect from_op="Write CSV Inherited" from_port="through" to_port="result 2"/>
      <connect from_op="MACROParser" from_port="output 1" to_op="Write CSV MACRO Parsed" to_port="input"/>
      <connect from_op="Write CSV MACRO Parsed" from_port="through" to_port="result 3"/>
      <connect from_op="MACROInheritor" from_port="output 1" to_op="Write CSV MACRO Inherited" to_port="input"/>
      <connect from_op="Write CSV MACRO Inherited" from_port="through" to_port="result 4"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
      <portSpacing port="sink_result 3" spacing="0"/>
      <portSpacing port="sink_result 4" spacing="0"/>
      <portSpacing port="sink_result 5" spacing="0"/>
      <description align="left" color="yellow" colored="false" height="446" resized="true" width="483" x="651" y="26">Parse all the vocabularies passed from the Excel file, saving Date of scraping, subject, predicate, object, subjectTerm, predicateTerm, objectTerm, domain, domain version, domain date, URI, Title, Languages for every vocabulary's triple&lt;br&gt;It saves this informations in 2 ways, triples containing the full URI path and containing only the last term of it, in the same row&lt;br&gt;It can save only the triples with a given predicate setted in the macro Predicates as a list of URIs or Terms, otherwise, if this macro contains a string of only spaces, it saves every triple of the vocabularies&lt;br&gt;It stores directly into the base folder, given in input with the macro folderDestination, the intermediate results of the parsing&lt;br&gt;It uses the Folder column from the input file to create a Folder for every value, and then every vocabulary the relative Folder&lt;br&gt;For each vocabulary it creates an Excel File with its triples in the desidered way&lt;br&gt;For each vocabulary, serialize it in the formats given in input with the macro Formats, saved in Folder/Resources&lt;br&gt;Furthermore it returns to RapidMiner every obtained triples</description>
      <description align="center" color="blue" colored="true" height="309" resized="true" width="259" x="362" y="99">Read the worksheet of Excel that contains the info about the vocabulatories to transform&lt;br&gt;It must have the first row as: prefix, URI, Title, Languages, VersionName, VersionDate, Link, Folder and then all the vocabularies that you want to parse&lt;br/&gt;This list of knowledge can be used by both Parser and Inheritor&lt;br&gt;</description>
      <description align="center" color="blue" colored="true" height="434" resized="true" width="320" x="10" y="189">Set the macros used by the scripts:&lt;br&gt;- folderDestination: path to the folder destination on which save the outputs of Parser/Inheritor&lt;br&gt;- Predicates: list of predicates and predicateTerms used to filter the Parser/Inheritor outputs&lt;br&gt;- fileName: full path of the name of the file to serialize using the MACROParser, MACROInheritor, or Serializer python module;&lt;br&gt;- Formats: list of the formats that Serializer will use to serialize the given file, separated by spaces &amp;quot; &amp;quot;; the compatible formats are: rdf, ttl, n3, nt, json&lt;br&gt;Then the connection to the scripts is made to ensure that the Set Macros module is executed before the scripts, otherwise there will be errors</description>
      <description align="center" color="green" colored="true" height="397" resized="true" width="209" x="1184" y="232">Store the outputs of Parser and Inheritor as csv files since Excel has a 1048575 row limit and with big lists of vocabularies this limit can be exceeded quite easily</description>
      <description align="center" color="yellow" colored="false" height="218" resized="true" width="215" x="257" y="699">Parse the file given in input with the macro fileName&lt;br/&gt;and &lt;br/&gt;Serialize it in the formats given in input with the macro Formats</description>
      <description align="left" color="yellow" colored="false" height="198" resized="true" width="427" x="708" y="482">Also saves the relative triples for a subject with a subClassOf predicate, hence implementing inheritance of the attributes&lt;br/&gt;Do not serialize the vocabularies</description>
      <description align="center" color="yellow" colored="false" height="285" resized="true" width="288" x="760" y="687">Parser and Inheritor modules that use the MACRO fileName as input to generate the Parsed and Inherited versions of that file</description>
    </process>
  </operator>
</process>
