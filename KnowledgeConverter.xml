<?xml version="1.0" encoding="UTF-8"?><process version="9.2.001">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="9.2.001" expanded="true" name="Process">
    <parameter key="logverbosity" value="init"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read CrossData" width="90" x="179" y="289">
        <parameter key="excel_file" value="/home/marco/Desktop/DBPedia_CrossData.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="total.true.integer.attribute"/>
          <parameter key="1" value="Names.true.polynominal.attribute"/>
          <parameter key="2" value="number.true.integer.attribute"/>
          <parameter key="3" value="Elements.true.polynominal.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="103" name="KnowledgeConverter" width="90" x="447" y="238">
        <parameter key="script" value="# Import libraries&#10;from rdflib import Graph, Literal, RDFS, RDF, OWL, Namespace&#10;import pandas as pd&#10;import os&#10;&#10;# Check if rNames is a subClassOf Names&#10;def checkSub(names, rNames, subsAdded, namesRemaining):&#10;    bool_ = False&#10;    # Split Names and rNames in the different name and rName of relative composition&#10;    nameL = names.split(&quot;_-_&quot;)&#10;    rNameL = rNames.split(&quot;_-_&quot;)&#10;    # Count the number of rName found in rNames&#10;    i = 0&#10;    for name in nameL:&#10;        for rName in rNameL:&#10;            if(name == rName):&#10;                i+=1&#10;    # If the result of i is equal to the number of rName(all elements of rNames are on Names) and there isn't already a bigger subClass&#10;    if(i==len(rNameL) and checkBiggerSub(rNameL, subsAdded)):&#10;        # Set the boolean to true and add rNames as subClass of Names&#10;        bool_ = True&#10;        # Add rNames at the subClasses of Names&#10;        subsAdded.add(rNames)&#10;        # Remove the components of rNames from the components remaining&#10;        for rName in rNameL:&#10;            if(rName in namesRemaining):&#10;                namesRemaining.remove(rName)&#10;    # Return the bool and the modified sets&#10;    return bool_, subsAdded, namesRemaining&#10;&#10;# Check if subj is a subClassOf names&#10;def checkBiggerSub(rNameL, subsAdded):&#10;    # Iterate over every already added subClasses of Names&#10;    for subj in subsAdded:&#10;        # Count the number of rName found in subj&#10;        i = 0&#10;        for s in subj.split(&quot;_-_&quot;):&#10;            for rName in rNameL:&#10;                if(s == rName):&#10;                    i+=1&#10;        # If the result of i is equal to the number of rName(all elements of rNames are on an already added subClass of Names)&#10;        if(i == len(rNameL)):&#10;            # Return False since a bigger subClass has already been added to Names&#10;            return False&#10;    # Return True since there aren't subClasses of Names that covers all these components of Names&#10;    return True&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(data):&#10;    # Create the graph used to store the vocabulary&#10;    g = Graph()&#10;    # Create the Namespace for the vocabulary&#10;    n = Namespace(&quot;http://www.liveschema.org/test/&quot;)&#10;    g.bind(&quot;liveschema_test&quot;, n)&#10;    &#10;    # Create the DataFrame used to save the triples&#10;    triples = pd.DataFrame(columns=[&quot;Subject&quot;,&quot;Predicate&quot;, &quot;Object&quot;])&#10;&#10;    # Sort the DataFrame&#10;    data = data.sort_values(&quot;total&quot;, ascending=False)&#10;    # Iterate for every row present on data&#10;    for index, row in data.iterrows():&#10;        # Format Names&#10;        names = row[&quot;Names&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot;,&quot;, &quot;_-_&quot;)&#10;        # Split Names in the different name of its composition&#10;        nameList = names.split(&quot;_-_&quot;)&#10;        # Comment the Names&#10;        for name in nameList:&#10;            # Save the triple about Names having as comments the various name of which it is composed&#10;            triples = triples.append({&quot;Subject&quot;: names, &quot;Predicate&quot;: &quot;comment&quot;, &quot;Object&quot;: name}, ignore_index=True)&#10;            g.add((n[names], RDFS.comment, Literal(name)))&#10;&#10;        # Create set to contain the different subClasses of Names&#10;        subsAdded = set()&#10;        # Create set to contain the remaining single name of Names&#10;        namesRemaining = set()&#10;        namesRemaining.update(nameList)&#10;        # Iterate over every item with less members in Names until all the name in Names has been covered&#10;        for i, r in data.iterrows():&#10;            # If Names in that row has less members than the starting Names, and not all the name in Names has been covered&#10;            if(r[&quot;total&quot;] &lt; row[&quot;total&quot;] and len(namesRemaining) != 0):&#10;                # Format the new row Names: rNames&#10;                rNames = r[&quot;Names&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot;,&quot;, &quot;_-_&quot;)&#10;                # Check if rNames can be a subClassOf Names&#10;                bool_, subsAdded, namesRemaining = checkSub(names, rNames, subsAdded, namesRemaining)&#10;                if(bool_):&#10;                    # Save the triple about rNames being subClassOf Names&#10;                    triples = triples.append({&quot;Subject&quot;: rNames, &quot;Predicate&quot;: &quot;subClassOf&quot;, &quot;Object&quot;: names}, ignore_index=True)&#10;                    g.add((n[rNames], RDFS.subClassOf, n[names]))&#10;        # If the its a composition of at least 2 name, then add the remaining name as subClassOf&#10;        if(len(nameList)&gt;1):&#10;            # Iterate over any remaining name&#10;            for sub in namesRemaining:&#10;                # Save the triple about the single name being subClassOf Names&#10;                triples = triples.append({&quot;Subject&quot;: sub, &quot;Predicate&quot;: &quot;subClassOf&quot;, &quot;Object&quot;: names}, ignore_index=True)&#10;                g.add((n[sub], RDFS.subClassOf, n[names]))&#10;&#10;        # Map every element into its domain&#10;        elements = row[&quot;Elements&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).split(&quot;,&quot;)&#10;        #print(elements)&#10;        for element in elements:&#10;            # Save the triple about the element being an ObjectProperty&#10;            triples = triples.append({&quot;Subject&quot;: element, &quot;Predicate&quot;: &quot;type&quot;, &quot;Object&quot;: &quot;ObjectProperty&quot;}, ignore_index=True)&#10;            g.add((n[element], RDF.type, OWL.ObjectProperty))&#10;            # Save the triple about the element being a domain of that Names&#10;            triples = triples.append({&quot;Subject&quot;: element, &quot;Predicate&quot;: &quot;domain&quot;, &quot;Object&quot;: names}, ignore_index=True)&#10;            g.add((n[element], RDFS.domain, n[names]))&#10;&#10;    # Create the directory in which store the new vocabulary&#10;    location = os.path.normpath(os.path.expanduser(&quot;~/Desktop/K-Files/Converted/&quot;))&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;    # Serialize the new vocabulary&#10;    g.serialize(destination=str(os.path.join(location, &quot;test.rdf&quot;)), format=&quot;pretty-xml&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.n3&quot;)), format=&quot;n3&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.nt&quot;)), format=&quot;nt&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.ttl&quot;)), format=&quot;turtle&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.json-ld&quot;)), format=&quot;json-ld&quot;)&#10;&#10;    # Return the triples DataFrame for RapidMiner usage&#10;    return triples"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_csv" compatibility="9.2.001" expanded="true" height="82" name="Write CSV" width="90" x="648" y="238">
        <parameter key="csv_file" value="C:\Users\marco\Desktop\K-Files\Converted\Result.csv"/>
        <parameter key="column_separator" value=";"/>
        <parameter key="write_attribute_names" value="true"/>
        <parameter key="quote_nominal_values" value="true"/>
        <parameter key="format_date_attributes" value="true"/>
        <parameter key="append_to_file" value="false"/>
        <parameter key="encoding" value="SYSTEM"/>
      </operator>
      <connect from_op="Read CrossData" from_port="output" to_op="KnowledgeConverter" to_port="input 1"/>
      <connect from_op="KnowledgeConverter" from_port="output 1" to_op="Write CSV" to_port="input"/>
      <connect from_op="Write CSV" from_port="through" to_port="result 1"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
      <description align="left" color="yellow" colored="false" height="166" resized="true" width="183" x="402" y="353">Convert the CrossData obtained from the KnowledgeAnalyser and generate the relative new vocabulary in various formats into the folder Desktop/K-Files/Converted</description>
    </process>
  </operator>
</process>
