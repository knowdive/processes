<?xml version="1.0" encoding="UTF-8"?><process version="9.2.001">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="9.2.001" expanded="true" name="Process">
    <parameter key="logverbosity" value="init"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="false" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read CrossData" width="90" x="179" y="85">
        <parameter key="excel_file" value="/home/marco/Desktop/Schema_CrossData.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="total.true.integer.attribute"/>
          <parameter key="1" value="Names.true.polynominal.attribute"/>
          <parameter key="2" value="number.true.integer.attribute"/>
          <parameter key="3" value="Elements.true.polynominal.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
      </operator>
      <operator activated="false" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="103" name="Knowledge Converter" width="90" x="447" y="85">
        <parameter key="script" value="# Import libraries&#10;from rdflib import Graph, Literal, RDFS, RDF, OWL, Namespace&#10;import pandas as pd&#10;import os&#10;&#10;# Check if rNames is a subClassOf Names&#10;def checkSub(names, rNames, subsAdded, namesRemaining):&#10;    bool_ = False&#10;    # Split Names and rNames in the different name and rName of relative composition&#10;    nameL = names.split(&quot;_-_&quot;)&#10;    rNameL = rNames.split(&quot;_-_&quot;)&#10;    # Count the number of rName found in rNames&#10;    i = 0&#10;    for name in nameL:&#10;        for rName in rNameL:&#10;            if(name == rName):&#10;                i+=1&#10;    # If the result of i is equal to the number of rName(all elements of rNames are on Names) and there isn't already a bigger subClass&#10;    if(i==len(rNameL) and checkBiggerSub(rNameL, subsAdded)):&#10;        # Set the boolean to true and add rNames as subClass of Names&#10;        bool_ = True&#10;        # Add rNames at the subClasses of Names&#10;        subsAdded.add(rNames)&#10;        # Remove the components of rNames from the components remaining&#10;        for rName in rNameL:&#10;            if(rName in namesRemaining):&#10;                namesRemaining.remove(rName)&#10;    # Return the bool and the modified sets&#10;    return bool_, subsAdded, namesRemaining&#10;&#10;# Check if subj is a subClassOf names&#10;def checkBiggerSub(rNameL, subsAdded):&#10;    # Iterate over every already added subClasses of Names&#10;    for subj in subsAdded:&#10;        # Count the number of rName found in subj&#10;        i = 0&#10;        for s in subj.split(&quot;_-_&quot;):&#10;            for rName in rNameL:&#10;                if(s == rName):&#10;                    i+=1&#10;        # If the result of i is equal to the number of rName(all elements of rNames are on an already added subClass of Names)&#10;        if(i == len(rNameL)):&#10;            # Return False since a bigger subClass has already been added to Names&#10;            return False&#10;    # Return True since there aren't subClasses of Names that covers all these components of Names&#10;    return True&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(data):&#10;    # Create the graph used to store the vocabulary&#10;    g = Graph()&#10;    # Create the Namespace for the vocabulary&#10;    n = Namespace(&quot;http://www.liveschema.org/test/&quot;)&#10;    g.bind(&quot;liveschema_test&quot;, n)&#10;    &#10;    # Create the DataFrame used to save the triples&#10;    triples = pd.DataFrame(columns=[&quot;Subject&quot;,&quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;,&quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;])&#10;&#10;    # Sort the DataFrame&#10;    data = data.sort_values(&quot;total&quot;, ascending=False)&#10;    # Iterate for every row present on data&#10;    for index, row in data.iterrows():&#10;        # Format Names&#10;        names = row[&quot;Names&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot;,&quot;, &quot;_-_&quot;)&#10;        # Split Names in the different name of its composition&#10;        nameList = names.split(&quot;_-_&quot;)&#10;        # Comment the Names&#10;        for name in nameList:&#10;            # Save the triple about Names having as comments the various name of which it is composed&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[names]), &quot;Predicate&quot;: str(RDFS.comment), &quot;Object&quot;: str(Literal(name)), &quot;SubjectTerm&quot;: names, &quot;PredicateTerm&quot;: &quot;comment&quot;, &quot;ObjectTerm&quot;: name}, ignore_index=True)&#10;            g.add((n[names], RDFS.comment, Literal(name)))&#10;&#10;        # Create set to contain the different subClasses of Names&#10;        subsAdded = set()&#10;        # Create set to contain the remaining single name of Names&#10;        namesRemaining = set()&#10;        namesRemaining.update(nameList)&#10;        # Iterate over every item with less members in Names until all the name in Names has been covered&#10;        for i, r in data.iterrows():&#10;            # If Names in that row has less members than the starting Names, and not all the name in Names has been covered&#10;            if(r[&quot;total&quot;] &lt; row[&quot;total&quot;] and len(namesRemaining) != 0):&#10;                # Format the new row Names: rNames&#10;                rNames = r[&quot;Names&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot;,&quot;, &quot;_-_&quot;)&#10;                # Check if rNames can be a subClassOf Names&#10;                bool_, subsAdded, namesRemaining = checkSub(names, rNames, subsAdded, namesRemaining)&#10;                if(bool_):&#10;                    # Save the triple about rNames being subClassOf Names&#10;                    triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[rNames]), &quot;Predicate&quot;: str(RDFS.subClassOf), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: rNames, &quot;PredicateTerm&quot;: &quot;subClassOf&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;                    g.add((n[rNames], RDFS.subClassOf, n[names]))&#10;        # If the its a composition of at least 2 name, then add the remaining name as subClassOf&#10;        if(len(nameList)&gt;1):&#10;            # Iterate over any remaining name&#10;            for sub in namesRemaining:&#10;                # Save the triple about the single name being subClassOf Names&#10;                triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[sub]), &quot;Predicate&quot;: str(RDFS.subClassOf), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: sub, &quot;PredicateTerm&quot;: &quot;subClassOf&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;                g.add((n[sub], RDFS.subClassOf, n[names]))&#10;&#10;        # Map every element into its domain&#10;        elements = row[&quot;Elements&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).split(&quot;,&quot;)&#10;        #print(elements)&#10;        for element in elements:&#10;            # Save the triple about the element being an ObjectProperty&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: str(RDF.type), &quot;Object&quot;: str(OWL.ObjectProperty), &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: &quot;type&quot;, &quot;ObjectTerm&quot;: &quot;ObjectProperty&quot;}, ignore_index=True)&#10;            g.add((n[element], RDF.type, OWL.ObjectProperty))&#10;            # Save the triple about the element being a domain of that Names&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: str(RDFS.domain), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: &quot;domain&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;            g.add((n[element], RDFS.domain, n[names]))&#10;&#10;    # Create the directory in which store the new vocabulary&#10;    location = os.path.normpath(os.path.expanduser(&quot;~/Desktop/K-Files/Converted/&quot;))&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;    # Serialize the new vocabulary&#10;    g.serialize(destination=str(os.path.join(location, &quot;test.rdf&quot;)), format=&quot;pretty-xml&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.n3&quot;)), format=&quot;n3&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.nt&quot;)), format=&quot;nt&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.ttl&quot;)), format=&quot;turtle&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.json-ld&quot;)), format=&quot;json-ld&quot;)&#10;&#10;    # Return the triples DataFrame for RapidMiner usage&#10;    return triples"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="false" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Excel" width="90" x="715" y="85">
        <parameter key="excel_file" value="/home/marco/Desktop/K-Files/Converted/test.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read CrossData (2)" width="90" x="179" y="238">
        <parameter key="excel_file" value="/home/marco/Desktop/CIS_CrossData.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="total.true.integer.attribute"/>
          <parameter key="1" value="Names.true.polynominal.attribute"/>
          <parameter key="2" value="number.true.integer.attribute"/>
          <parameter key="3" value="Elements.true.polynominal.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
      </operator>
      <operator activated="true" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read Knowledge Parsed" width="90" x="179" y="340">
        <parameter key="excel_file" value="/home/marco/Desktop/Inh_CIS.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="Date.true.polynominal.attribute"/>
          <parameter key="1" value="Subject.true.polynominal.attribute"/>
          <parameter key="2" value="Predicate.true.polynominal.attribute"/>
          <parameter key="3" value="Object.true.polynominal.attribute"/>
          <parameter key="4" value="SubjectTerm.true.polynominal.attribute"/>
          <parameter key="5" value="PredicateTerm.true.polynominal.attribute"/>
          <parameter key="6" value="ObjectTerm.true.polynominal.attribute"/>
          <parameter key="7" value="Domain.true.polynominal.attribute"/>
          <parameter key="8" value="Domain Version.true.polynominal.attribute"/>
          <parameter key="9" value="Domain Date.true.polynominal.attribute"/>
          <parameter key="10" value="URI.true.polynominal.attribute"/>
          <parameter key="11" value="Title.true.polynominal.attribute"/>
          <parameter key="12" value="Languages.true.polynominal.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
      </operator>
      <operator activated="true" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read Predicates" width="90" x="45" y="442">
        <parameter key="excel_file" value="/home/marco/Desktop/Predicate.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="PredicateTerm.true.polynominal.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
        <description align="center" color="transparent" colored="false" width="126">Optional, used to filter the triples to add to the Converted File</description>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="145" name="Knowledge Converter Full" width="90" x="447" y="238">
        <parameter key="script" value="# Import libraries&#10;from rdflib import Graph, Literal, RDFS, RDF, OWL, Namespace, URIRef&#10;import pandas as pd&#10;import os&#10;&#10;# Check if rNames is a subClassOf Names&#10;def checkSub(names, rNames, subsAdded, namesRemaining):&#10;    bool_ = False&#10;    # Split Names and rNames in the different name and rName of relative composition&#10;    nameL = names.split(&quot;_-_&quot;)&#10;    rNameL = rNames.split(&quot;_-_&quot;)&#10;    # Count the number of rName found in rNames&#10;    i = 0&#10;    for name in nameL:&#10;        for rName in rNameL:&#10;            if(name == rName):&#10;                i+=1&#10;    # If the result of i is equal to the number of rName(all elements of rNames are on Names) and there isn't already a bigger subClass&#10;    if(i==len(rNameL) and checkBiggerSub(rNameL, subsAdded)):&#10;        # Set the boolean to true and add rNames as subClass of Names&#10;        bool_ = True&#10;        # Add rNames at the subClasses of Names&#10;        subsAdded.add(rNames)&#10;        # Remove the components of rNames from the components remaining&#10;        for rName in rNameL:&#10;            if(rName in namesRemaining):&#10;                namesRemaining.remove(rName)&#10;    # Return the bool and the modified sets&#10;    return bool_, subsAdded, namesRemaining&#10;&#10;# Check if subj is a subClassOf names&#10;def checkBiggerSub(rNameL, subsAdded):&#10;    # Iterate over every already added subClasses of Names&#10;    for subj in subsAdded:&#10;        # Count the number of rName found in subj&#10;        i = 0&#10;        for s in subj.split(&quot;_-_&quot;):&#10;            for rName in rNameL:&#10;                if(s == rName):&#10;                    i+=1&#10;        # If the result of i is equal to the number of rName(all elements of rNames are on an already added subClass of Names)&#10;        if(i == len(rNameL)):&#10;            # Return False since a bigger subClass has already been added to Names&#10;            return False&#10;    # Return True since there aren't subClasses of Names that covers all these components of Names&#10;    return True&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(data, orig, predicates = pd.DataFrame()):&#10;    # Create the graph used to store the vocabulary&#10;    g = Graph()&#10;    # Create the Namespace for the vocabulary&#10;    n = Namespace(&quot;http://www.liveschema.org/test/&quot;)&#10;    g.bind(&quot;liveschema_test&quot;, n)&#10;    &#10;    # Create the DataFrame used to save the triples&#10;    triples = pd.DataFrame(columns=[&quot;Subject&quot;,&quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;,&quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;])&#10;&#10;    # Sets used to avoid adding 2 equal rows in the DataFrame&#10;    nameSet = set()&#10;    elementSet = set()&#10;&#10;    # Sort the DataFrame&#10;    data = data.sort_values(&quot;total&quot;, ascending=False)&#10;    # Iterate for every row present on data&#10;    for index, row in data.iterrows():&#10;        # Format Names&#10;        names = row[&quot;Names&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot;,&quot;, &quot;_-_&quot;)&#10;        # Split Names in the different name of its composition&#10;        nameList = names.split(&quot;_-_&quot;)&#10;        # Comment the Names&#10;        for name in nameList:&#10;            # Save the triple about Names having as comments the various name of which it is composed&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[names]), &quot;Predicate&quot;: str(RDFS.comment), &quot;Object&quot;: str(Literal(name)), &quot;SubjectTerm&quot;: names, &quot;PredicateTerm&quot;: &quot;comment&quot;, &quot;ObjectTerm&quot;: name}, ignore_index=True)&#10;            g.add((n[names], RDFS.comment, Literal(name)))&#10;&#10;        # Create set to contain the different subClasses of Names&#10;        subsAdded = set()&#10;        # Create set to contain the remaining single name of Names&#10;        namesRemaining = set()&#10;        namesRemaining.update(nameList)&#10;        # Iterate over every item with less members in Names until all the name in Names has been covered&#10;        for i, r in data.iterrows():&#10;            # If Names in that row has less members than the starting Names, and not all the name in Names has been covered&#10;            if(r[&quot;total&quot;] &lt; row[&quot;total&quot;] and len(namesRemaining) != 0):&#10;                # Format the new row Names: rNames&#10;                rNames = r[&quot;Names&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot;,&quot;, &quot;_-_&quot;)&#10;                # Check if rNames can be a subClassOf Names&#10;                bool_, subsAdded, namesRemaining = checkSub(names, rNames, subsAdded, namesRemaining)&#10;                if(bool_):&#10;                    # Save the triple about rNames being subClassOf Names&#10;                    triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[rNames]), &quot;Predicate&quot;: str(RDFS.subClassOf), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: rNames, &quot;PredicateTerm&quot;: &quot;subClassOf&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;                    g.add((n[rNames], RDFS.subClassOf, n[names]))&#10;        # If the its a composition of at least 2 name, then add the remaining name as subClassOf&#10;        if(len(nameList)&gt;1):&#10;            # Iterate over any remaining name&#10;            for sub in namesRemaining:&#10;                # Save the triple about the single name being subClassOf Names&#10;                triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[sub]), &quot;Predicate&quot;: str(RDFS.subClassOf), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: sub, &quot;PredicateTerm&quot;: &quot;subClassOf&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;                g.add((n[sub], RDFS.subClassOf, n[names]))&#10;&#10;        # Map every element into its domain&#10;        elements = row[&quot;Elements&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).split(&quot;,&quot;)&#10;        for element in elements:&#10;            # Save the triple about the element being an ObjectProperty&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: str(RDF.type), &quot;Object&quot;: str(OWL.ObjectProperty), &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: &quot;type&quot;, &quot;ObjectTerm&quot;: &quot;ObjectProperty&quot;}, ignore_index=True)&#10;            g.add((n[element], RDF.type, OWL.ObjectProperty))&#10;            # Save the triple about the element being a domain of that Names&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: str(RDFS.domain), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: &quot;domain&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;            g.add((n[element], RDFS.domain, n[names]))&#10;&#10;        # Complete the file and excel with the original vocabulary&#10;        # Iterate over every item of the original vocabulary&#10;        for i, r in orig.iterrows():&#10;            # Check if the triple has to be saved, if there is a predicate selection then checks if that predicate has to be saved&#10;            bool_ = False&#10;            # If there is no predicate selection then save every triple&#10;            if(len(predicates) == 0):&#10;                bool_ = True&#10;            # If there is a predicate selection then check if that predicate has to be saved&#10;            else:&#10;                for pred in predicates[predicates.columns[0]]:&#10;                    if(pred == str(r[&quot;PredicateTerm&quot;]) or pred == str(r[&quot;Predicate&quot;])):&#10;                        bool_ = True&#10;                        break&#10;            # Check if the triple has to be saved&#10;            if(bool_ == True):&#10;                # Save the original name triples on the new graph&#10;                for name in nameList:&#10;                    nameSet.add(name+r[&quot;PredicateTerm&quot;]+r[&quot;ObjectTerm&quot;])&#10;                    # If the name is an original subject then try to save it&#10;                    if(name == r[&quot;SubjectTerm&quot;]):&#10;                        # Save the triple about the name being a SubjectTerm&#10;                        triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[name]), &quot;Predicate&quot;: &quot; &quot;+str(n[&quot;isA&quot;]), &quot;Object&quot;: r[&quot;Subject&quot;], &quot;SubjectTerm&quot;: name, &quot;PredicateTerm&quot;: &quot;isA&quot;, &quot;ObjectTerm&quot;: r[&quot;SubjectTerm&quot;]}, ignore_index=True)&#10;                        g.add((n[name], n[&quot;isA&quot;], Literal(r[&quot;Subject&quot;])))&#10;                        # Save the triple about the name becoming the SubjectTerm in that triple&#10;                        triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[name]), &quot;Predicate&quot;: r[&quot;Predicate&quot;], &quot;Object&quot;: r[&quot;Object&quot;], &quot;SubjectTerm&quot;: name, &quot;PredicateTerm&quot;: r[&quot;PredicateTerm&quot;], &quot;ObjectTerm&quot;: r[&quot;ObjectTerm&quot;]}, ignore_index=True)&#10;                        g.add((n[name], URIRef(r[&quot;Predicate&quot;]), Literal(r[&quot;Object&quot;])))&#10;&#10;                # Save the original element triples on the new graph&#10;                for element in elements:&#10;                    elementSet.add(element+r[&quot;PredicateTerm&quot;]+r[&quot;ObjectTerm&quot;])&#10;                    # If the element is a part of the original then try to save it&#10;                    if(element in r[&quot;SubjectTerm&quot;].lower()):&#10;                        term = r[&quot;SubjectTerm&quot;]&#10;                        a = len(term)&#10;                        # Check on which word element is connected with SubjectTerm, and check that are the same words&#10;                        for i in range(a-1, -1, -1):&#10;                            if(term[i].isupper() or i == 0):&#10;                                if(term[i:a].lower() == element):&#10;                                    # Save the triple about the element being a SubjectTerm&#10;                                    triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: &quot; &quot;+str(n[&quot;isA&quot;]), &quot;Object&quot;: r[&quot;Subject&quot;], &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: &quot;isA&quot;, &quot;ObjectTerm&quot;: r[&quot;SubjectTerm&quot;]}, ignore_index=True)&#10;                                    g.add((n[element], n[&quot;isA&quot;], Literal(r[&quot;Subject&quot;])))&#10;                                    # Save the triple about the element becoming the SubjectTerm in that triple&#10;                                    triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: r[&quot;Predicate&quot;], &quot;Object&quot;: r[&quot;Object&quot;], &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: r[&quot;PredicateTerm&quot;], &quot;ObjectTerm&quot;: r[&quot;ObjectTerm&quot;]}, ignore_index=True)&#10;                                    g.add((n[element], URIRef(r[&quot;Predicate&quot;]), Literal(r[&quot;Object&quot;])))&#10;                                # Update the index&#10;                                a = i            &#10;&#10;    # Create the directory in which store the new vocabulary&#10;    location = os.path.normpath(os.path.expanduser(&quot;~/Desktop/K-Files/Converted/&quot;))&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;    # Serialize the new vocabulary&#10;    g.serialize(destination=str(os.path.join(location, &quot;test.rdf&quot;)), format=&quot;pretty-xml&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.n3&quot;)), format=&quot;n3&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.nt&quot;)), format=&quot;nt&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.ttl&quot;)), format=&quot;turtle&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.json-ld&quot;)), format=&quot;json-ld&quot;)&#10;&#10;    # Return the triples DataFrame for RapidMiner usage&#10;    return triples"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Excel (2)" width="90" x="715" y="238">
        <parameter key="excel_file" value="/home/marco/Desktop/K-Files/Converted/test.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <connect from_op="Read CrossData" from_port="output" to_op="Knowledge Converter" to_port="input 1"/>
      <connect from_op="Knowledge Converter" from_port="output 1" to_op="Write Excel" to_port="input"/>
      <connect from_op="Read CrossData (2)" from_port="output" to_op="Knowledge Converter Full" to_port="input 1"/>
      <connect from_op="Read Knowledge Parsed" from_port="output" to_op="Knowledge Converter Full" to_port="input 2"/>
      <connect from_op="Read Predicates" from_port="output" to_op="Knowledge Converter Full" to_port="input 3"/>
      <connect from_op="Knowledge Converter Full" from_port="output 1" to_op="Write Excel (2)" to_port="input"/>
      <connect from_op="Write Excel (2)" from_port="through" to_port="result 1"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
      <description align="left" color="yellow" colored="false" height="166" resized="true" width="183" x="420" y="553">Convert the CrossData obtained from the KnowledgeAnalyser and generate the relative new vocabulary in various formats into the folder Desktop/K-Files/Converted</description>
    </process>
  </operator>
</process>
