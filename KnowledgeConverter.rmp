<?xml version="1.0" encoding="UTF-8"?><process version="9.2.001">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="9.2.001" expanded="true" name="Process">
    <parameter key="logverbosity" value="init"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read CrossData" width="90" x="246" y="238">
        <parameter key="excel_file" value="/home/marco/C:\Users\marco\Desktop\analysis-step\CrossData.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="total.true.integer.attribute"/>
          <parameter key="1" value="Names.true.polynominal.attribute"/>
          <parameter key="2" value="number.true.integer.attribute"/>
          <parameter key="3" value="Elements.true.polynominal.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
      </operator>
      <operator activated="true" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read Knowledge Parsed" width="90" x="313" y="493">
        <parameter key="excel_file" value="/home/marco/Desktop/Inh_OWL.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="Date.true.polynominal.attribute"/>
          <parameter key="1" value="Subject.true.polynominal.attribute"/>
          <parameter key="2" value="Predicate.true.polynominal.attribute"/>
          <parameter key="3" value="Object.true.polynominal.attribute"/>
          <parameter key="4" value="SubjectTerm.true.polynominal.attribute"/>
          <parameter key="5" value="PredicateTerm.true.polynominal.attribute"/>
          <parameter key="6" value="ObjectTerm.true.polynominal.attribute"/>
          <parameter key="7" value="Domain.true.polynominal.attribute"/>
          <parameter key="8" value="Domain Version.true.polynominal.attribute"/>
          <parameter key="9" value="Domain Date.true.polynominal.attribute"/>
          <parameter key="10" value="URI.true.polynominal.attribute"/>
          <parameter key="11" value="Title.true.polynominal.attribute"/>
          <parameter key="12" value="Languages.true.polynominal.attribute"/>
          <parameter key="13" value="Inherited.true.integer.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
      </operator>
      <operator activated="true" class="set_macros" compatibility="9.2.001" expanded="true" height="103" name="Set Macros" width="90" x="45" y="340">
        <list key="macros">
          <parameter key="NameSpace" value="http://liveschema.org/test"/>
          <parameter key="fileDestination" value="~/Desktop/K-Files/Converted/testConverted.ttl"/>
          <parameter key="Predicates" value="  "/>
        </list>
      </operator>
      <operator activated="true" class="multiply" compatibility="9.2.001" expanded="true" height="103" name="Multiply" width="90" x="380" y="238"/>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="145" name="Knowledge Converter Full" width="90" x="648" y="442">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;from rdflib import Graph, Literal, RDFS, RDF, OWL, Namespace, URIRef&#10;import os&#10;&#10;# Check if rNames is a subClassOf Names&#10;def checkSub(names, rNames, subsAdded, namesRemaining):&#10;    bool_ = False&#10;    # Split Names and rNames in the different name and rName of relative composition&#10;    nameL = names.split(&quot;_-_&quot;)&#10;    rNameL = rNames.split(&quot;_-_&quot;)&#10;    # Count the number of rName found in rNames&#10;    i = 0&#10;    for name in nameL:&#10;        for rName in rNameL:&#10;            if(name == rName):&#10;                i+=1&#10;    # If the result of i is equal to the number of rName(all elements of rNames are on Names) and there isn't already a bigger subClass&#10;    if(i==len(rNameL) and checkBiggerSub(rNameL, subsAdded)):&#10;        # Set the boolean to true and add rNames as subClass of Names&#10;        bool_ = True&#10;        # Add rNames at the subClasses of Names&#10;        subsAdded.add(rNames)&#10;        # Remove the components of rNames from the components remaining&#10;        for rName in rNameL:&#10;            if(rName in namesRemaining):&#10;                namesRemaining.remove(rName)&#10;    # Return the bool and the modified sets&#10;    return bool_, subsAdded, namesRemaining&#10;&#10;# Check if subj is a subClassOf names&#10;def checkBiggerSub(rNameL, subsAdded):&#10;    # Iterate over every already added subClasses of Names&#10;    for subj in subsAdded:&#10;        # Count the number of rName found in subj&#10;        i = 0&#10;        for s in subj.split(&quot;_-_&quot;):&#10;            for rName in rNameL:&#10;                if(s == rName):&#10;                    i+=1&#10;        # If the result of i is equal to the number of rName(all elements of rNames are on an already added subClass of Names)&#10;        if(i == len(rNameL)):&#10;            # Return False since a bigger subClass has already been added to Names&#10;            return False&#10;    # Return True since there aren't subClasses of Names that covers all these components of Names&#10;    return True&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(data, orig):&#10;    # Create the graph used to store the vocabulary&#10;    g = Graph()&#10;    # Create the Namespace for the vocabulary&#10;    strNameSpace = &quot;%{NameSpace}&quot;&#10;    n = Namespace(strNameSpace)&#10;    g.bind(strNameSpace.split(&quot;/&quot;)[-1], n)&#10;    &#10;    # Create the DataFrame used to save the triples&#10;    triples = pd.DataFrame(columns=[&quot;Subject&quot;,&quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;,&quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;])&#10;&#10;    # Sets used to avoid adding 2 equal rows in the DataFrame&#10;    nameSet = set()&#10;    elementSet = set()&#10;&#10;    # Sort the DataFrame&#10;    data = data.sort_values(&quot;total&quot;, ascending=False)&#10;    # Iterate for every row present on data&#10;    for index, row in data.iterrows():&#10;        # Format Names&#10;        names = row[&quot;Names&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot;,&quot;, &quot;_-_&quot;)&#10;        # Save a new triple about Names having as label a new concept&#10;        triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[names]), &quot;Predicate&quot;: str(RDFS.label), &quot;Object&quot;: str(Literal(&quot;concept#&quot;+str(index))), &quot;SubjectTerm&quot;: names, &quot;PredicateTerm&quot;: &quot;comment&quot;, &quot;ObjectTerm&quot;: &quot;concept#&quot;+str(index)}, ignore_index=True)&#10;        g.add((n[names], RDFS.label, Literal(&quot;concept#&quot;+str(index))))&#10;        # Split Names in the different name of its composition&#10;        nameList = names.split(&quot;_-_&quot;)&#10;        # Label the Names&#10;        for name in nameList:&#10;            # Save the triple about Names having as labels the various name of which it is composed&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[names]), &quot;Predicate&quot;: str(RDFS.label), &quot;Object&quot;: str(Literal(name)), &quot;SubjectTerm&quot;: names, &quot;PredicateTerm&quot;: &quot;comment&quot;, &quot;ObjectTerm&quot;: name}, ignore_index=True)&#10;            g.add((n[names], RDFS.label, Literal(name)))&#10;&#10;        # Create set to contain the different subClasses of Names&#10;        subsAdded = set()&#10;        # Create set to contain the remaining single name of Names&#10;        namesRemaining = set()&#10;        namesRemaining.update(nameList)&#10;        # Iterate over every item with less members in Names until all the name in Names has been covered&#10;        for i, r in data.iterrows():&#10;            # If Names in that row has less members than the starting Names, and not all the name in Names has been covered&#10;            if(r[&quot;total&quot;] &lt; row[&quot;total&quot;] and len(namesRemaining) != 0):&#10;                # Format the new row Names: rNames&#10;                rNames = r[&quot;Names&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot;,&quot;, &quot;_-_&quot;)&#10;                # Check if rNames can be a subClassOf Names&#10;                bool_, subsAdded, namesRemaining = checkSub(names, rNames, subsAdded, namesRemaining)&#10;                if(bool_):&#10;                    # Save the triple about rNames being subClassOf Names&#10;                    triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[rNames]), &quot;Predicate&quot;: str(RDFS.subClassOf), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: rNames, &quot;PredicateTerm&quot;: &quot;subClassOf&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;                    g.add((n[rNames], RDFS.subClassOf, n[names]))&#10;        # If the its a composition of at least 2 name, then add the remaining name as subClassOf&#10;        if(len(nameList)&gt;1):&#10;            # Iterate over any remaining name&#10;            for sub in namesRemaining:&#10;                # Save the triple about the single name being subClassOf Names&#10;                triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[sub]), &quot;Predicate&quot;: str(RDFS.subClassOf), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: sub, &quot;PredicateTerm&quot;: &quot;subClassOf&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;                g.add((n[sub], RDFS.subClassOf, n[names]))&#10;&#10;        # Map every element into its domain&#10;        elements = row[&quot;Elements&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).split(&quot;,&quot;)&#10;        for element in elements:&#10;            # Save the triple about the element being an ObjectProperty&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: str(RDF.type), &quot;Object&quot;: str(OWL.ObjectProperty), &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: &quot;type&quot;, &quot;ObjectTerm&quot;: &quot;ObjectProperty&quot;}, ignore_index=True)&#10;            g.add((n[element], RDF.type, OWL.ObjectProperty))&#10;            # Save the triple about the element being a domain of that Names&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: str(RDFS.domain), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: &quot;domain&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;            g.add((n[element], RDFS.domain, n[names]))&#10;&#10;        # Complete the file and excel with the original vocabulary&#10;        # Iterate over every item of the original vocabulary&#10;        for i, r in orig.iterrows():&#10;            # Check if the triple has to be saved, if there is a predicate selection then checks if that predicate has to be saved&#10;            bool_ = False&#10;            # If there is no predicate selection then save every triple&#10;            strPredicates = &quot;%{Predicates}&quot;&#10;            if(len(strPredicates.split()) == 0):&#10;                bool_ = True&#10;            # If there is a predicate selection then check if that predicate has to be saved&#10;            else:&#10;                for pred in strPredicates.split():&#10;                    if(pred == str(r[&quot;PredicateTerm&quot;]) or pred == str(r[&quot;Predicate&quot;])):&#10;                        bool_ = True&#10;                        break&#10;            # Check if the triple has to be saved&#10;            if(bool_ == True):&#10;                # Save the original name triples on the new graph&#10;                for name in nameList:&#10;                    nameSet.add(name+r[&quot;PredicateTerm&quot;]+r[&quot;ObjectTerm&quot;])&#10;                    # If the name is an original subject then try to save it&#10;                    if(name == r[&quot;SubjectTerm&quot;]):&#10;                        # Save the triple about the name being a SubjectTerm&#10;                        triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[name]), &quot;Predicate&quot;: &quot; &quot;+str(n[&quot;isA&quot;]), &quot;Object&quot;: r[&quot;Subject&quot;], &quot;SubjectTerm&quot;: name, &quot;PredicateTerm&quot;: &quot;isA&quot;, &quot;ObjectTerm&quot;: r[&quot;SubjectTerm&quot;]}, ignore_index=True)&#10;                        g.add((n[name], n[&quot;isA&quot;], URIRef(r[&quot;Subject&quot;])))&#10;                        # Save the triple about the name becoming the SubjectTerm in that triple&#10;                        triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[name]), &quot;Predicate&quot;: r[&quot;Predicate&quot;], &quot;Object&quot;: r[&quot;Object&quot;], &quot;SubjectTerm&quot;: name, &quot;PredicateTerm&quot;: r[&quot;PredicateTerm&quot;], &quot;ObjectTerm&quot;: r[&quot;ObjectTerm&quot;]}, ignore_index=True)&#10;                        # Add the triple to the graph as URIRef or Literal respectively&#10;                        if(len(str(r[&quot;Object&quot;])) &gt; 5 and &quot;http&quot; == r[&quot;Object&quot;][0:3]):&#10;                            g.add((n[name], URIRef(r[&quot;Predicate&quot;]), URIRef(r[&quot;Object&quot;])))&#10;                        else:&#10;                            g.add((n[name], URIRef(r[&quot;Predicate&quot;]), Literal(r[&quot;Object&quot;])))&#10;&#10;                # Save the original element triples on the new graph&#10;                for element in elements:&#10;                    elementSet.add(element+r[&quot;PredicateTerm&quot;]+r[&quot;ObjectTerm&quot;])&#10;                    # If the element is a part of the original then try to save it&#10;                    if(element in r[&quot;SubjectTerm&quot;].lower()):&#10;                        term = r[&quot;SubjectTerm&quot;]&#10;                        a = len(term)&#10;                        # Check on which word element is connected with SubjectTerm, and check that are the same words&#10;                        for i in range(a-1, -1, -1):&#10;                            if(term[i].isupper() or i == 0):&#10;                                if(term[i:a].lower() == element):&#10;                                    # Save the triple about the element being a SubjectTerm&#10;                                    triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: &quot; &quot;+str(n[&quot;isA&quot;]), &quot;Object&quot;: r[&quot;Subject&quot;], &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: &quot;isA&quot;, &quot;ObjectTerm&quot;: r[&quot;SubjectTerm&quot;]}, ignore_index=True)&#10;                                    g.add((n[element], n[&quot;isA&quot;], Literal(r[&quot;Subject&quot;])))&#10;                                    # Save the triple about the element becoming the SubjectTerm in that triple&#10;                                    triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: r[&quot;Predicate&quot;], &quot;Object&quot;: r[&quot;Object&quot;], &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: r[&quot;PredicateTerm&quot;], &quot;ObjectTerm&quot;: r[&quot;ObjectTerm&quot;]}, ignore_index=True)&#10;                                    # Add the triple to the graph as URIRef or Literal respectively&#10;                                    if(len(str(r[&quot;Object&quot;])) &gt; 5 and &quot;http&quot; == r[&quot;Object&quot;][0:3]):&#10;                                        g.add((n[element], URIRef(r[&quot;Predicate&quot;]), URIRef(r[&quot;Object&quot;])))&#10;                                    else:&#10;                                        g.add((n[element], URIRef(r[&quot;Predicate&quot;]), Literal(r[&quot;Object&quot;])))&#10;                                    &#10;                                # Update the index&#10;                                a = i            &#10;&#10;    # Create the directory in which store the new vocabulary&#10;    fileDestination = &quot;%{fileDestination}&quot;&#10;    location = os.path.normpath(os.path.expanduser(&quot;/&quot;.join(fileDestination.split(&quot;/&quot;)[0:-1])))&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;    # Serialize the new vocabulary&#10;    if(&quot;rdf&quot; in fileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, fileDestination.split(&quot;/&quot;)[-1])), format=&quot;pretty-xml&quot;)&#10;    if(&quot;n3&quot; in fileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, fileDestination.split(&quot;/&quot;)[-1])), format=&quot;n3&quot;)&#10;    if(&quot;nt&quot; in fileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, fileDestination.split(&quot;/&quot;)[-1])), format=&quot;nt&quot;)&#10;    if(&quot;ttl&quot; in fileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, fileDestination.split(&quot;/&quot;)[-1])), format=&quot;turtle&quot;)&#10;    if(&quot;json&quot; in fileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, fileDestination.split(&quot;/&quot;)[-1])), format=&quot;json-ld&quot;)&#10;&#10;    # Return the triples DataFrame for RapidMiner usage&#10;    return triples"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Excel (2)" width="90" x="983" y="391">
        <parameter key="excel_file" value="/home/marco/Desktop/K-Files/Converted/test.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="124" name="Knowledge Converter" width="90" x="648" y="238">
        <parameter key="script" value="# Import libraries&#10;from rdflib import Graph, Literal, RDFS, RDF, OWL, Namespace&#10;import pandas as pd&#10;import os&#10;&#10;# Check if rNames is a subClassOf Names&#10;def checkSub(names, rNames, subsAdded, namesRemaining):&#10;    bool_ = False&#10;    # Split Names and rNames in the different name and rName of relative composition&#10;    nameL = names.split(&quot;_-_&quot;)&#10;    rNameL = rNames.split(&quot;_-_&quot;)&#10;    # Count the number of rName found in rNames&#10;    i = 0&#10;    for name in nameL:&#10;        for rName in rNameL:&#10;            if(name == rName):&#10;                i+=1&#10;    # If the result of i is equal to the number of rName(all elements of rNames are on Names) and there isn't already a bigger subClass&#10;    if(i==len(rNameL) and checkBiggerSub(rNameL, subsAdded)):&#10;        # Set the boolean to true and add rNames as subClass of Names&#10;        bool_ = True&#10;        # Add rNames at the subClasses of Names&#10;        subsAdded.add(rNames)&#10;        # Remove the components of rNames from the components remaining&#10;        for rName in rNameL:&#10;            if(rName in namesRemaining):&#10;                namesRemaining.remove(rName)&#10;    # Return the bool and the modified sets&#10;    return bool_, subsAdded, namesRemaining&#10;&#10;# Check if subj is a subClassOf names&#10;def checkBiggerSub(rNameL, subsAdded):&#10;    # Iterate over every already added subClasses of Names&#10;    for subj in subsAdded:&#10;        # Count the number of rName found in subj&#10;        i = 0&#10;        for s in subj.split(&quot;_-_&quot;):&#10;            for rName in rNameL:&#10;                if(s == rName):&#10;                    i+=1&#10;        # If the result of i is equal to the number of rName(all elements of rNames are on an already added subClass of Names)&#10;        if(i == len(rNameL)):&#10;            # Return False since a bigger subClass has already been added to Names&#10;            return False&#10;    # Return True since there aren't subClasses of Names that covers all these components of Names&#10;    return True&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(data):&#10;    # Create the graph used to store the vocabulary&#10;    g = Graph()&#10;    # Create the Namespace for the vocabulary&#10;    strNameSpace = &quot;%{NameSpace}&quot;&#10;    n = Namespace(strNameSpace)&#10;    g.bind(strNameSpace.split(&quot;/&quot;)[-1], n)&#10;    &#10;    # Create the DataFrame used to save the triples&#10;    triples = pd.DataFrame(columns=[&quot;Subject&quot;,&quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;,&quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;])&#10;&#10;    # Sort the DataFrame&#10;    data = data.sort_values(&quot;total&quot;, ascending=False)&#10;    # Iterate for every row present on data&#10;    for index, row in data.iterrows():&#10;        # Format Names&#10;        names = row[&quot;Names&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot;,&quot;, &quot;_-_&quot;)&#10;        # Save a new triple about Names having as label a new concept&#10;        triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[names]), &quot;Predicate&quot;: str(RDFS.label), &quot;Object&quot;: str(Literal(&quot;concept#&quot;+str(index))), &quot;SubjectTerm&quot;: names, &quot;PredicateTerm&quot;: &quot;comment&quot;, &quot;ObjectTerm&quot;: &quot;concept#&quot;+str(index)}, ignore_index=True)&#10;        g.add((n[names], RDFS.label, Literal(&quot;concept#&quot;+str(index))))&#10;        # Split Names in the different name of its composition&#10;        nameList = names.split(&quot;_-_&quot;)&#10;        # Label the Names&#10;        for name in nameList:&#10;            # Save the triple about Names having as labels the various name of which it is composed&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[names]), &quot;Predicate&quot;: str(RDFS.label), &quot;Object&quot;: str(Literal(name)), &quot;SubjectTerm&quot;: names, &quot;PredicateTerm&quot;: &quot;comment&quot;, &quot;ObjectTerm&quot;: name}, ignore_index=True)&#10;            g.add((n[names], RDFS.label, Literal(name)))&#10;&#10;        # Create set to contain the different subClasses of Names&#10;        subsAdded = set()&#10;        # Create set to contain the remaining single name of Names&#10;        namesRemaining = set()&#10;        namesRemaining.update(nameList)&#10;        # Iterate over every item with less members in Names until all the name in Names has been covered&#10;        for i, r in data.iterrows():&#10;            # If Names in that row has less members than the starting Names, and not all the name in Names has been covered&#10;            if(r[&quot;total&quot;] &lt; row[&quot;total&quot;] and len(namesRemaining) != 0):&#10;                # Format the new row Names: rNames&#10;                rNames = r[&quot;Names&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).replace(&quot;,&quot;, &quot;_-_&quot;)&#10;                # Check if rNames can be a subClassOf Names&#10;                bool_, subsAdded, namesRemaining = checkSub(names, rNames, subsAdded, namesRemaining)&#10;                if(bool_):&#10;                    # Save the triple about rNames being subClassOf Names&#10;                    triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[rNames]), &quot;Predicate&quot;: str(RDFS.subClassOf), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: rNames, &quot;PredicateTerm&quot;: &quot;subClassOf&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;                    g.add((n[rNames], RDFS.subClassOf, n[names]))&#10;        # If the its a composition of at least 2 name, then add the remaining name as subClassOf&#10;        if(len(nameList)&gt;1):&#10;            # Iterate over any remaining name&#10;            for sub in namesRemaining:&#10;                # Save the triple about the single name being subClassOf Names&#10;                triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[sub]), &quot;Predicate&quot;: str(RDFS.subClassOf), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: sub, &quot;PredicateTerm&quot;: &quot;subClassOf&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;                g.add((n[sub], RDFS.subClassOf, n[names]))&#10;&#10;        # Map every element into its domain&#10;        elements = row[&quot;Elements&quot;].replace(&quot; &quot;,&quot;&quot;).replace(&quot;[&quot;,&quot;&quot;).replace(&quot;]&quot;,&quot;&quot;).replace(&quot;'&quot;,&quot;&quot;).split(&quot;,&quot;)&#10;        #print(elements)&#10;        for element in elements:&#10;            # Save the triple about the element being an ObjectProperty&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: str(RDF.type), &quot;Object&quot;: str(OWL.ObjectProperty), &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: &quot;type&quot;, &quot;ObjectTerm&quot;: &quot;ObjectProperty&quot;}, ignore_index=True)&#10;            g.add((n[element], RDF.type, OWL.ObjectProperty))&#10;            # Save the triple about the element being a domain of that Names&#10;            triples = triples.append({&quot;Subject&quot;: &quot; &quot;+str(n[element]), &quot;Predicate&quot;: str(RDFS.domain), &quot;Object&quot;: &quot; &quot;+str(n[names]), &quot;SubjectTerm&quot;: element, &quot;PredicateTerm&quot;: &quot;domain&quot;, &quot;ObjectTerm&quot;: names}, ignore_index=True)&#10;            g.add((n[element], RDFS.domain, n[names]))&#10;&#10;    # Create the directory in which store the new vocabulary&#10;    fileDestination = &quot;%{fileDestination}&quot;&#10;    location = os.path.normpath(os.path.expanduser(&quot;/&quot;.join(fileDestination.split(&quot;/&quot;)[0:-1])))&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;    # Serialize the new vocabulary&#10;    if(&quot;rdf&quot; in fileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, fileDestination.split(&quot;/&quot;)[-1])), format=&quot;pretty-xml&quot;)&#10;    if(&quot;n3&quot; in fileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, fileDestination.split(&quot;/&quot;)[-1])), format=&quot;n3&quot;)&#10;    if(&quot;nt&quot; in fileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, fileDestination.split(&quot;/&quot;)[-1])), format=&quot;nt&quot;)&#10;    if(&quot;ttl&quot; in fileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, fileDestination.split(&quot;/&quot;)[-1])), format=&quot;turtle&quot;)&#10;    if(&quot;json&quot; in fileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, fileDestination.split(&quot;/&quot;)[-1])), format=&quot;json-ld&quot;)&#10;&#10;&#10;    # Return the triples DataFrame for RapidMiner usage&#10;    return triples"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Excel" width="90" x="983" y="289">
        <parameter key="excel_file" value="/home/marco/Desktop/K-Files/Converted/test.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <connect from_op="Read CrossData" from_port="output" to_op="Multiply" to_port="input"/>
      <connect from_op="Read Knowledge Parsed" from_port="output" to_op="Knowledge Converter Full" to_port="input 2"/>
      <connect from_op="Set Macros" from_port="through 1" to_op="Knowledge Converter" to_port="input 2"/>
      <connect from_op="Set Macros" from_port="through 2" to_op="Knowledge Converter Full" to_port="input 3"/>
      <connect from_op="Multiply" from_port="output 1" to_op="Knowledge Converter" to_port="input 1"/>
      <connect from_op="Multiply" from_port="output 2" to_op="Knowledge Converter Full" to_port="input 1"/>
      <connect from_op="Knowledge Converter Full" from_port="output 1" to_op="Write Excel (2)" to_port="input"/>
      <connect from_op="Write Excel (2)" from_port="through" to_port="result 2"/>
      <connect from_op="Knowledge Converter" from_port="output 1" to_op="Write Excel" to_port="input"/>
      <connect from_op="Write Excel" from_port="through" to_port="result 1"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
      <portSpacing port="sink_result 3" spacing="0"/>
      <description align="left" color="yellow" colored="false" height="133" resized="true" width="240" x="580" y="603">ConverterFull also stores every triple of the original parsed vocabulary&lt;br&gt;Eventually filtering the triples to add thanks to the macro Predicates that permits to filter the predicate's URI or Term</description>
      <description align="center" color="blue" colored="true" height="453" resized="true" width="219" x="10" y="126">Set the macros used by the scripts:&lt;br&gt;- nameSpace: the base URI for the newly generated terms&lt;br&gt;- Predicates: list of predicates and predicateTerms used to filter the ConverterFull parsed input, , separated by spaces &amp;quot; &amp;quot;&lt;br&gt;- fileDestination: full path of the name of the desidered destination file to be serialized by Converter/ConverterFull&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;Then the connection to the scripts is made to ensure that the Set Macros module is executed before the scripts, otherwise there will be errors</description>
      <description align="left" color="blue" colored="true" height="208" resized="true" width="269" x="232" y="162">Read the CrossData obtained from the KnowledgeAnalyser process and use it as input for both Converter and ConverterFull</description>
      <description align="center" color="green" colored="true" height="282" resized="true" width="193" x="938" y="219">Store the resulting parsed outputs for the newly generated vocabularies</description>
      <description align="left" color="blue" colored="true" height="165" resized="true" width="255" x="235" y="481">&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Read the original parsed vocabulary to return to a newly generated vocabulary from the CrossData info</description>
      <description align="left" color="yellow" colored="false" height="599" resized="true" width="417" x="511" y="10">Convert the CrossData input into a newly generated vocabulary named as specified in the path of the macro fileDestination&lt;br&gt;The triples of this new vocabulary will be:&lt;br&gt;- every Name row of the CrossData will be saved as a new URI, with as label: concept#... and the single names of its composition&lt;br&gt;- every element of that row will have a new URI and be a domain of the new Name URI&lt;br/&gt;- every new Name URI will then be added as subClassOf the parent Name URI that is composed with at least every name of the new Name URI, hence creating a hierarchy of URIs&lt;br&gt;The new vocabulary will use a new NameSpace to identify the new terms, and this NameSpace is defined as the macro nameSpace</description>
    </process>
  </operator>
</process>
