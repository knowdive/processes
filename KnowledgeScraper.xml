<?xml version="1.0" encoding="UTF-8"?><process version="9.2.001">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="9.2.001" expanded="true" name="Process">
    <parameter key="logverbosity" value="init"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="82" name="LOVScraperFull" width="90" x="246" y="136">
        <parameter key="script" value="# Import libraries&#10;from bs4 import BeautifulSoup&#10;import requests&#10;import time&#10;import pandas as pd&#10;import re&#10;import json&#10;&#10;# Get all the vocabulary of that page&#10;def vocabList(link, url, end, list_):&#10;    # Connect to the URL&#10;    response = requests.get(link)&#10;    # Parse HTML and save to BeautifulSoup object&#10;    soup = BeautifulSoup(response.text, &quot;html.parser&quot;)&#10;    # To download the whole data set, let's do a for loop through all a tags&#10;    voc = soup(&quot;div&quot;, {&quot;class&quot;:&quot;SearchContainer&quot;})&#10;    # If there is at least a vocabulary on that page's list&#10;    if(len(voc)&gt;0):&#10;        # To check the next page&#10;        end += 1&#10;        # Set the index for saving the vocabularies in a list&#10;        index = 0&#10;        # Iterate for every vocabularies present on that page's list&#10;        for i in range(0, len(voc)):&#10;            link = voc[i].a[&quot;href&quot;]&#10;            vocabPage(url+link, list_, index)&#10;    # Return the list of vocabularies and the index for the pages&#10;    return list_, end&#10;&#10;# Get all the info from the vocabulary page&#10;def vocabPage(link, list_, index):&#10;    # Pause the code for half a sec&#10;    time.sleep(.500)&#10;    # Connect to the URL&#10;    response = requests.get(link)&#10;    # Parse HTML and save to BeautifulSoup object&#10;    soup = BeautifulSoup(response.text, &quot;html.parser&quot;)&#10;    &#10;    # Get the title and prefix from the vocabulary page&#10;    title = soup(&quot;h1&quot;)[0]&#10;    prefix = title.span.extract().text.strip()&#10;    title = title.text.strip()&#10;    prefix = prefix.replace(&quot;(&quot;, &quot;&quot;).replace(&quot;)&quot;, &quot;&quot;)&#10;    #Get the URI and Languages from the vocabulary page&#10;    uri = &quot;URI&quot;&#10;    languages = &quot; &quot;&#10;    for child in soup(&quot;tbody&quot;)[0].find_all(&quot;tr&quot;):&#10;        # Get the URI&#10;        if child.td.text.strip() == &quot;URI&quot;:&#10;            uri = child.find_all(&quot;td&quot;)[1].text.strip() &#10;        # Get the Languages&#10;        if child.td.text.strip() == &quot;Language&quot;:&#10;            language = child.find_all(&quot;td&quot;)[1]&#10;            # Append the Languages with a space as separator&#10;            for childL in language.find_all(&quot;a&quot;):&#10;                nameL = childL.find(&quot;div&quot;, {&quot;class&quot;: &quot;agentThumbPrefUri&quot;}).text.strip()&#10;                languages += nameL+&quot; &quot;&#10;&#10;    # Get the latest versions and save it with all its relative information&#10;    script = soup(&quot;script&quot;, {&quot;src&quot;: None})[3].text.strip()&#10;    versions = re.compile(&quot;{\&quot;events\&quot;:(.|\n|\r)*?}]}&quot;).search(script)&#10;    if(versions != None):&#10;        versions = json.loads(versions.group(0))[&quot;events&quot;]&#10;        # Store the last version with a single line on the list&#10;        i = 0&#10;        for version in range(0, len(versions)):&#10;            # If the version has the relative link for the vocabulary the add it&#10;            if(&quot;link&quot; in versions[version].keys()):&#10;                versionName = str(prefix) + &quot;_&quot; + str(i)&#10;                i += 1&#10;                if(&quot;title&quot; in versions[version].keys()):&#10;                    versionName = versions[version][&quot;title&quot;].replace(&quot; &quot;,&quot;-&quot;).replace(&quot;\\&quot;,&quot;&quot;).replace(&quot;/&quot;,&quot;&quot;).replace(&quot;:&quot;,&quot;&quot;).replace(&quot;*&quot;,&quot;&quot;).replace(&quot;?&quot;,&quot;&quot;).replace(&quot;\&quot;&quot;,&quot;&quot;).replace(&quot;&lt;&quot;,&quot;&quot;).replace(&quot;&gt;&quot;,&quot;&quot;).replace(&quot;|&quot;,&quot;&quot;)&#10;                versionDate = &quot;&quot;&#10;                if(&quot;start&quot; in versions[version].keys()):&#10;                    versionDate = versions[version][&quot;start&quot;]&#10;                # Create the dictionary for a new version&#10;                versionD = {&quot;prefix&quot;: prefix, &quot;URI&quot;: uri, &quot;Title&quot;: title, &quot;Languages&quot;: languages, &quot;VersionName&quot;: versionName, &quot;VersionDate&quot;: versions[version][&quot;start&quot;], &quot;Link&quot;: versions[version][&quot;link&quot;], &quot;Folder&quot;: &quot;LOV_Full&quot;}&#10;                # Add the version to the list&#10;                list_.append(versionD)&#10;        # If new versions has to be added&#10;        if(i):&#10;            # Update the index for the next element of the list&#10;            index += (i/i)&#10;    &#10;    # Return the DataFrame to save the added vocab&#10;    return list_, index&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main():&#10;    # Create the DataFrame to save the LOVs' vocabs' information&#10;    df = pd.DataFrame(columns=[&quot;prefix&quot;, &quot;URI&quot;, &quot;Title&quot;, &quot;Languages&quot;, &quot;VersionName&quot;, &quot;VersionDate&quot;, &quot;Link&quot;, &quot;Folder&quot;])&#10;&#10;    # Set the URL you want to webscrape from&#10;    url = &quot;https://lov.linkeddata.es&quot;&#10;    # Set the starting and ending page to scrape, that updates dynamically&#10;    page = 1&#10;    end = 2&#10;    &#10;    # Scrape every page from the vocabs tab of LOV&#10;    while page &lt; end:&#10;        # Get the #page with the vocabs list&#10;        link = url+&quot;/dataset/lov/vocabs?&amp;page=&quot;+str(page)&#10;        # Examine the list of vocabs&#10;        list_, end = vocabList(link, url, end, list())&#10;        # Add the list of that page to the DataFrame, if there are vocabularies in that page&#10;        if(len(list_)):&#10;            df = df.append(list_, ignore_index=True)&#10;        # Iterate the next page if there were vocabs in this page, otherwise end the program there&#10;        page += 1&#10;    &#10;    # Return the DataFrame for RapidMiner visualization&#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="82" name="LOVScraperLatest" width="90" x="246" y="595">
        <parameter key="script" value="# Import libraries&#10;from bs4 import BeautifulSoup&#10;import requests&#10;import time&#10;import pandas as pd&#10;import re&#10;import json&#10;&#10;# Get all the vocabulary of that page&#10;def vocabList(link, url, end, list_):&#10;    # Connect to the URL&#10;    response = requests.get(link)&#10;    # Parse HTML and save to BeautifulSoup object&#10;    soup = BeautifulSoup(response.text, &quot;html.parser&quot;)&#10;    # To download the whole data set, let's do a for loop through all a tags&#10;    voc = soup(&quot;div&quot;, {&quot;class&quot;:&quot;SearchContainer&quot;})&#10;    # If there is at least a vocabulary on that page's list&#10;    if(len(voc)&gt;0):&#10;        # To check the next page&#10;        end += 1&#10;        # Set the index for saving the vocabularies in a list&#10;        index = 0&#10;        # Iterate for every vocabularies present on that page's list&#10;        for i in range(0, len(voc)):&#10;            link = voc[i].a[&quot;href&quot;]&#10;            vocabPage(url+link, list_, index)&#10;    # Return the list of vocabularies and the index for the pages&#10;    return list_, end&#10;&#10;# Get all the info from the vocabulary page&#10;def vocabPage(link, list_, index):&#10;    # Pause the code for half a sec&#10;    time.sleep(.500)&#10;    # Connect to the URL&#10;    response = requests.get(link)&#10;    # Parse HTML and save to BeautifulSoup object&#10;    soup = BeautifulSoup(response.text, &quot;html.parser&quot;)&#10;    &#10;    # Get the title and prefix from the vocabulary page&#10;    title = soup(&quot;h1&quot;)[0]&#10;    prefix = title.span.extract().text.strip()&#10;    title = title.text.strip()&#10;    prefix = prefix.replace(&quot;(&quot;, &quot;&quot;).replace(&quot;)&quot;, &quot;&quot;)&#10;    #Get the URI and Languages from the vocabulary page&#10;    uri = &quot;URI&quot;&#10;    languages = &quot; &quot;&#10;    for child in soup(&quot;tbody&quot;)[0].find_all(&quot;tr&quot;):&#10;        # Get the URI&#10;        if child.td.text.strip() == &quot;URI&quot;:&#10;            uri = child.find_all(&quot;td&quot;)[1].text.strip() &#10;        # Get the Languages&#10;        if child.td.text.strip() == &quot;Language&quot;:&#10;            language = child.find_all(&quot;td&quot;)[1]&#10;            # Append the Languages with a space as separator&#10;            for childL in language.find_all(&quot;a&quot;):&#10;                nameL = childL.find(&quot;div&quot;, {&quot;class&quot;: &quot;agentThumbPrefUri&quot;}).text.strip()&#10;                languages += nameL+&quot; &quot;&#10;&#10;    # Get the latest versions and save it with all its relative information&#10;    script = soup(&quot;script&quot;, {&quot;src&quot;: None})[3].text.strip()&#10;    versions = re.compile(&quot;{\&quot;events\&quot;:(.|\n|\r)*?}]}&quot;).search(script)&#10;    if(versions != None):&#10;        versions = json.loads(versions.group(0))[&quot;events&quot;]&#10;        # Store the last version with a single line on the list&#10;        i = 0&#10;        for version in range(0, len(versions)):&#10;            # If the version has the relative link for the vocabulary the add it&#10;            if(&quot;link&quot; in versions[version].keys()):&#10;                versionName = str(prefix) + &quot;_&quot; + str(i)&#10;                i += 1&#10;                if(&quot;title&quot; in versions[version].keys()):&#10;                    versionName = versions[version][&quot;title&quot;].replace(&quot; &quot;,&quot;-&quot;).replace(&quot;\\&quot;,&quot;&quot;).replace(&quot;/&quot;,&quot;&quot;).replace(&quot;:&quot;,&quot;&quot;).replace(&quot;*&quot;,&quot;&quot;).replace(&quot;?&quot;,&quot;&quot;).replace(&quot;\&quot;&quot;,&quot;&quot;).replace(&quot;&lt;&quot;,&quot;&quot;).replace(&quot;&gt;&quot;,&quot;&quot;).replace(&quot;|&quot;,&quot;&quot;)&#10;                versionDate = &quot;&quot;&#10;                if(&quot;start&quot; in versions[version].keys()):&#10;                    versionDate = versions[version][&quot;start&quot;]&#10;                # Create the dictionary for a new version&#10;                versionD = {&quot;prefix&quot;: prefix, &quot;URI&quot;: uri, &quot;Title&quot;: title, &quot;Languages&quot;: languages, &quot;VersionName&quot;: versionName, &quot;VersionDate&quot;: versions[version][&quot;start&quot;], &quot;Link&quot;: versions[version][&quot;link&quot;], &quot;Folder&quot;: &quot;LOV_Latest&quot;}&#10;        # If a new version has to be added&#10;        if(i):&#10;            # Add the version to the list&#10;            list_.append(versionD)&#10;            # Update the index for the next element of the list&#10;            index += (i/i)&#10;    &#10;    # Return the DataFrame to save the added vocab&#10;    return list_, index&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main():&#10;    # Create the DataFrame to save the LOVs' vocabs' information&#10;    df = pd.DataFrame(columns=[&quot;prefix&quot;, &quot;URI&quot;, &quot;Title&quot;, &quot;Languages&quot;, &quot;VersionName&quot;, &quot;VersionDate&quot;, &quot;Link&quot;, &quot;Folder&quot;])&#10;&#10;    # Set the URL you want to webscrape from&#10;    url = &quot;https://lov.linkeddata.es&quot;&#10;    # Set the starting and ending page to scrape, that updates dynamically&#10;    page = 1&#10;    end = 2&#10;    &#10;    # Scrape every page from the vocabs tab of LOV&#10;    while page &lt; end:&#10;        # Get the #page with the vocabs list&#10;        link = url+&quot;/dataset/lov/vocabs?&amp;page=&quot;+str(page)&#10;        # Examine the list of vocabs&#10;        list_, end = vocabList(link, url, end, list())&#10;        # Add the list of that page to the DataFrame, if there are vocabularies in that page&#10;        if(len(list_)):&#10;            df = df.append(list_, ignore_index=True)&#10;        # Iterate the next page if there were vocabs in this page, otherwise end the program there&#10;        page += 1&#10;    &#10;    # Return the DataFrame for RapidMiner visualization&#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="82" name="otherScraper" width="90" x="246" y="391">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main():&#10;    # Create the DataFrame to save the LOVs' vocabs' information&#10;    df = pd.DataFrame(columns=[&quot;prefix&quot;, &quot;URI&quot;, &quot;Title&quot;, &quot;Languages&quot;, &quot;VersionName&quot;, &quot;VersionDate&quot;, &quot;Link&quot;, &quot;Folder&quot;])&#10;&#10;    # Get the other vocabularies from the Excel file from github&#10;    vocabs = pd.read_excel(&quot;https://raw.githubusercontent.com/knowdive/resources/master/otherVocabs.xlsx&quot;)&#10;    # Create the list used to contain the information about the other vocabularies&#10;    list_ = list()&#10;    index = 0&#10;    # Iterate for every vocabulary read from the Excel file&#10;    for index, row in vocabs.iterrows():&#10;        # Add the vocabulary to the list&#10;        list_.insert(index,{&quot;prefix&quot;: row[&quot;prefix&quot;], &quot;URI&quot;: row[&quot;URI&quot;], &quot;Title&quot;: row[&quot;Title&quot;], &quot;Languages&quot;: row[&quot;Languages&quot;], &quot;VersionName&quot;: row[&quot;VersionName&quot;], &quot;VersionDate&quot;: row[&quot;VersionDate&quot;], &quot;Link&quot;: row[&quot;Link&quot;], &quot;Folder&quot;: row[&quot;Folder&quot;]})&#10;        # Update the index for the next element of the list&#10;        index += 1&#10;    # Add the list of that Excel file to the DataFrame, if there are vocabularies in that Excel file&#10;    if(len(list_)):&#10;        df = df.append(list_, ignore_index=True)&#10;    &#10;    # Return the DataFrame for RapidMiner visualization&#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="multiply" compatibility="9.2.001" expanded="true" height="103" name="Multiply" width="90" x="380" y="391"/>
      <operator activated="true" class="union" compatibility="9.2.001" expanded="true" height="82" name="Union" width="90" x="648" y="238"/>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Excel Full" width="90" x="916" y="289">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\KnowledgeFull.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="union" compatibility="9.2.001" expanded="true" height="82" name="Union (2)" width="90" x="648" y="493"/>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Excel Latest" width="90" x="916" y="442">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\KnowledgeLatest.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <connect from_op="LOVScraperFull" from_port="output 1" to_op="Union" to_port="example set 1"/>
      <connect from_op="LOVScraperLatest" from_port="output 1" to_op="Union (2)" to_port="example set 1"/>
      <connect from_op="otherScraper" from_port="output 1" to_op="Multiply" to_port="input"/>
      <connect from_op="Multiply" from_port="output 1" to_op="Union" to_port="example set 2"/>
      <connect from_op="Multiply" from_port="output 2" to_op="Union (2)" to_port="example set 2"/>
      <connect from_op="Union" from_port="union" to_op="Write Excel Full" to_port="input"/>
      <connect from_op="Write Excel Full" from_port="through" to_port="result 1"/>
      <connect from_op="Union (2)" from_port="union" to_op="Write Excel Latest" to_port="input"/>
      <connect from_op="Write Excel Latest" from_port="through" to_port="result 2"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
      <portSpacing port="sink_result 3" spacing="0"/>
      <description align="center" color="yellow" colored="false" height="186" resized="true" width="399" x="100" y="515">Retrieve the latest version of the vocabularies from LOV saving Date of scraping, prefix, URI, Title, Languages, VersionName, VersionDate, Link for every vocabulary, plus a Folder column used to divide the parsed vocabularies in folders</description>
      <description align="center" color="yellow" colored="false" height="191" resized="true" width="424" x="77" y="55">Retrieve all the existing versions of the vocabularies from LOV saving Date of scraping, prefix, URI, Title, Languages, VersionName, VersionDate, Link for every vocabulary, plus a Folder column used to divide the parsed vocabularies in folders</description>
      <description align="center" color="green" colored="true" height="317" resized="true" width="146" x="884" y="229">Store the results as Excel Files</description>
      <description align="center" color="yellow" colored="false" height="225" resized="true" width="450" x="68" y="283">Retrieve the vocabularies from the github file otherVocabs (https://github.com/knowdive/resources/blob/master/otherVocabs.xlsx), saving Date of scraping, prefix, URI, Title, Languages, VersionName, VersionDate, Link for every vocabulary, plus a Folder column used to divide the parsed vocabularies in folders</description>
      <description align="center" color="yellow" colored="false" height="454" resized="true" width="178" x="607" y="174">Merge other Vocabularies with LOV Vocabularies</description>
      <description align="center" color="yellow" colored="false" height="108" resized="true" width="620" x="15" y="714">It will possible to add a new script for scraping other locations and union it with LOV Vocabularies and other Vocabularies, given that it produces a DataFrame with the following columns, as defined by the other Vocabularies file in GitHub:&lt;br&gt;Date of scraping, prefix, URI, Title, Languages, VersionName, VersionDate, Link for every vocabulary, plus a Folder column used to divide the parsed vocabularies in folders</description>
    </process>
  </operator>
</process>
