<?xml version="1.0" encoding="UTF-8"?><process version="9.2.000">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="9.2.000" expanded="true" name="Process">
    <parameter key="logverbosity" value="init"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="read_excel" compatibility="9.2.000" expanded="true" height="68" name="Read Input Destination File" width="90" x="112" y="136">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\Internship\IN.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="false"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="A.true.polynominal.attribute"/>
          <parameter key="1" value="B.true.polynominal.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
        <description align="center" color="transparent" colored="false" width="126">Read the worksheet of Excel that contains the folder and name of where to save the File that will contain the info about LOV's vocabularies, it can be the same file from where the informations are read&lt;br&gt;The worksheet must have the first 2 cells of the first row as:&lt;br&gt;[0,0]: FolderPath&lt;br&gt;[0,1]: FileName</description>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="103" name="LOV_Scraper_Latest" width="90" x="380" y="136">
        <parameter key="script" value="# Import libraries&#10;from bs4 import BeautifulSoup&#10;from pathlib import Path&#10;import requests&#10;import urllib.request&#10;import time&#10;import pandas as pd&#10;import re&#10;import json&#10;import os&#10;&#10;# Class to handle the Excel file and relative index&#10;class ExcelFile:&#10;  def __init__(self, writer):&#10;    self.writer = writer&#10;    self.index = 1&#10;&#10;# Get all the vocabulary of that page&#10;def vocabList(link, url, end, excel):&#10;    # Connect to the URL&#10;    response = requests.get(link)&#10;    # Parse HTML and save to BeautifulSoup object&#10;    soup = BeautifulSoup(response.text, &quot;html.parser&quot;)&#10;    # To download the whole data set, let's do a for loop through all a tags&#10;    voc = soup(&quot;div&quot;, {&quot;class&quot;:&quot;SearchContainer&quot;})&#10;    # if there is at least a vocabulary on that page's list&#10;    if(len(voc)&gt;0):&#10;        # To check the next page&#10;        end += 1&#10;        # Iterate for every vocabularies present on that page's list&#10;        for i in range(0, len(voc)):&#10;            link = voc[i].a[&quot;href&quot;]&#10;            vocabPage(url+link, excel)&#10;    return end&#10;&#10;# Get all the info and the N3 from the vocabulary page&#10;def vocabPage(link, excel):&#10;    # Pause the code for half a sec&#10;    time.sleep(.500)&#10;    # Connect to the URL&#10;    response = requests.get(link)&#10;    # Parse HTML and save to BeautifulSoup object&#10;    soup = BeautifulSoup(response.text, &quot;html.parser&quot;)&#10;&#10;    # Get the excel file and relative worksheets&#10;    workbook = excel.writer.book&#10;    sheet = workbook.get_worksheet_by_name(&quot;LOVVersions&quot;)&#10;    &#10;    # Get the title and prefix from the vocabulary page&#10;    title = soup(&quot;h1&quot;)[0]&#10;    prefix = title.span.extract().text.strip()&#10;    title = title.text.strip()&#10;    prefix = prefix.replace(&quot;(&quot;, &quot;&quot;).replace(&quot;)&quot;, &quot;&quot;)&#10;    #Get the URI and Languages from the vocabulary page&#10;    uri = &quot;URI&quot;&#10;    languages = &quot; &quot;&#10;    for child in soup(&quot;tbody&quot;)[0].find_all(&quot;tr&quot;):&#10;        # Get the URI&#10;        if child.td.text.strip() == &quot;URI&quot;:&#10;            uri = child.find_all(&quot;td&quot;)[1].text.strip() &#10;        # Get the Languages&#10;        if child.td.text.strip() == &quot;Language&quot;:&#10;            language = child.find_all(&quot;td&quot;)[1]&#10;            # Append the Languages with a space as separator&#10;            for childL in language.find_all(&quot;a&quot;):&#10;                nameL = childL.find(&quot;div&quot;, {&quot;class&quot;: &quot;agentThumbName&quot;}).text.strip()&#10;                languages += nameL+&quot; &quot;&#10;&#10;    # Get all the versions and save them with all their relative informations&#10;    script = soup(&quot;script&quot;, {&quot;src&quot;: None})[3].text.strip()&#10;    versions = re.compile(&quot;{\&quot;events\&quot;:(.|\n|\r)*?}]}&quot;).search(script)&#10;    if(versions != None):&#10;        versions = json.loads(versions.group(0))[&quot;events&quot;]&#10;        # Store every version with a line on the Excel File&#10;        index = 0&#10;        for version in range(0, len(versions)):&#10;            if(&quot;title&quot; in versions[version].keys() and &quot;start&quot; in versions[version].keys() and &quot;link&quot; in versions[version].keys()):&#10;                versionName = versions[version][&quot;title&quot;].replace(&quot; &quot;,&quot;-&quot;).replace(&quot;\\&quot;,&quot;&quot;).replace(&quot;/&quot;,&quot;&quot;).replace(&quot;:&quot;,&quot;&quot;).replace(&quot;*&quot;,&quot;&quot;).replace(&quot;?&quot;,&quot;&quot;).replace(&quot;\&quot;&quot;,&quot;&quot;).replace(&quot;&lt;&quot;,&quot;&quot;).replace(&quot;&gt;&quot;,&quot;&quot;).replace(&quot;|&quot;,&quot;&quot;)&#10;                sheet.write_row(excel.index, 0, (prefix, uri, title, languages, versionName, versions[version][&quot;start&quot;], versions[version][&quot;link&quot;], &quot;LOV_Latest&quot;))&#10;                index = 1&#10;        excel.index += index&#10;&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(data):&#10;    # Get the destination folder and file name from RapidMiner&#10;    location = &quot;C:\\Users\\marco\\Desktop\\Internship\\Results&quot;&#10;    destination = &quot;LOV_Latest&quot;&#10;    if(len(data)):&#10;        location = data.iloc[0,0]&#10;        destination = data.iloc[0,1]&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;&#10;    # Create a Pandas Excel writer using XlsxWriter as the engine.&#10;    writer = pd.ExcelWriter(str(os.path.join(location, destination)) + &quot;.xlsx&quot;, engine='xlsxwriter')&#10;    excel = ExcelFile(writer)&#10;    # Get the xlsxwriter workbook and worksheet objects.&#10;    workbook  = writer.book&#10;    # Add WorkSheet with relative titles and relative bold header &#10;    sheet = workbook.add_worksheet(&quot;LOVVersions&quot;)&#10;    sheet.write_row(0, 0, (&quot;prefix&quot;, &quot;URI&quot;, &quot;Title&quot;, &quot;Languages&quot;, &quot;VersionName&quot;, &quot;VersionDate&quot;, &quot;Link&quot;, &quot;Folder&quot;), workbook.add_format({&quot;bold&quot;: True}))&#10;    sheet.set_column(0, 6, 30)&#10;    # Add WorkSheet with relative titles and relative bold header &#10;    results = workbook.add_worksheet(&quot;Results&quot;)&#10;    results.write_row(0, 0, (location, destination+&quot;_vocabs&quot;))&#10;    results.set_column(0, 0, 50)&#10;&#10;    # Set the URL you want to webscrape from&#10;    url = &quot;https://lov.linkeddata.es&quot;&#10;    # Set the starting and ending page to scrape, that updates dynamically&#10;    page = 1&#10;    end = 2&#10;&#10;    # Scrape every page from the vocabs tab of LOV&#10;    while page &lt; end:&#10;        # Get the #page with the vocabs list&#10;        link = url+&quot;/dataset/lov/vocabs?&amp;page=&quot;+str(page)&#10;        end = vocabList(link, url, end, excel)&#10;        # Iterate the next page if there were vocabs in this page, otherwise end the program there&#10;        page += 1&#10;&#10;    # Close and save the Excel file&#10;    workbook.close()&#10;    writer.save()"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
        <description align="center" color="transparent" colored="false" width="126">Retrieve the latest version of the vocabularies from LOV, saving Date of scraping, prefix, URI, Title, Languages, VersionName, VersionDate, Link for every vocabulary, plus a Folder column used to divide the vocabularies in folders</description>
      </operator>
      <operator activated="true" class="read_excel" compatibility="9.2.000" expanded="true" height="68" name="Read Resulting File" width="90" x="648" y="187">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\Internship\Results\LOV_Latest.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="A.true.polynominal.attribute"/>
          <parameter key="1" value="B.true.polynominal.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
        <description align="center" color="transparent" colored="false" width="126">Used to view the resulting file of LOV_Scraper_Latest&lt;br&gt;</description>
      </operator>
      <operator activated="true" class="store" compatibility="9.2.000" expanded="true" height="68" name="Store" width="90" x="782" y="187">
        <parameter key="repository_entry" value="../Data/LOV_Latest"/>
      </operator>
      <connect from_op="Read Input Destination File" from_port="output" to_op="LOV_Scraper_Latest" to_port="input 1"/>
      <connect from_op="Read Resulting File" from_port="output" to_op="Store" to_port="input"/>
      <connect from_op="Store" from_port="through" to_port="result 1"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
    </process>
  </operator>
</process>
