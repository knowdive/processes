<?xml version="1.0" encoding="UTF-8"?><process version="9.2.001">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="9.2.001" expanded="true" name="Process">
    <parameter key="logverbosity" value="init"/>
    <parameter key="logfile" value="/Users/mattiafumagalli/Desktop/prova.log"/>
    <parameter key="resultfile" value="/Users/mattiafumagalli/prova.res"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read Knowledge" width="90" x="45" y="340">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\K-Files\IN\2019-04-18_Inherited_algo_v0.2_2014-01-24_0.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="Date.true.polynominal.attribute"/>
          <parameter key="1" value="Subject.true.polynominal.attribute"/>
          <parameter key="2" value="Predicate.true.polynominal.attribute"/>
          <parameter key="3" value="Object.true.polynominal.attribute"/>
          <parameter key="4" value="SubjectTerm.true.polynominal.attribute"/>
          <parameter key="5" value="PredicateTerm.true.polynominal.attribute"/>
          <parameter key="6" value="ObjectTerm.true.polynominal.attribute"/>
          <parameter key="7" value="Domain.true.polynominal.attribute"/>
          <parameter key="8" value="Domain Version.true.polynominal.attribute"/>
          <parameter key="9" value="Domain Date.true.polynominal.attribute"/>
          <parameter key="10" value="URI.true.polynominal.attribute"/>
          <parameter key="11" value="Title.true.polynominal.attribute"/>
          <parameter key="12" value="Languages.true.polynominal.attribute"/>
          <parameter key="13" value="Inherited.true.integer.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
      </operator>
      <operator activated="true" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read Predicate" width="90" x="45" y="442">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\Internship\InternshipCode\RapidMinerCode\knowledgeFilter\Predicate.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="PredicateTerm.true.polynominal.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
        <description align="center" color="transparent" colored="false" width="126">Optional</description>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="82" name="KnowledgeFilter" width="90" x="246" y="289">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(triples, predicates = pd.DataFrame()):    &#10;    # Create the DataFrame to save the vocabs' triples with the predicates present on the argument 'predicates'&#10;    df = pd.DataFrame(columns=[&quot;Type&quot;, &quot;Property&quot;])&#10;&#10;    # Return all the triples if there is no predicate filtering&#10;    if((len(predicates) == 0)):&#10;        df[&quot;Type&quot;] = triples[&quot;ObjectTerm&quot;]&#10;        df[&quot;Property&quot;] = triples[&quot;SubjectTerm&quot;]&#10;        return df&#10;&#10;    # Iterate for every predicate present on 'predicates'&#10;    for ind_, r in predicates.iterrows():&#10;        # Create the list used to add the triples that has that predicate&#10;        list_ = list()&#10;        index_ = 0&#10;        # Iterate for every triples present on the file passed on the argument 'triples'&#10;        for index, row in triples.iterrows():&#10;            # if a triple has a specified PredicateTerm or the predicates are not set&#10;            if((str(row[&quot;PredicateTerm&quot;]) == str(r[predicates.columns[0]]))):&#10;                # Save that triple on the list&#10;                list_.insert(index_,{&quot;Type&quot;: row[&quot;ObjectTerm&quot;], &quot;Property&quot;: row[&quot;SubjectTerm&quot;]})&#10;                index_ += 1&#10;        # Save the information on the list to the DataFrame for each predicate checked&#10;        if(index_ and len(list_)):&#10;            df = df.append(list_)&#10;&#10;    # Return the DataFrame for RapidMiner usage&#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="set_role" compatibility="9.2.001" expanded="true" height="82" name="Set Role" width="90" x="380" y="187">
        <parameter key="attribute_name" value="Type"/>
        <parameter key="target_role" value="label"/>
        <list key="set_additional_roles"/>
      </operator>
      <operator activated="true" class="nominal_to_text" compatibility="9.2.001" expanded="true" height="82" name="Nominal to Text" width="90" x="380" y="289">
        <parameter key="attribute_filter_type" value="all"/>
        <parameter key="attribute" value=""/>
        <parameter key="attributes" value=""/>
        <parameter key="use_except_expression" value="false"/>
        <parameter key="value_type" value="nominal"/>
        <parameter key="use_value_type_exception" value="false"/>
        <parameter key="except_value_type" value="file_path"/>
        <parameter key="block_type" value="single_value"/>
        <parameter key="use_block_type_exception" value="false"/>
        <parameter key="except_block_type" value="single_value"/>
        <parameter key="invert_selection" value="false"/>
        <parameter key="include_special_attributes" value="false"/>
      </operator>
      <operator activated="true" class="replace" compatibility="9.2.001" expanded="true" height="82" name="Replace (2)" width="90" x="514" y="187">
        <parameter key="attribute_filter_type" value="all"/>
        <parameter key="attribute" value=""/>
        <parameter key="attributes" value=""/>
        <parameter key="use_except_expression" value="false"/>
        <parameter key="value_type" value="nominal"/>
        <parameter key="use_value_type_exception" value="false"/>
        <parameter key="except_value_type" value="file_path"/>
        <parameter key="block_type" value="single_value"/>
        <parameter key="use_block_type_exception" value="false"/>
        <parameter key="except_block_type" value="single_value"/>
        <parameter key="invert_selection" value="false"/>
        <parameter key="include_special_attributes" value="false"/>
        <parameter key="replace_what" value="([A-Z])"/>
        <parameter key="replace_by" value=" $1"/>
      </operator>
      <operator activated="true" class="text:process_document_from_data" compatibility="8.1.000" expanded="true" height="82" name="Process Documents from Data" width="90" x="514" y="289">
        <parameter key="create_word_vector" value="true"/>
        <parameter key="vector_creation" value="Term Occurrences"/>
        <parameter key="add_meta_information" value="true"/>
        <parameter key="keep_text" value="true"/>
        <parameter key="prune_method" value="none"/>
        <parameter key="prune_below_percent" value="3.0"/>
        <parameter key="prune_above_percent" value="30.0"/>
        <parameter key="prune_below_rank" value="0.05"/>
        <parameter key="prune_above_rank" value="0.95"/>
        <parameter key="datamanagement" value="double_sparse_array"/>
        <parameter key="data_management" value="auto"/>
        <parameter key="select_attributes_and_weights" value="false"/>
        <list key="specify_weights"/>
        <process expanded="true">
          <operator activated="true" class="text:tokenize" compatibility="8.1.000" expanded="true" height="68" name="Tokenize (3)" width="90" x="112" y="34">
            <parameter key="mode" value="non letters"/>
            <parameter key="characters" value=".:"/>
            <parameter key="language" value="English"/>
            <parameter key="max_token_length" value="3"/>
          </operator>
          <operator activated="true" class="text:transform_cases" compatibility="8.1.000" expanded="true" height="68" name="Transform Cases (3)" width="90" x="246" y="34">
            <parameter key="transform_to" value="lower case"/>
          </operator>
          <operator activated="true" class="text:filter_stopwords_english" compatibility="8.1.000" expanded="true" height="68" name="Filter Stopwords (3)" width="90" x="380" y="34"/>
          <operator activated="true" class="text:filter_by_length" compatibility="8.1.000" expanded="true" height="68" name="Filter Tokens (by Length)" width="90" x="514" y="34">
            <parameter key="min_chars" value="3"/>
            <parameter key="max_chars" value="25"/>
          </operator>
          <connect from_port="document" to_op="Tokenize (3)" to_port="document"/>
          <connect from_op="Tokenize (3)" from_port="document" to_op="Transform Cases (3)" to_port="document"/>
          <connect from_op="Transform Cases (3)" from_port="document" to_op="Filter Stopwords (3)" to_port="document"/>
          <connect from_op="Filter Stopwords (3)" from_port="document" to_op="Filter Tokens (by Length)" to_port="document"/>
          <connect from_op="Filter Tokens (by Length)" from_port="document" to_port="document 1"/>
          <portSpacing port="source_document" spacing="0"/>
          <portSpacing port="sink_document 1" spacing="0"/>
          <portSpacing port="sink_document 2" spacing="0"/>
        </process>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write FCA Matrix" width="90" x="648" y="85">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\analysis-step\out2.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" breakpoints="after" class="text:wordlist_to_data" compatibility="8.1.000" expanded="true" height="82" name="WordList to Data" width="90" x="715" y="289"/>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Data Matrix" width="90" x="849" y="289">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\analysis-step\out.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="124" name="KnowledgeAnalysis" width="90" x="1050" y="340">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(data):&#10;    # Drop the column 'in documents', that is equal to 'total'&#10;    if(&quot;in documents&quot; in data.columns):&#10;        data.drop(&quot;in documents&quot;, axis=1, inplace=True)&#10;&#10;    # Create the DataFrame used to save the Cues&#10;    cue = pd.DataFrame(columns=[&quot;Class&quot;,&quot;Cue1&quot;, &quot;Cue2&quot;, &quot;Cue3&quot;, &quot;Cue4&quot;, &quot;Cue5&quot;, &quot;Cue6&quot;])&#10;    # Iterate for every column present on data&#10;    for column in data:&#10;        # Rename the columns&#10;        if &quot;word&quot; not in column:&#10;            data.rename(index=str, columns={column: str(column+&quot;_456&quot;)}, inplace= True)&#10;            column += &quot;_456&quot;&#10;        # Checks if the column identify a Class&#10;        if(&quot;in class&quot; in column):&#10;            # Create the new column for the Cues in the input DataFrame, and calculate the values for every Element &#10;            index = data.columns.get_loc(column)&#10;            className = &quot;Cue(&quot; + column[10:-5] + &quot;)_456&quot;&#10;            tempColumn = data[column] / data[&quot;total_456&quot;]&#10;            data.insert(index, className, tempColumn)&#10;            &#10;            # Calculate the metrics of that Class&#10;            cue4 = data[className].sum()&#10;            cue5 = cue4 / data[column].sum()&#10;            cue6 = 1 - cue5&#10;            # Save the metrics of that Class&#10;            cue.at[column[10:-5], 'Class'] = column[10:-5]&#10;            cue.at[column[10:-5], 'Cue4'] = cue4&#10;            cue.at[column[10:-5], 'Cue5'] = cue5&#10;            cue.at[column[10:-5], 'Cue6'] = cue6&#10;&#10;            # Create the new column for the Cues in the input DataFrame, and copy the values for every Element &#10;            index = data.columns.get_loc(column)&#10;            className = column[0:-4] &#10;            tempColumn = data[column]&#10;            data.insert(index, className + &quot;_123&quot;, tempColumn)&#10;&#10;    # Calculate the Knowledge metrics of the input&#10;    cue4 = cue[&quot;Cue4&quot;].sum()&#10;    cue5 = cue4 / data[&quot;total_456&quot;].sum()&#10;    cue6 = 1 - cue5&#10;    # Save the Knowledge metrics of the input&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Class'] = &quot;KNOWLEDGE&quot;&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue4'] = cue4&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue5'] = cue5&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue6'] = cue6&#10;&#10;    # Create the new column for the Cues in the input DataFrame, and copy the values from the original column&#10;    index = data.columns.get_loc(&quot;total_456&quot;)&#10;    className = &quot;total_123&quot;&#10;    tempColumn = data[&quot;total_456&quot;]&#10;    data.insert(index, className, tempColumn)&#10;&#10;    # Create the DataFrame used to save the occurrences of the Names present on the Element row&#10;    DF = pd.DataFrame(columns=[&quot;Element&quot;, &quot;Names&quot;])&#10;    # Iterate for every row present on data, for every Element&#10;    for index, row in data.iterrows():&#10;        # Create a list to save the Names present on that row&#10;        list_ = list()&#10;        # For evert column that indicates a Name&#10;        for column in data:&#10;            # Check if the Name is present on that Element/row&#10;            if((&quot;in class&quot; in column and &quot;_456&quot; in column) and row[column]): &#10;                # Save the Name on the list&#10;                list_.append(column[10:-5])&#10;&#10;                # Format data for another kind of cue Analysis&#10;                if(row[column] &gt; 1):&#10;                    data.at[index, &quot;total_123&quot;] = row[&quot;total_123&quot;] - row[column] + 1 &#10;                    row[&quot;total_123&quot;] = row[&quot;total_123&quot;] - row[column] + 1&#10;                    data.at[index, column[0:-4] + &quot;_123&quot; ] = 1&#10;        # Save the correlation between Element and Names&#10;        DF = DF.append({&quot;Element&quot;: row[&quot;word&quot;], &quot;Names&quot;: list_}, ignore_index=True)&#10;&#10;    # Create the DataFrame used to save the table used to identify common Elements between Names&#10;    DTF = pd.DataFrame(columns=[&quot;total&quot;, &quot;Names&quot;, &quot;number&quot;, &quot;Elements&quot;])&#10;    # Create the set used to check if new Names has to be added or if existing Names has to be updated&#10;    set_ = set()&#10;    # Iterate for every row present on DF, for every Element and the relative Names&#10;    for index_, row in DF.iterrows():&#10;        # Check if new Names has to be added or if existing Names has to be updated&#10;        a = len(set_)&#10;        set_.add(str(row[&quot;Names&quot;]))&#10;        if(a &lt; len(set_)):&#10;            # Create a new row on the DataFrame for that Names&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;total&quot;] = len(row[&quot;Names&quot;])&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;Names&quot;] = row[&quot;Names&quot;]&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;Elements&quot;] = str(row[&quot;Element&quot;])&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;number&quot;] = 1&#10;        else:&#10;            # Update the row for that Names, adding the new Element&#10;            elements = str(DTF.at[str(row[&quot;Names&quot;]), &quot;Elements&quot;])&#10;            number = DTF.at[str(row[&quot;Names&quot;]), &quot;number&quot;]&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;Elements&quot;] = elements + &quot; , &quot; + str(row[&quot;Element&quot;])&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;number&quot;] = number + 1&#10;&#10;    # Iterate for every column present on data&#10;    for column in data:&#10;        # Checks if the column identify a Class&#10;        if((&quot;in class&quot; in column and &quot;_123&quot; in column)):&#10;            # Create the new column for the Cue in the input DataFrame, and calculate the values for every Element &#10;            index = data.columns.get_loc(column) - 1&#10;            className = &quot;Cue(&quot; + column[10:-5] + &quot;)_123&quot;&#10;            tempColumn = data[column] / data[&quot;total_123&quot;]&#10;            data.insert(index, className, tempColumn)&#10;            &#10;            # Calculate the metrics of that Class&#10;            cue1 = data[className].sum()&#10;            cue2 = cue1 / data[column].sum()&#10;            cue3 = 1 - cue2&#10;            # Save the metrics of that Class&#10;            cue.at[column[10:-5], 'Cue1'] = cue1&#10;            cue.at[column[10:-5], 'Cue2'] = cue2&#10;            cue.at[column[10:-5], 'Cue3'] = cue3&#10;&#10;    # Calculate the Knowledge metrics of the input&#10;    cue1 = cue[&quot;Cue1&quot;].sum()&#10;    cue2 = cue1 / data[&quot;total_123&quot;].sum()&#10;    cue3 = 1 - cue2&#10;    # Save the Knowledge metrics of the input&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue1'] = cue1&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue2'] = cue2&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue3'] = cue3&#10;&#10;    # Return the 3 DataFrames for RapidMiner usage&#10;    return data, cue, DTF"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Cross" width="90" x="1184" y="442">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\analysis-step\CrossData.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Cue" width="90" x="1184" y="340">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\analysis-step\CueData.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Filtered" width="90" x="1184" y="238">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\analysis-step\FilteredData.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="82" name="KnowledgeRMAnalyser" width="90" x="246" y="493">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;&#10;# List of stopWords&#10;mySQLStopWords = ['able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'amongst', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'around', 'aside', 'ask', 'asking', 'associated', 'available', 'away', 'awfully', 'became', 'because', 'become', 'becomes','becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'came', 'can', 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', 'course', 'currently', 'definitely', 'described', 'despite', 'did', 'different', 'does', 'doing', 'done','down', 'downwards', 'during', 'each', 'edu', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'get', 'gets', 'getting', 'given', 'gives', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'had', 'happens', 'hardly', 'has', 'have', 'having', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'ignored', 'immediate', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'its', 'itself', 'just', 'keep', 'keeps', 'kept', 'know','known', 'knows', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'mainly', 'many', 'may', 'maybe', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'myself','name', 'namely', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'obviously', 'off', 'often', 'okay', 'old', 'once', 'one', 'ones', 'only', 'onto', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall','own', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'que', 'quite', 'rather', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', 'since', 'six', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 'take', 'taken', 'tell', 'tends', 'than', 'thank', 'thanks', 'thanx', 'that', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein','theres', 'thereupon', 'these', 'they', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'upon', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'value', 'various', 'very', 'via', 'viz', 'want', 'wants', 'was', 'way', 'welcome', 'well', 'went', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', 'would', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'zero']&#10;&#10;# Function that tokenize on capitalLetters the SubjectTerm, obtaining the simple words of its composition as strings separated by &quot; &quot;&#10;def tokenTerm(term, row):&#10;    # Get the lenght of the string to tokenize&#10;    lenght = len(term)&#10;    # Create the index used to scan the string&#10;    index = 0&#10;    # Create a list with the characters which compose the term&#10;    termL = list(term)&#10;    # Iterate over every character of the list&#10;    for term in termL:&#10;        # If the character is capitalized&#10;        if(termL[index]).isupper():&#10;            # Scan over the remaining right part of the term, excluding the character already visited&#10;            for index_ in range(index+1, lenght):&#10;                # If the next character is not a capital letter&#10;                if((termL[index_].islower())):&#10;                    # Make sure the previous character is a capitalLetter&#10;                    termL[index_-1] = termL[index_-1].upper()&#10;                    # Update the index to avoid useless controls&#10;                    index = index_-1&#10;                    # End the iteration&#10;                    break&#10;                # Otherwise if the next character is a capitalLetter&#10;                elif(termL[index_].isupper()):&#10;                    # Modify it to obtain a non capital letter(in order not to exclude words composed only in capitalLetters)&#10;                    termL[index_] = termL[index_].lower()&#10;        # Update the index&#10;        index += 1&#10;&#10;    # Create a term from the correcterd list of characters obtained&#10;    term = &quot;&quot;.join(termL)&#10;&#10;    # Create the returning string of tokens&#10;    pTok = &quot;&quot;&#10;&#10;    # Scan over the term in inverse order to tokenize the term&#10;    for index in range(lenght-1, -1, -1):&#10;        # If a character is a capitalLetter, a digit, or the first one(last one chronologically)&#10;        if(term[index].isupper() or term[index].isdigit() or index == 0):&#10;            # If the subTerm is &gt;2, and its total lower is not present on the list of mySQLStopWords&#10;            if((lenght-index)&gt;2 and (term[index:lenght].lower() not in mySQLStopWords)):&#10;                # Add the subTerm to the string of tokens &#10;                pTok = pTok + &quot; &quot; + term[index:lenght].lower()&#10;            # Update the index of the last character&#10;            lenght = index&#10;&#10;    # Return the string of tokens&#10;    return pTok&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(triples, predicates = pd.DataFrame()):&#10;    # Create the DataFrame used to create the FCA matrix  &#10;    matrix = pd.DataFrame(columns=[&quot;Type&quot;, &quot;TypeTerm&quot;, &quot;Properties&quot;, &quot;PropertiesTerms&quot;, &quot;PropertiesTokens&quot;])&#10;&#10;    # Sort the DataFrame&#10;    triples = triples.sort_values(&quot;Object&quot;)&#10;&#10;    # Create the strings used to store multiple values in the same row, using &quot; - &quot; as separator&#10;    obj = &quot;&quot;&#10;    prop = &quot;&quot;&#10;    propTerms = &quot;&quot;&#10;    propTokens = &quot;&quot;&#10;    # Dictionary used to store the triple&#10;    dict_ = dict()&#10;&#10;    # Iterate over every triples row&#10;    for index, row in triples.iterrows():&#10;        # Check if the triple has to be saved, if there is a predicate selection then checks if that predicate has to be saved&#10;        bool_ = False&#10;        # If there is no predicate selection then save every triple&#10;        if(len(predicates) == 0):&#10;            bool_ = True&#10;        # If there is a predicate selection then check if that predicate has to be saved&#10;        else:&#10;            for pred in predicates[predicates.columns[0]]:&#10;                if(pred == str(row[&quot;PredicateTerm&quot;]) or pred == str(row[&quot;Predicate&quot;])):&#10;                    bool_ = True&#10;                    break&#10;        # Check if the triple has to be saved&#10;        if(bool_ == True):&#10;            # If the object value on the row has changed(first row or a new object)&#10;            if(row[&quot;Object&quot;] != obj):&#10;                # If the name of the object is not null&#10;                if(len(obj)):&#10;                    # Add to the dictionary the latest values of the row&#10;                    dict_[&quot;Properties&quot;] = prop[2:] &#10;                    dict_[&quot;PropertiesTerms&quot;] = propTerms[3:] &#10;                    dict_[&quot;PropertiesTokens&quot;] = propTokens[3:] &#10;                    # Store the row in the matrix&#10;                    matrix = matrix.append(dict_, ignore_index=True)&#10;                # Reset the name of the new object&#10;                obj = row[&quot;Object&quot;]&#10;                # Reset the other values of the row&#10;                prop = &quot;&quot;&#10;                propTerms = &quot;&quot;&#10;                propTokens = &quot;&quot;&#10;                # Store in the dictionary the fixed values of the row&#10;                dict_ = {&quot;Type&quot;: &quot; &quot; + row[&quot;Object&quot;], &quot;TypeTerm&quot;: row[&quot;ObjectTerm&quot;]}&#10;            &#10;            # Add the info on the row to the strings containing multiple values&#10;            prop = prop + &quot; - &quot; + row[&quot;Subject&quot;]&#10;            propTerms = propTerms + &quot; - &quot; + row[&quot;SubjectTerm&quot;]&#10;            &#10;            # Tokenize on capitalLetters the SubjectTerm obtaining the simple words of its composition as strings separated by &quot; &quot;&#10;            pTok = tokenTerm(row[&quot;SubjectTerm&quot;], row)&#10;            # Add the info of the tokens to the string containing multiple values&#10;            propTokens = propTokens + &quot; - &quot; + pTok&#10;&#10;    # Update the last row with the latest info&#10;    dict_[&quot;Properties&quot;] = prop[2:] &#10;    dict_[&quot;PropertiesTerms&quot;] = propTerms[3:] &#10;    dict_[&quot;PropertiesTokens&quot;] = propTokens[3:] &#10;    # Store the last row in the matrix&#10;    matrix = matrix.append(dict_, ignore_index=True)&#10;&#10;    # Set used to avoid the creation of the same column more than once&#10;    tokSet = set()&#10;    # Iterate over every row of the matrix&#10;    for index, row in matrix.iterrows():&#10;        # Create a list of tokens from that row's PropertiesTokens' cell&#10;        toks = [x for x in row[&quot;PropertiesTokens&quot;].replace(&quot;- &quot;, &quot;&quot;).split(&quot; &quot;) if x]&#10;        # For every token in toks&#10;        for tok in toks:&#10;            # Check if that token is already a column&#10;            setInd = len(tokSet)&#10;            tokSet.add(tok)&#10;            # If the token is new&#10;            if(setInd &lt; len(tokSet)):&#10;                # Create a column of 0 for that token&#10;                matrix[tok] = 0&#10;            # Update the value of the cell in the row of the matrix with tok as column(obtaining the number of token for that row)&#10;            matrix.at[index, tok] = matrix.at[index, tok] + 1&#10;    &#10;    # Return the DataFrame for RapidMiner usage&#10;    return matrix"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write FCA Matrix (2)" width="90" x="514" y="544">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\analysis-step\out2.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="82" name="KnowledgeFCALister" width="90" x="715" y="442">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(matrix):&#10;    # Generate the resulting DataFrame having as words the tokenized columns of the matrix, and a total of 0 for every row&#10;    data = pd.DataFrame({&quot;word&quot;: matrix.columns[6:], &quot;total&quot;: 0})&#10;&#10;    # Sort the DataFrame on the new columns&#10;    matrix = matrix.sort_values(&quot;TypeTerm&quot;)&#10;&#10;    # Use a set to avoid creating duplicate Columns&#10;    typeSet = set()&#10;    # Iterate over every row of the matrix&#10;    for index, row in matrix.iterrows():&#10;        # Generate the name of the new column&#10;        colName = &quot;in class (&quot; + row[&quot;TypeTerm&quot;] + &quot;)&quot;&#10;        # Check if that column is already present on the DataFrame&#10;        typeSetL = len(typeSet)&#10;        typeSet.add(row[&quot;TypeTerm&quot;])&#10;        # If there weren't columns with that name&#10;        if(typeSetL &lt; len(typeSet)):&#10;            # Create a new column of 0 for that name&#10;            data[colName] = 0&#10;&#10;        # Iterate over overy tokenized column of the matrix&#10;        i = 0&#10;        for column in matrix.columns[6:]:&#10;            # If the row has a value, then upload the values of the DataFrame&#10;            if(row[column]):&#10;                data.at[i, colName] = data.at[i, colName] + row[column]&#10;                data.at[i, &quot;total&quot;] = data.at[i, &quot;total&quot;] + row[column]&#10;            i+=1&#10;&#10;    # Sort the DataFrame on the words&#10;    data = data.sort_values(&quot;word&quot;)&#10;&#10;    # Return the DataFrame for RapidMiner usage&#10;    return data"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Data Matrix (2)" width="90" x="849" y="442">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\analysis-step\out.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="82" name="KnowledgeFCAReverser" width="90" x="715" y="646">
        <parameter key="script" value="# Import libraries&#10;from rdflib import Graph, Literal, RDFS, RDF, OWL, Namespace, URIRef&#10;import pandas as pd&#10;import os&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(matrix):&#10;    # Create the graph used to store the vocabulary&#10;    g = Graph()&#10;    # Create the Namespace for the vocabulary&#10;    n = Namespace(&quot;http://www.liveschema.org/test/&quot;)&#10;    g.bind(&quot;liveschema_test&quot;, n)&#10;    &#10;    # Create the DataFrame used to save the triples&#10;    triples = pd.DataFrame(columns=[&quot;Subject&quot;,&quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;,&quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;], )&#10;&#10;    # Iterate over all the rows of the matrix&#10;    for index, row in matrix.iterrows():&#10;        # Get the list of Properties of that row&#10;        subjList = row[&quot;Properties&quot;].split(&quot; -&quot;)&#10;        # Get the list of PropertiesTerms of that row&#10;        subjTermList = row[&quot;PropertiesTerms&quot;].split(&quot; -&quot;)&#10;        # Iterate over every Property&#10;        for i in range(0, len(subjTermList)):&#10;            # Save the triple about that Property being a domain of that row Type/Object&#10;            triples = triples.append({&quot;Subject&quot;: str(subjList[i]), &quot;Predicate&quot;: str(RDFS.domain), &quot;Object&quot;: str(row[&quot;Type&quot;]), &quot;SubjectTerm&quot;: subjTermList[i], &quot;PredicateTerm&quot;: &quot;domain&quot;, &quot;ObjectTerm&quot;: row[&quot;TypeTerm&quot;]}, ignore_index=True)&#10;            g.add((URIRef(subjList[i].replace(&quot; &quot;, &quot;&quot;)), RDFS.comment, URIRef(row[&quot;Type&quot;].replace(&quot; &quot;, &quot;&quot;))))&#10;&#10;    # Create the directory in which store the new vocabulary&#10;    location = os.path.normpath(os.path.expanduser(&quot;~/Desktop/K-Files/FCAConverted/&quot;))&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;    # Serialize the new vocabulary&#10;    g.serialize(destination=str(os.path.join(location, &quot;test.rdf&quot;)), format=&quot;pretty-xml&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.n3&quot;)), format=&quot;n3&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.nt&quot;)), format=&quot;nt&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.ttl&quot;)), format=&quot;turtle&quot;)&#10;    #g.serialize(destination=str(os.path.join(location, &quot;test.json-ld&quot;)), format=&quot;json-ld&quot;)&#10;&#10;    # Return the triples DataFrame for RapidMiner usage&#10;    return triples"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Reversed" width="90" x="849" y="646">
        <parameter key="excel_file" value="C:\Users\marco\Desktop\analysis-step\out.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <connect from_op="KnowledgeFilter" from_port="output 1" to_op="Set Role" to_port="example set input"/>
      <connect from_op="Set Role" from_port="example set output" to_op="Nominal to Text" to_port="example set input"/>
      <connect from_op="Nominal to Text" from_port="example set output" to_op="Replace (2)" to_port="example set input"/>
      <connect from_op="Replace (2)" from_port="example set output" to_op="Process Documents from Data" to_port="example set"/>
      <connect from_op="Process Documents from Data" from_port="example set" to_op="Write FCA Matrix" to_port="input"/>
      <connect from_op="Process Documents from Data" from_port="word list" to_op="WordList to Data" to_port="word list"/>
      <connect from_op="Write FCA Matrix" from_port="through" to_port="result 4"/>
      <connect from_op="WordList to Data" from_port="example set" to_op="Write Data Matrix" to_port="input"/>
      <connect from_op="KnowledgeAnalysis" from_port="output 1" to_op="Write Filtered" to_port="input"/>
      <connect from_op="KnowledgeAnalysis" from_port="output 2" to_op="Write Cue" to_port="input"/>
      <connect from_op="KnowledgeAnalysis" from_port="output 3" to_op="Write Cross" to_port="input"/>
      <connect from_op="Write Cross" from_port="through" to_port="result 3"/>
      <connect from_op="Write Cue" from_port="through" to_port="result 2"/>
      <connect from_op="Write Filtered" from_port="through" to_port="result 1"/>
      <connect from_op="KnowledgeRMAnalyser" from_port="output 1" to_op="Write FCA Matrix (2)" to_port="input"/>
      <connect from_op="KnowledgeFCALister" from_port="output 1" to_op="Write Data Matrix (2)" to_port="input"/>
      <connect from_op="KnowledgeFCAReverser" from_port="output 1" to_op="Write Reversed" to_port="input"/>
      <connect from_op="Write Reversed" from_port="through" to_port="result 5"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
      <portSpacing port="sink_result 3" spacing="0"/>
      <portSpacing port="sink_result 4" spacing="0"/>
      <portSpacing port="sink_result 5" spacing="0"/>
      <portSpacing port="sink_result 6" spacing="0"/>
    </process>
  </operator>
</process>
