<?xml version="1.0" encoding="UTF-8"?><process version="9.2.001">
  <context>
    <input/>
    <output/>
    <macros/>
  </context>
  <operator activated="true" class="process" compatibility="9.2.001" expanded="true" name="Process">
    <parameter key="logverbosity" value="init"/>
    <parameter key="logfile" value="/Users/mattiafumagalli/Desktop/prova.log"/>
    <parameter key="resultfile" value="/Users/mattiafumagalli/prova.res"/>
    <parameter key="random_seed" value="2001"/>
    <parameter key="send_mail" value="never"/>
    <parameter key="notification_email" value=""/>
    <parameter key="process_duration_for_mail" value="30"/>
    <parameter key="encoding" value="SYSTEM"/>
    <process expanded="true">
      <operator activated="true" class="read_excel" compatibility="9.2.001" expanded="true" height="68" name="Read Knowledge" width="90" x="45" y="340">
        <parameter key="excel_file" value="/home/marco/Desktop/Inh_OWL.xlsx"/>
        <parameter key="sheet_selection" value="sheet number"/>
        <parameter key="sheet_number" value="1"/>
        <parameter key="imported_cell_range" value="A1"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="first_row_as_names" value="true"/>
        <list key="annotations"/>
        <parameter key="date_format" value=""/>
        <parameter key="time_zone" value="SYSTEM"/>
        <parameter key="locale" value="English (United States)"/>
        <parameter key="read_all_values_as_polynominal" value="false"/>
        <list key="data_set_meta_data_information">
          <parameter key="0" value="Date.true.polynominal.attribute"/>
          <parameter key="1" value="Subject.true.polynominal.attribute"/>
          <parameter key="2" value="Predicate.true.polynominal.attribute"/>
          <parameter key="3" value="Object.true.polynominal.attribute"/>
          <parameter key="4" value="SubjectTerm.true.polynominal.attribute"/>
          <parameter key="5" value="PredicateTerm.true.polynominal.attribute"/>
          <parameter key="6" value="ObjectTerm.true.polynominal.attribute"/>
          <parameter key="7" value="Domain.true.polynominal.attribute"/>
          <parameter key="8" value="Domain Version.true.polynominal.attribute"/>
          <parameter key="9" value="Domain Date.true.polynominal.attribute"/>
          <parameter key="10" value="URI.true.polynominal.attribute"/>
          <parameter key="11" value="Title.true.polynominal.attribute"/>
          <parameter key="12" value="Languages.true.polynominal.attribute"/>
          <parameter key="13" value="Inherited.true.integer.attribute"/>
        </list>
        <parameter key="read_not_matching_values_as_missings" value="false"/>
        <parameter key="datamanagement" value="double_array"/>
        <parameter key="data_management" value="auto"/>
      </operator>
      <operator activated="true" class="multiply" compatibility="9.2.001" expanded="true" height="124" name="Multiply" width="90" x="179" y="340"/>
      <operator activated="true" class="set_macros" compatibility="9.2.001" expanded="true" height="103" name="Set Macros" width="90" x="112" y="901">
        <list key="macros">
          <parameter key="Predicates" value=" "/>
          <parameter key="reversedFileDestination" value="~/Desktop/K-Files/Converted/reversedConverted.ttl"/>
          <parameter key="mergedFileDestination" value="~/Desktop/K-Files/Converted/mergedConverted.ttl"/>
          <parameter key="comparisonThreshold" value="&gt; 0.9"/>
          <parameter key="NameSpace" value="http://liveschema.org/test"/>
        </list>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="124" name="KnowledgeFilter" width="90" x="447" y="799">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(triples):    &#10;    # Create the DataFrame to save the vocabs' triples with the predicates present on the argument 'predicates'&#10;    df = pd.DataFrame(columns=[&quot;Type&quot;, &quot;Property&quot;])&#10;&#10;    # Get the list of predicates&#10;    strPredicates = &quot;%{Predicates}&quot;&#10;&#10;    # Return all the triples if there is no predicate filtering&#10;    if(len(strPredicates.split()) == 0):&#10;        df[&quot;Type&quot;] = triples[&quot;ObjectTerm&quot;]&#10;        df[&quot;Property&quot;] = triples[&quot;SubjectTerm&quot;]&#10;        return df&#10;&#10;    # Iterate for every predicate&#10;    for pred in strPredicates.split():&#10;        # Create the list used to add the triples that has that predicate&#10;        list_ = list()&#10;        index_ = 0&#10;        # Iterate for every triples present on the file passed on the argument 'triples'&#10;        for index, row in triples.iterrows():&#10;            # if a triple has a specified PredicateTerm or the predicates are not set&#10;            if((str(row[&quot;PredicateTerm&quot;]) == str(pred)) or (str(row[&quot;Predicate&quot;]) == str(pred))):&#10;                # Save that triple on the list&#10;                list_.insert(index_,{&quot;Type&quot;: row[&quot;ObjectTerm&quot;], &quot;Property&quot;: row[&quot;SubjectTerm&quot;]})&#10;                index_ += 1&#10;        # Save the information on the list to the DataFrame for each predicate checked&#10;        if(index_ and len(list_)):&#10;            df = df.append(list_)&#10;&#10;    # Return the DataFrame for RapidMiner usage&#10;    return df"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="set_role" compatibility="9.2.001" expanded="true" height="82" name="Set Role" width="90" x="648" y="748">
        <parameter key="attribute_name" value="Type"/>
        <parameter key="target_role" value="label"/>
        <list key="set_additional_roles"/>
      </operator>
      <operator activated="true" class="nominal_to_text" compatibility="9.2.001" expanded="true" height="82" name="Nominal to Text" width="90" x="648" y="850">
        <parameter key="attribute_filter_type" value="all"/>
        <parameter key="attribute" value=""/>
        <parameter key="attributes" value=""/>
        <parameter key="use_except_expression" value="false"/>
        <parameter key="value_type" value="nominal"/>
        <parameter key="use_value_type_exception" value="false"/>
        <parameter key="except_value_type" value="file_path"/>
        <parameter key="block_type" value="single_value"/>
        <parameter key="use_block_type_exception" value="false"/>
        <parameter key="except_block_type" value="single_value"/>
        <parameter key="invert_selection" value="false"/>
        <parameter key="include_special_attributes" value="false"/>
      </operator>
      <operator activated="true" class="replace" compatibility="9.2.001" expanded="true" height="82" name="Replace (2)" width="90" x="782" y="748">
        <parameter key="attribute_filter_type" value="all"/>
        <parameter key="attribute" value=""/>
        <parameter key="attributes" value=""/>
        <parameter key="use_except_expression" value="false"/>
        <parameter key="value_type" value="nominal"/>
        <parameter key="use_value_type_exception" value="false"/>
        <parameter key="except_value_type" value="file_path"/>
        <parameter key="block_type" value="single_value"/>
        <parameter key="use_block_type_exception" value="false"/>
        <parameter key="except_block_type" value="single_value"/>
        <parameter key="invert_selection" value="false"/>
        <parameter key="include_special_attributes" value="false"/>
        <parameter key="replace_what" value="([A-Z])"/>
        <parameter key="replace_by" value=" $1"/>
      </operator>
      <operator activated="true" class="text:process_document_from_data" compatibility="8.1.000" expanded="true" height="82" name="Process Documents from Data" width="90" x="782" y="850">
        <parameter key="create_word_vector" value="true"/>
        <parameter key="vector_creation" value="Term Occurrences"/>
        <parameter key="add_meta_information" value="true"/>
        <parameter key="keep_text" value="true"/>
        <parameter key="prune_method" value="none"/>
        <parameter key="prune_below_percent" value="3.0"/>
        <parameter key="prune_above_percent" value="30.0"/>
        <parameter key="prune_below_rank" value="0.05"/>
        <parameter key="prune_above_rank" value="0.95"/>
        <parameter key="datamanagement" value="double_sparse_array"/>
        <parameter key="data_management" value="auto"/>
        <parameter key="select_attributes_and_weights" value="false"/>
        <list key="specify_weights"/>
        <process expanded="true">
          <operator activated="true" class="text:tokenize" compatibility="8.1.000" expanded="true" height="68" name="Tokenize (3)" width="90" x="112" y="34">
            <parameter key="mode" value="non letters"/>
            <parameter key="characters" value=".:"/>
            <parameter key="language" value="English"/>
            <parameter key="max_token_length" value="3"/>
          </operator>
          <operator activated="true" class="text:transform_cases" compatibility="8.1.000" expanded="true" height="68" name="Transform Cases (3)" width="90" x="246" y="34">
            <parameter key="transform_to" value="lower case"/>
          </operator>
          <operator activated="true" class="text:filter_stopwords_english" compatibility="8.1.000" expanded="true" height="68" name="Filter Stopwords (3)" width="90" x="380" y="34"/>
          <operator activated="true" class="text:filter_by_length" compatibility="8.1.000" expanded="true" height="68" name="Filter Tokens (by Length)" width="90" x="514" y="34">
            <parameter key="min_chars" value="3"/>
            <parameter key="max_chars" value="25"/>
          </operator>
          <connect from_port="document" to_op="Tokenize (3)" to_port="document"/>
          <connect from_op="Tokenize (3)" from_port="document" to_op="Transform Cases (3)" to_port="document"/>
          <connect from_op="Transform Cases (3)" from_port="document" to_op="Filter Stopwords (3)" to_port="document"/>
          <connect from_op="Filter Stopwords (3)" from_port="document" to_op="Filter Tokens (by Length)" to_port="document"/>
          <connect from_op="Filter Tokens (by Length)" from_port="document" to_port="document 1"/>
          <portSpacing port="source_document" spacing="0"/>
          <portSpacing port="sink_document 1" spacing="0"/>
          <portSpacing port="sink_document 2" spacing="0"/>
        </process>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write FCA Matrix" width="90" x="1988" y="1105">
        <parameter key="excel_file" value="~/Desktop/analysis-step/FCAMatrix.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" breakpoints="after" class="text:wordlist_to_data" compatibility="8.1.000" expanded="true" height="82" name="WordList to Data" width="90" x="916" y="799"/>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Data Matrix" width="90" x="1184" y="697">
        <parameter key="excel_file" value="~/Desktop/analysis-step/DataMatrix.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="multiply" compatibility="9.2.001" expanded="true" height="103" name="Multiply (4)" width="90" x="1318" y="697"/>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="145" name="KnowledgeAnalysis" width="90" x="1586" y="697">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(data):&#10;    # Drop the column 'in documents', that is equal to 'total'&#10;    if(&quot;in documents&quot; in data.columns):&#10;        data.drop(&quot;in documents&quot;, axis=1, inplace=True)&#10;&#10;    # Create the DataFrame used to save the Cues&#10;    cue = pd.DataFrame(columns=[&quot;Class&quot;,&quot;Cue1&quot;, &quot;Cue2&quot;, &quot;Cue3&quot;, &quot;Cue4&quot;, &quot;Cue5&quot;, &quot;Cue6&quot;])&#10;    # Iterate for every column present on data&#10;    for column in data:&#10;        # Rename the columns&#10;        if &quot;word&quot; not in column:&#10;            data.rename(index=str, columns={column: str(column+&quot;_456&quot;)}, inplace= True)&#10;            column += &quot;_456&quot;&#10;        # Checks if the column identify a Class&#10;        if(&quot;in class&quot; in column):&#10;            # Create the new column for the Cues in the input DataFrame, and calculate the values for every Element &#10;            index = data.columns.get_loc(column)&#10;            className = &quot;Cue(&quot; + column[10:-5] + &quot;)_456&quot;&#10;            tempColumn = data[column] / data[&quot;total_456&quot;]&#10;            data.insert(index, className, tempColumn)&#10;            &#10;            # Calculate the metrics of that Class&#10;            cue4 = data[className].sum()&#10;            cue5 = cue4 / data[column].sum()&#10;            cue6 = 1 - cue5&#10;            # Save the metrics of that Class&#10;            cue.at[column[10:-5], 'Class'] = column[10:-5]&#10;            cue.at[column[10:-5], 'Cue4'] = cue4&#10;            cue.at[column[10:-5], 'Cue5'] = cue5&#10;            cue.at[column[10:-5], 'Cue6'] = cue6&#10;&#10;            # Create the new column for the Cues in the input DataFrame, and copy the values for every Element &#10;            index = data.columns.get_loc(column)&#10;            className = column[0:-4] &#10;            tempColumn = data[column]&#10;            data.insert(index, className + &quot;_123&quot;, tempColumn)&#10;&#10;    # Calculate the Knowledge metrics of the input&#10;    cue4 = cue[&quot;Cue4&quot;].sum()&#10;    cue5 = cue4 / data[&quot;total_456&quot;].sum()&#10;    cue6 = 1 - cue5&#10;    # Save the Knowledge metrics of the input&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Class'] = &quot;KNOWLEDGE&quot;&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue4'] = cue4&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue5'] = cue5&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue6'] = cue6&#10;&#10;    # Create the new column for the Cues in the input DataFrame, and copy the values from the original column&#10;    index = data.columns.get_loc(&quot;total_456&quot;)&#10;    className = &quot;total_123&quot;&#10;    tempColumn = data[&quot;total_456&quot;]&#10;    data.insert(index, className, tempColumn)&#10;&#10;    # Create the DataFrame used to save the occurrences of the Names present on the Element row&#10;    DF = pd.DataFrame(columns=[&quot;Element&quot;, &quot;Names&quot;])&#10;    # Iterate for every row present on data, for every Element&#10;    for index, row in data.iterrows():&#10;        # Create a list to save the Names present on that row&#10;        list_ = list()&#10;        # For evert column that indicates a Name&#10;        for column in data:&#10;            # Check if the Name is present on that Element/row&#10;            if((&quot;in class&quot; in column and &quot;_456&quot; in column) and row[column]): &#10;                # Save the Name on the list&#10;                list_.append(column[10:-5])&#10;&#10;                # Format data for another kind of cue Analysis&#10;                if(row[column] &gt; 1):&#10;                    data.at[index, &quot;total_123&quot;] = row[&quot;total_123&quot;] - row[column] + 1 &#10;                    row[&quot;total_123&quot;] = row[&quot;total_123&quot;] - row[column] + 1&#10;                    data.at[index, column[0:-4] + &quot;_123&quot; ] = 1&#10;        # Save the correlation between Element and Names&#10;        DF = DF.append({&quot;Element&quot;: row[&quot;word&quot;], &quot;Names&quot;: list_}, ignore_index=True)&#10;&#10;    # Create the DataFrame used to save the table used to identify common Elements between Names&#10;    DTF = pd.DataFrame(columns=[&quot;total&quot;, &quot;Names&quot;, &quot;number&quot;, &quot;Elements&quot;])&#10;    # Create the set used to check if new Names has to be added or if existing Names has to be updated&#10;    set_ = set()&#10;    # Iterate for every row present on DF, for every Element and the relative Names&#10;    for index_, row in DF.iterrows():&#10;        # Check if new Names has to be added or if existing Names has to be updated&#10;        a = len(set_)&#10;        set_.add(str(row[&quot;Names&quot;]))&#10;        if(a &lt; len(set_)):&#10;            # Create a new row on the DataFrame for that Names&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;total&quot;] = len(row[&quot;Names&quot;])&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;Names&quot;] = row[&quot;Names&quot;]&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;Elements&quot;] = str(row[&quot;Element&quot;])&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;number&quot;] = 1&#10;        else:&#10;            # Update the row for that Names, adding the new Element&#10;            elements = str(DTF.at[str(row[&quot;Names&quot;]), &quot;Elements&quot;])&#10;            number = DTF.at[str(row[&quot;Names&quot;]), &quot;number&quot;]&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;Elements&quot;] = elements + &quot; , &quot; + str(row[&quot;Element&quot;])&#10;            DTF.at[str(row[&quot;Names&quot;]), &quot;number&quot;] = number + 1&#10;&#10;    # Iterate for every column present on data&#10;    for column in data:&#10;        # Checks if the column identify a Class&#10;        if((&quot;in class&quot; in column and &quot;_123&quot; in column)):&#10;            # Create the new column for the Cue in the input DataFrame, and calculate the values for every Element &#10;            index = data.columns.get_loc(column) - 1&#10;            className = &quot;Cue(&quot; + column[10:-5] + &quot;)_123&quot;&#10;            tempColumn = data[column] / data[&quot;total_123&quot;]&#10;            data.insert(index, className, tempColumn)&#10;            &#10;            # Calculate the metrics of that Class&#10;            cue1 = data[className].sum()&#10;            cue2 = cue1 / data[column].sum()&#10;            cue3 = 1 - cue2&#10;            # Save the metrics of that Class&#10;            cue.at[column[10:-5], 'Cue1'] = cue1&#10;            cue.at[column[10:-5], 'Cue2'] = cue2&#10;            cue.at[column[10:-5], 'Cue3'] = cue3&#10;&#10;    # Calculate the Knowledge metrics of the input&#10;    cue1 = cue[&quot;Cue1&quot;].sum()&#10;    cue2 = cue1 / data[&quot;total_123&quot;].sum()&#10;    cue3 = 1 - cue2&#10;    # Save the Knowledge metrics of the input&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue1'] = cue1&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue2'] = cue2&#10;    cue.at[&quot;KNOWLEDGE&quot;, 'Cue3'] = cue3&#10;&#10;    # Iterate for every column present on data to format a new table in a UpSet CSV input file&#10;    upSet = pd.DataFrame()&#10;    upSet[&quot;word&quot;] = data[&quot;word&quot;]&#10;    for column in data:&#10;        if(&quot;in class&quot; in column and &quot;_123&quot; in column):&#10;            upSet[column[10:-5]] = data[column]&#10;&#10;    # Return the 4 DataFrames for RapidMiner usage&#10;    return data, cue, DTF, upSet"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_csv" compatibility="9.2.001" expanded="true" height="82" name="Write UpSet format" width="90" x="1988" y="952">
        <parameter key="csv_file" value="~/Desktop/analysis-step/UpSetData.csv"/>
        <parameter key="column_separator" value=","/>
        <parameter key="write_attribute_names" value="true"/>
        <parameter key="quote_nominal_values" value="true"/>
        <parameter key="format_date_attributes" value="true"/>
        <parameter key="append_to_file" value="false"/>
        <parameter key="encoding" value="SYSTEM"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Cross" width="90" x="1988" y="748">
        <parameter key="excel_file" value="~/Desktop/analysis-step/CrossData.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Cue" width="90" x="1988" y="595">
        <parameter key="excel_file" value="~/Desktop/analysis-step/CueData.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Filtered" width="90" x="1988" y="442">
        <parameter key="excel_file" value="~/Desktop/analysis-step/FilteredData.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="124" name="KnowledgeRMAnalyser" width="90" x="447" y="340">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;&#10;# List of stopWords&#10;mySQLStopWords = ['able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'amongst', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'around', 'aside', 'ask', 'asking', 'associated', 'available', 'away', 'awfully', 'became', 'because', 'become', 'becomes','becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'came', 'can', 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', 'course', 'currently', 'definitely', 'described', 'despite', 'did', 'different', 'does', 'doing', 'done','down', 'downwards', 'during', 'each', 'edu', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'get', 'gets', 'getting', 'given', 'gives', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'had', 'happens', 'hardly', 'has', 'have', 'having', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'ignored', 'immediate', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'its', 'itself', 'just', 'keep', 'keeps', 'kept', 'know','known', 'knows', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'mainly', 'many', 'may', 'maybe', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'myself','name', 'namely', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'obviously', 'off', 'often', 'okay', 'old', 'once', 'one', 'ones', 'only', 'onto', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall','own', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'que', 'quite', 'rather', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', 'since', 'six', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 'take', 'taken', 'tell', 'tends', 'than', 'thank', 'thanks', 'thanx', 'that', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein','theres', 'thereupon', 'these', 'they', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'upon', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'value', 'various', 'very', 'via', 'viz', 'want', 'wants', 'was', 'way', 'welcome', 'well', 'went', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', 'would', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'zero']&#10;&#10;# Function that tokenize on capitalLetters the SubjectTerm, obtaining the simple words of its composition as strings separated by &quot; &quot;&#10;def tokenTerm(term):&#10;    # Get the lenght of the string to tokenize&#10;    lenght = len(term)&#10;    # Create the index used to scan the string&#10;    index = 0&#10;    # Create a list with the characters which compose the term&#10;    termL = list(term)&#10;    # Iterate over every character of the list&#10;    for t in termL:&#10;        # If the character is capitalized&#10;        if(termL[index]).isupper():&#10;            # Scan over the remaining right part of the term, excluding the character already visited&#10;            for index_ in range(index+1, lenght):&#10;                # If the next character is not a capital letter&#10;                if((termL[index_].islower())):&#10;                    # Make sure the previous character is a capitalLetter&#10;                    termL[index_-1] = termL[index_-1].upper()&#10;                    # End the iteration&#10;                    break&#10;                # Otherwise if the next character is a capitalLetter&#10;                elif(termL[index_].isupper()):&#10;                    # Modify it to obtain a non capital letter(in order not to exclude words composed only in capitalLetters)&#10;                    termL[index_] = termL[index_].lower()&#10;        # Update the index&#10;        index += 1&#10;&#10;    # Create a term from the correcterd list of characters obtained&#10;    term = &quot;&quot;.join(termL)&#10;&#10;    # Create the returning string of tokens&#10;    pTok = &quot;&quot;&#10;&#10;    # Scan over the term in inverse order to tokenize the term&#10;    for index in range(lenght-1, -1, -1):&#10;        # If a character is a capitalLetter, a digit, or the first one(last one chronologically)&#10;        if(term[index].isupper() or term[index].isdigit() or index == 0):&#10;            # If the subTerm is &gt;2, and its total lower is not present on the list of mySQLStopWords&#10;            if((lenght-index)&gt;2 and (term[index:lenght].lower() not in mySQLStopWords)):&#10;                # Add the subTerm to the string of tokens &#10;                pTok = pTok + &quot; &quot; + term[index:lenght].lower()&#10;            # Update the index of the last character&#10;            lenght = index&#10;&#10;    # Return the string of tokens&#10;    return pTok&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(triples):&#10;    # Create the DataFrame used to create the FCA matrix  &#10;    matrix = pd.DataFrame(columns=[&quot;Type&quot;, &quot;TypeTerm&quot;, &quot;Properties&quot;, &quot;PropertiesTerms&quot;, &quot;PropertiesTokens&quot;])&#10;&#10;    # Sort the DataFrame&#10;    triples = triples.sort_values(&quot;Object&quot;)&#10;&#10;    # Create the strings used to store multiple values in the same row, using &quot; - &quot; as separator&#10;    obj = &quot;&quot;&#10;    prop = &quot;&quot;&#10;    propTerms = &quot;&quot;&#10;    propTokens = &quot;&quot;&#10;    # Dictionary used to store the triple&#10;    dict_ = dict()&#10;&#10;    # Iterate over every triples row&#10;    for index, row in triples.iterrows():&#10;        # Check if the triple has to be saved, if there is a predicate selection then checks if that predicate has to be saved&#10;        bool_ = False&#10;        # If there is no predicate selection then save every triple&#10;        strPredicates = &quot;%{Predicates}&quot;&#10;        if(len(strPredicates.split()) == 0):&#10;            bool_ = True&#10;        # If there is a predicate selection then check if that predicate has to be saved&#10;        else:&#10;            for pred in strPredicates.split():&#10;                if(pred == str(row[&quot;PredicateTerm&quot;]) or pred == str(row[&quot;Predicate&quot;])):&#10;                    bool_ = True&#10;                    break&#10;        # Check if the triple has to be saved&#10;        if(bool_ == True):&#10;            # If the object value on the row has changed(first row or a new object)&#10;            if(row[&quot;Object&quot;] != obj):&#10;                # If the name of the object is not null&#10;                if(len(obj)):&#10;                    # Add to the dictionary the latest values of the row&#10;                    dict_[&quot;Properties&quot;] = prop[2:] &#10;                    dict_[&quot;PropertiesTerms&quot;] = propTerms[3:] &#10;                    dict_[&quot;PropertiesTokens&quot;] = propTokens[3:] &#10;                    # Store the row in the matrix&#10;                    matrix = matrix.append(dict_, ignore_index=True)&#10;                # Reset the name of the new object&#10;                obj = row[&quot;Object&quot;]&#10;                # Reset the other values of the row&#10;                prop = &quot;&quot;&#10;                propTerms = &quot;&quot;&#10;                propTokens = &quot;&quot;&#10;                # Store in the dictionary the fixed values of the row&#10;                dict_ = {&quot;Type&quot;: &quot; &quot; + row[&quot;Object&quot;], &quot;TypeTerm&quot;: row[&quot;ObjectTerm&quot;]}&#10;            &#10;            # Add the info on the row to the strings containing multiple values&#10;            prop = prop + &quot; - &quot; + row[&quot;Subject&quot;]&#10;            propTerms = propTerms + &quot; - &quot; + row[&quot;SubjectTerm&quot;]&#10;            &#10;            # Tokenize on capitalLetters the SubjectTerm obtaining the simple words of its composition as strings separated by &quot; &quot;&#10;            pTok = tokenTerm(row[&quot;SubjectTerm&quot;])&#10;            # Add the info of the tokens to the string containing multiple values&#10;            propTokens = propTokens + &quot; - &quot; + pTok&#10;&#10;    # Update the last row with the latest info&#10;    dict_[&quot;Properties&quot;] = prop[2:] &#10;    dict_[&quot;PropertiesTerms&quot;] = propTerms[3:] &#10;    dict_[&quot;PropertiesTokens&quot;] = propTokens[3:] &#10;    # Store the last row in the matrix&#10;    matrix = matrix.append(dict_, ignore_index=True)&#10;&#10;    # Set used to avoid the creation of the same column more than once&#10;    tokSet = set()&#10;    # Iterate over every row of the matrix&#10;    for index, row in matrix.iterrows():&#10;        # Create a list of tokens from that row's PropertiesTokens' cell&#10;        toks = [x for x in row[&quot;PropertiesTokens&quot;].replace(&quot;- &quot;, &quot;&quot;).split(&quot; &quot;) if x]&#10;        # For every token in toks&#10;        for tok in toks:&#10;            # Check if that token is already a column&#10;            setInd = len(tokSet)&#10;            tokSet.add(tok)&#10;            # If the token is new&#10;            if(setInd &lt; len(tokSet)):&#10;                # Create a column of 0 for that token&#10;                matrix[tok] = 0&#10;            # Update the value of the cell in the row of the matrix with tok as column(obtaining the number of token for that row)&#10;            matrix.at[index, tok] = matrix.at[index, tok] + 1&#10;    &#10;    # Return the DataFrame for RapidMiner usage&#10;    return matrix"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write FCA Matrix (2)" width="90" x="648" y="340">
        <parameter key="excel_file" value="~/Desktop/analysis-step/FCAMatrix.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="multiply" compatibility="9.2.001" expanded="true" height="103" name="Multiply (2)" width="90" x="782" y="340"/>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="103" name="KnowledgeFCALister" width="90" x="983" y="493">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(matrix):&#10;    # Generate the resulting DataFrame having as words the tokenized columns of the matrix, and a total of 0 for every row&#10;    data = pd.DataFrame({&quot;word&quot;: matrix.columns[6:], &quot;total&quot;: 0})&#10;&#10;    # Sort the DataFrame on the new columns&#10;    matrix = matrix.sort_values(&quot;TypeTerm&quot;)&#10;&#10;    # Use a set to avoid creating duplicate Columns&#10;    typeSet = set()&#10;    # Iterate over every row of the matrix&#10;    for index, row in matrix.iterrows():&#10;        # Generate the name of the new column&#10;        colName = &quot;in class (&quot; + row[&quot;TypeTerm&quot;] + &quot;)&quot;&#10;        # Check if that column is already present on the DataFrame&#10;        typeSetL = len(typeSet)&#10;        typeSet.add(row[&quot;TypeTerm&quot;])&#10;        # If there weren't columns with that name&#10;        if(typeSetL &lt; len(typeSet)):&#10;            # Create a new column of 0 for that name&#10;            data[colName] = 0&#10;&#10;        # Iterate over overy tokenized column of the matrix&#10;        i = 0&#10;        for column in matrix.columns[6:]:&#10;            # If the row has a value, then upload the values of the DataFrame&#10;            if(row[column]):&#10;                data.at[i, colName] = data.at[i, colName] + row[column]&#10;                data.at[i, &quot;total&quot;] = data.at[i, &quot;total&quot;] + row[column]&#10;            i+=1&#10;&#10;    # Sort the DataFrame on the words&#10;    data = data.sort_values(&quot;word&quot;)&#10;&#10;    # Return the DataFrame for RapidMiner usage&#10;    return data"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="transpose" compatibility="9.2.001" expanded="true" height="82" name="Transpose" width="90" x="1318" y="442"/>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="103" name="KnowledgeFCAReverser" width="90" x="983" y="136">
        <parameter key="script" value="# Import libraries&#10;from rdflib import Graph, Literal, RDFS, RDF, OWL, Namespace, URIRef&#10;import pandas as pd&#10;import os&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(matrix):&#10;    # Create the graph used to store the vocabulary&#10;    g = Graph()&#10;    # Create the Namespace for the vocabulary&#10;    strNameSpace = &quot;%{NameSpace}&quot;&#10;    n = Namespace(strNameSpace)&#10;    g.bind(strNameSpace.split(&quot;/&quot;)[-1], n)&#10;    &#10;    # Create the DataFrame used to save the triples&#10;    triples = pd.DataFrame(columns=[&quot;Subject&quot;,&quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;,&quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;], )&#10;&#10;    # Iterate over all the rows of the matrix&#10;    for index, row in matrix.iterrows():&#10;        # Get the list of Properties of that row&#10;        subjList = row[&quot;Properties&quot;].split(&quot; -&quot;)&#10;        # Get the list of PropertiesTerms of that row&#10;        subjTermList = row[&quot;PropertiesTerms&quot;].split(&quot; -&quot;)&#10;        # Iterate over every Property&#10;        for i in range(0, len(subjTermList)):&#10;            # Save the triple about that Property being a domain of that row Type/Object&#10;            triples = triples.append({&quot;Subject&quot;: str(subjList[i]), &quot;Predicate&quot;: str(RDFS.domain), &quot;Object&quot;: str(row[&quot;Type&quot;]), &quot;SubjectTerm&quot;: subjTermList[i], &quot;PredicateTerm&quot;: &quot;domain&quot;, &quot;ObjectTerm&quot;: row[&quot;TypeTerm&quot;]}, ignore_index=True)&#10;            g.add((URIRef(subjList[i].replace(&quot; &quot;, &quot;&quot;)), RDFS.comment, URIRef(row[&quot;Type&quot;].replace(&quot; &quot;, &quot;&quot;))))&#10;&#10;    # Create the directory in which store the new vocabulary&#10;    reversedFileDestination = &quot;%{reversedFileDestination}&quot;&#10;    location = os.path.normpath(os.path.expanduser(&quot;/&quot;.join(reversedFileDestination.split(&quot;/&quot;)[0:-1])))&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;    # Serialize the new vocabulary&#10;    if(&quot;rdf&quot; in reversedFileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, reversedFileDestination.split(&quot;/&quot;)[-1])), format=&quot;pretty-xml&quot;)&#10;    if(&quot;n3&quot; in reversedFileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, reversedFileDestination.split(&quot;/&quot;)[-1])), format=&quot;n3&quot;)&#10;    if(&quot;nt&quot; in reversedFileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, reversedFileDestination.split(&quot;/&quot;)[-1])), format=&quot;nt&quot;)&#10;    if(&quot;ttl&quot; in reversedFileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, reversedFileDestination.split(&quot;/&quot;)[-1])), format=&quot;turtle&quot;)&#10;    if(&quot;json&quot; in reversedFileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, reversedFileDestination.split(&quot;/&quot;)[-1])), format=&quot;json-ld&quot;)&#10;&#10;    # Return the triples DataFrame for RapidMiner usage&#10;    return triples"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Reversed" width="90" x="1988" y="85">
        <parameter key="excel_file" value="~/Desktop/analysis-step/FCAReversed.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <operator activated="true" class="data_to_similarity" compatibility="9.2.001" expanded="true" height="82" name="Data to Similarity" width="90" x="1251" y="340">
        <parameter key="measure_types" value="NominalMeasures"/>
        <parameter key="mixed_measure" value="MixedEuclideanDistance"/>
        <parameter key="nominal_measure" value="JaccardSimilarity"/>
        <parameter key="numerical_measure" value="EuclideanDistance"/>
        <parameter key="divergence" value="GeneralizedIDivergence"/>
        <parameter key="kernel_type" value="radial"/>
        <parameter key="kernel_gamma" value="1.0"/>
        <parameter key="kernel_sigma1" value="1.0"/>
        <parameter key="kernel_sigma2" value="0.0"/>
        <parameter key="kernel_sigma3" value="2.0"/>
        <parameter key="kernel_degree" value="3.0"/>
        <parameter key="kernel_shift" value="1.0"/>
        <parameter key="kernel_a" value="1.0"/>
        <parameter key="kernel_b" value="0.0"/>
      </operator>
      <operator activated="true" class="similarity_to_data" compatibility="9.2.001" expanded="true" height="82" name="Similarity to Data" width="90" x="1385" y="340">
        <parameter key="table_type" value="long_table"/>
      </operator>
      <operator activated="true" class="python_scripting:execute_python" compatibility="9.2.000" expanded="true" height="124" name="KnowledgeSimilarityMerger" width="90" x="1653" y="340">
        <parameter key="script" value="# Import libraries&#10;import pandas as pd&#10;from collections import defaultdict&#10;from rdflib import Graph, Literal, RDFS, RDF, OWL, Namespace, URIRef&#10;import os&#10;&#10;# Mandatory function for RapidMiner&#10;def rm_main(triples, similarities):&#10;    # Create the defaultdict used to store the combination of ObjectTerm and SubjectTerms&#10;    comb = set()&#10;    # Iterate over every triples row&#10;    for index, row in similarities.iterrows():&#10;        if(&quot;in class&quot; in row[&quot;FIRST_ID&quot;] and &quot;in class&quot; in row[&quot;SECOND_ID&quot;] and row[similarities.columns[2]] %{comparisonThreshold}):&#10;            # Set a boolean to know if that combination is already in the defaultdict&#10;            b = False&#10;            # Iterate over every key of the defaultdict&#10;            for key in comb:&#10;                # Create a list of the keys for every key of the defaultdict&#10;                keys = key.split()&#10;                # If both the typeTerms are in the keys then there is nothing to add&#10;                if(row[&quot;FIRST_ID&quot;][10:-1] in keys and row[&quot;SECOND_ID&quot;][10:-1] in keys):&#10;                    # Set the boolean to True and exit the for cycle since the combination is in the defaultdict&#10;                    b=True&#10;                    break&#10;                # If a typeTerm is already in the defaultdict and the other typeTerm is not, add the other typeTerm to the combination of the first typeTerm in the defaultdict&#10;                if(not b and row[&quot;FIRST_ID&quot;][10:-1] in keys and row[&quot;SECOND_ID&quot;][10:-1] not in keys):&#10;                    # Delete the old combination&#10;                    comb.remove(key)&#10;                    # Generate the new key for the combination adding the new typeTerm&#10;                    key = key + &quot; &quot; + row[&quot;SECOND_ID&quot;][10:-1]&#10;                    # Update the defaultdict with the new combination&#10;                    comb.add(key)&#10;                    # Set the boolean to True and exit the for cycle since the combination is in the defaultdict now&#10;                    b=True&#10;                    break&#10;                # If a typeTerm is already in the defaultdict and the other typeTerm is not, add the other typeTerm to the combination of the first typeTerm in the defaultdict&#10;                if(not b and row[&quot;SECOND_ID&quot;][10:-1] in keys and row[&quot;FIRST_ID&quot;][10:-1] not in keys):&#10;                    # Delete the old combination&#10;                    comb.remove(key)&#10;                    # Generate the new key for the combination adding the new typeTerm&#10;                    key = key + &quot; &quot; + row[&quot;FIRST_ID&quot;][10:-1]&#10;                    # Update the defaultdict with the new combination&#10;                    comb.add(key)&#10;                    # Set the boolean to True and exit the for cycle since the combination is in the defaultdict now&#10;                    b=True&#10;                    break&#10;&#10;            # If the combination is not in the defaultdict&#10;            if(not b):&#10;                # Add the combination to the defaultdict&#10;                comb.add(row[&quot;FIRST_ID&quot;][10:-1]+&quot; &quot;+row[&quot;SECOND_ID&quot;][10:-1])&#10;    &#10;    # Create the graph used to store the vocabulary&#10;    g = Graph()&#10;    # Create the Namespace for the vocabulary&#10;    strNameSpace = &quot;%{NameSpace}&quot;&#10;    n = Namespace(strNameSpace)&#10;    g.bind(strNameSpace.split(&quot;/&quot;)[-1], n)&#10;&#10;    # Create the DataFrame to save the vocabs' Date of parsing, Subject, Predicate, Object, Domain, Domain Version, Domain Date, URI, Title, Languages&#10;    newTriples = pd.DataFrame(columns=[&quot;Date&quot;, &quot;Subject&quot;, &quot;Predicate&quot;, &quot;Object&quot;, &quot;SubjectTerm&quot;, &quot;PredicateTerm&quot;, &quot;ObjectTerm&quot;, &quot;Domain&quot;, &quot;Domain Version&quot;, &quot;Domain Date&quot;, &quot;URI&quot;, &quot;Title&quot;, &quot;Languages&quot;])&#10;    &#10;    # Iterate over every triples row&#10;    for index, row in triples.iterrows():&#10;        # Boolean used to identify if the row has been added as jaccard value&#10;        jBool = True&#10;        # Iterate over every jaccard combination of &#10;        for key in comb:&#10;            if(jBool and row[&quot;ObjectTerm&quot;] in key.split()):&#10;                objTerms = &quot;_-_&quot;.join(key.split())&#10;                # Save the triple&#10;                newTriples = newTriples.append({&quot;Date&quot;: row[&quot;Date&quot;], &quot;Subject&quot;: row[&quot;Subject&quot;], &quot;Predicate&quot;: row[&quot;Predicate&quot;], &quot;Object&quot;: n[objTerms], &quot;SubjectTerm&quot;: row[&quot;SubjectTerm&quot;], &quot;PredicateTerm&quot;: row[&quot;PredicateTerm&quot;], &quot;ObjectTerm&quot;: objTerms, &quot;Domain&quot;: row[&quot;Domain&quot;], &quot;Domain Version&quot;: row[&quot;Domain Version&quot;], &quot;Domain Date&quot;: row[&quot;Domain Date&quot;], &quot;URI&quot;: row[&quot;URI&quot;], &quot;Title&quot;: row[&quot;Title&quot;], &quot;Languages&quot;: row[&quot;Languages&quot;]}, ignore_index=True)&#10;                g.add((URIRef(row[&quot;Subject&quot;]), URIRef(row[&quot;Predicate&quot;]), n[objTerms]))&#10;                jBool = False&#10;            if(jBool and row[&quot;SubjectTerm&quot;] in key.split()):&#10;                subjTerms = &quot;_-_&quot;.join(key.split())&#10;                # Save the triple&#10;                newTriples = newTriples.append({&quot;Date&quot;: row[&quot;Date&quot;], &quot;Subject&quot;: n[subjTerms], &quot;Predicate&quot;: row[&quot;Predicate&quot;], &quot;Object&quot;: row[&quot;Object&quot;], &quot;SubjectTerm&quot;: subjTerms, &quot;PredicateTerm&quot;: row[&quot;PredicateTerm&quot;], &quot;ObjectTerm&quot;: row[&quot;ObjectTerm&quot;], &quot;Domain&quot;: row[&quot;Domain&quot;], &quot;Domain Version&quot;: row[&quot;Domain Version&quot;], &quot;Domain Date&quot;: row[&quot;Domain Date&quot;], &quot;URI&quot;: row[&quot;URI&quot;], &quot;Title&quot;: row[&quot;Title&quot;], &quot;Languages&quot;: row[&quot;Languages&quot;]}, ignore_index=True)&#10;                # Add the triple to the graph as URIRef or Literal respectively&#10;                if(len(row[&quot;Object&quot;]) &gt; 5 and &quot;http&quot; == row[&quot;Object&quot;][0:3]):&#10;                    g.add((n[subjTerms], URIRef(row[&quot;Predicate&quot;]), URIRef(row[&quot;Object&quot;])))&#10;                else:&#10;                    g.add((n[subjTerms], URIRef(row[&quot;Predicate&quot;]), Literal(row[&quot;Object&quot;])))&#10;                jBool = False&#10;        if(jBool):&#10;            # Save the triple&#10;            newTriples = newTriples.append({&quot;Date&quot;: row[&quot;Date&quot;], &quot;Subject&quot;: row[&quot;Subject&quot;], &quot;Predicate&quot;: row[&quot;Predicate&quot;], &quot;Object&quot;: row[&quot;Object&quot;], &quot;SubjectTerm&quot;: row[&quot;SubjectTerm&quot;], &quot;PredicateTerm&quot;: row[&quot;PredicateTerm&quot;], &quot;ObjectTerm&quot;: row[&quot;ObjectTerm&quot;], &quot;Domain&quot;: row[&quot;Domain&quot;], &quot;Domain Version&quot;: row[&quot;Domain Version&quot;], &quot;Domain Date&quot;: row[&quot;Domain Date&quot;], &quot;URI&quot;: row[&quot;URI&quot;], &quot;Title&quot;: row[&quot;Title&quot;], &quot;Languages&quot;: row[&quot;Languages&quot;]}, ignore_index=True)&#10;            # Add the triple to the graph as URIRef or Literal respectively&#10;            if(len(row[&quot;Object&quot;]) &gt; 5 and &quot;http&quot; == row[&quot;Object&quot;][0:3]):&#10;                g.add((URIRef(row[&quot;Subject&quot;]), URIRef(row[&quot;Predicate&quot;]), URIRef(row[&quot;Object&quot;])))&#10;            else:&#10;                g.add((URIRef(row[&quot;Subject&quot;]), URIRef(row[&quot;Predicate&quot;]), Literal(row[&quot;Object&quot;])))&#10;    &#10;    # Create the directory in which store the new vocabulary&#10;    mergedFileDestination = &quot;%{mergedFileDestination}&quot;&#10;    location = os.path.normpath(os.path.expanduser(&quot;/&quot;.join(mergedFileDestination.split(&quot;/&quot;)[0:-1])))&#10;    if not os.path.isdir(location):&#10;        os.makedirs(location)&#10;    # Serialize the new vocabulary&#10;    if(&quot;rdf&quot; in mergedFileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, mergedFileDestination.split(&quot;/&quot;)[-1])), format=&quot;pretty-xml&quot;)&#10;    if(&quot;n3&quot; in mergedFileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, mergedFileDestination.split(&quot;/&quot;)[-1])), format=&quot;n3&quot;)&#10;    if(&quot;nt&quot; in mergedFileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, mergedFileDestination.split(&quot;/&quot;)[-1])), format=&quot;nt&quot;)&#10;    if(&quot;ttl&quot; in mergedFileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, mergedFileDestination.split(&quot;/&quot;)[-1])), format=&quot;turtle&quot;)&#10;    if(&quot;json&quot; in mergedFileDestination.split(&quot;.&quot;)[-1]):&#10;        g.serialize(destination=str(os.path.join(location, mergedFileDestination.split(&quot;/&quot;)[-1])), format=&quot;json-ld&quot;)&#10;    &#10;    # Return the DataFrame for RapidMiner usage&#10;    return newTriples"/>
        <parameter key="use_default_python" value="true"/>
        <parameter key="package_manager" value="conda (anaconda)"/>
      </operator>
      <operator activated="true" class="write_excel" compatibility="9.2.001" expanded="true" height="82" name="Write Merged" width="90" x="1988" y="289">
        <parameter key="excel_file" value="~/Desktop/analysis-step/SimilarityMerged.xlsx"/>
        <parameter key="file_format" value="xlsx"/>
        <parameter key="encoding" value="SYSTEM"/>
        <parameter key="sheet_name" value="RapidMiner Data"/>
        <parameter key="date_format" value="yyyy-MM-dd HH:mm:ss"/>
        <parameter key="number_format" value="#.0"/>
      </operator>
      <connect from_op="Read Knowledge" from_port="output" to_op="Multiply" to_port="input"/>
      <connect from_op="Multiply" from_port="output 1" to_op="KnowledgeRMAnalyser" to_port="input 1"/>
      <connect from_op="Multiply" from_port="output 2" to_op="KnowledgeFilter" to_port="input 1"/>
      <connect from_op="Multiply" from_port="output 3" to_op="KnowledgeSimilarityMerger" to_port="input 1"/>
      <connect from_op="Set Macros" from_port="through 1" to_op="KnowledgeRMAnalyser" to_port="input 2"/>
      <connect from_op="Set Macros" from_port="through 2" to_op="KnowledgeFilter" to_port="input 2"/>
      <connect from_op="KnowledgeFilter" from_port="output 1" to_op="Set Role" to_port="example set input"/>
      <connect from_op="Set Role" from_port="example set output" to_op="Nominal to Text" to_port="example set input"/>
      <connect from_op="Nominal to Text" from_port="example set output" to_op="Replace (2)" to_port="example set input"/>
      <connect from_op="Replace (2)" from_port="example set output" to_op="Process Documents from Data" to_port="example set"/>
      <connect from_op="Process Documents from Data" from_port="example set" to_op="Write FCA Matrix" to_port="input"/>
      <connect from_op="Process Documents from Data" from_port="word list" to_op="WordList to Data" to_port="word list"/>
      <connect from_op="Write FCA Matrix" from_port="through" to_port="result 4"/>
      <connect from_op="Write Data Matrix" from_port="through" to_op="Multiply (4)" to_port="input"/>
      <connect from_op="Multiply (4)" from_port="output 1" to_op="Transpose" to_port="example set input"/>
      <connect from_op="Multiply (4)" from_port="output 2" to_op="KnowledgeAnalysis" to_port="input 1"/>
      <connect from_op="KnowledgeAnalysis" from_port="output 1" to_op="Write Filtered" to_port="input"/>
      <connect from_op="KnowledgeAnalysis" from_port="output 2" to_op="Write Cue" to_port="input"/>
      <connect from_op="KnowledgeAnalysis" from_port="output 3" to_op="Write Cross" to_port="input"/>
      <connect from_op="KnowledgeAnalysis" from_port="output 4" to_op="Write UpSet format" to_port="input"/>
      <connect from_op="Write UpSet format" from_port="through" to_port="result 7"/>
      <connect from_op="Write Cross" from_port="through" to_port="result 3"/>
      <connect from_op="Write Cue" from_port="through" to_port="result 2"/>
      <connect from_op="Write Filtered" from_port="through" to_port="result 1"/>
      <connect from_op="KnowledgeRMAnalyser" from_port="output 1" to_op="Write FCA Matrix (2)" to_port="input"/>
      <connect from_op="Write FCA Matrix (2)" from_port="through" to_op="Multiply (2)" to_port="input"/>
      <connect from_op="Multiply (2)" from_port="output 1" to_op="KnowledgeFCAReverser" to_port="input 1"/>
      <connect from_op="Multiply (2)" from_port="output 2" to_op="KnowledgeFCALister" to_port="input 1"/>
      <connect from_op="Transpose" from_port="example set output" to_op="Data to Similarity" to_port="example set"/>
      <connect from_op="KnowledgeFCAReverser" from_port="output 1" to_op="Write Reversed" to_port="input"/>
      <connect from_op="Write Reversed" from_port="through" to_port="result 5"/>
      <connect from_op="Data to Similarity" from_port="similarity" to_op="Similarity to Data" to_port="similarity"/>
      <connect from_op="Data to Similarity" from_port="example set" to_op="Similarity to Data" to_port="exampleSet"/>
      <connect from_op="Similarity to Data" from_port="exampleSet" to_op="KnowledgeSimilarityMerger" to_port="input 2"/>
      <connect from_op="KnowledgeSimilarityMerger" from_port="output 1" to_op="Write Merged" to_port="input"/>
      <connect from_op="Write Merged" from_port="through" to_port="result 6"/>
      <portSpacing port="source_input 1" spacing="0"/>
      <portSpacing port="sink_result 1" spacing="0"/>
      <portSpacing port="sink_result 2" spacing="0"/>
      <portSpacing port="sink_result 3" spacing="0"/>
      <portSpacing port="sink_result 4" spacing="0"/>
      <portSpacing port="sink_result 5" spacing="0"/>
      <portSpacing port="sink_result 6" spacing="0"/>
      <portSpacing port="sink_result 7" spacing="0"/>
      <portSpacing port="sink_result 8" spacing="0"/>
      <description align="left" color="blue" colored="true" height="236" resized="true" width="287" x="13" y="239">Read the parsed/inherited vocabulary obtained from the Parser/Inheritor modules and use it as inputs for both RMAnalyser, Filter, and similarityMerger to compute an analysis of it</description>
      <description align="center" color="blue" colored="true" height="525" resized="true" width="272" x="24" y="529">Set the macros used by the scripts:&lt;br&gt;- Predicates: list of predicates and predicateTerms used to filter the parsed input, separated by spaces &amp;quot; &amp;quot;&lt;br&gt;- reversedFileDestination: full path of the name of the desidered destination file to be serialized by FCAReverser&lt;br&gt;- mergedFileDestination: full path of the name of the desidered destination file to be serialized by SimilarMerger&lt;br&gt;- comparisonThreshold: &amp;gt;= or &amp;gt; or &amp;lt; or &amp;lt;= or == and then {value}; used to evaluate the similarity range respective to {value}&lt;br&gt;- NameSpace: the base URI for the newly generated terms&lt;br&gt;Then the connection to the scripts is made to ensure that the Set Macros module is executed before the scripts, otherwise there will be errors</description>
      <description align="center" color="yellow" colored="false" height="307" resized="true" width="200" x="385" y="202">Emulate the RapidMiner's processes used to compute the FCA Matrix, generating as output a more complete FCA Matrix&lt;br&gt;Also can filter the triples of the input using the macro Predicates</description>
      <description align="center" color="yellow" colored="false" height="267" resized="false" width="199" x="397" y="690">Filter the triples of the input using the macro Predicates, and pass as output only SubjectTerm as Property and ObjectTerm as Type</description>
      <description align="center" color="purple" colored="true" height="205" resized="true" width="276" x="618" y="270">Store the FCA Matrix with also the URIs from the original input, used as input by both FCAReverser and FCALister</description>
      <description align="center" color="yellow" colored="false" height="282" resized="true" width="411" x="615" y="693">RapidMiner's module used to generate the FCA Matrix and the WordList used to generate the Data Matrix</description>
      <description align="center" color="yellow" colored="false" height="264" resized="true" width="279" x="1216" y="279">RapidMiner's modules to transpose the Data Matrix and calculate the Similarity values for it</description>
      <description align="center" color="yellow" colored="false" height="234" resized="true" width="183" x="933" y="29">Pass from the FCA Matrix to a vocabulary that will be stored as given in input by the macro reversedFileDestination</description>
      <description align="center" color="blue" colored="true" height="171" resized="true" width="185" x="1942" y="17">Store the reversed version of the FCA Matrix generated vocabulary</description>
      <description align="center" color="blue" colored="true" height="171" resized="false" width="185" x="1940" y="1044">Store the FCA Matrix generated by the Rapid Miner's modules</description>
      <description align="center" color="yellow" colored="false" height="234" resized="false" width="183" x="935" y="405">Pass from the FCA Matrix to the Data Matrix as generated by the Rapid Miner's modules</description>
      <description align="center" color="purple" colored="true" height="205" resized="false" width="276" x="1162" y="610">Store the Data Matrix that can be generated by both FCALister and the RapidMiner's modules and use it as input for both the similarity modules and Analysis</description>
      <description align="center" color="yellow" colored="false" height="338" resized="true" width="239" x="1505" y="526">Generate from the Data Matrix:&lt;br&gt;- Filtere: a new version of the Data Matrix, used for the Cue generation&lt;br&gt;- Cue: the different cue values of the input parsed vocabulary&lt;br&gt;- Cross: the list of intersected properties by types&lt;br&gt;- upSetFormat: a matrix used by Intervene to visualize the Data Matrix</description>
      <description align="center" color="yellow" colored="false" height="300" resized="true" width="280" x="1549" y="187">Generate from the parsed vocabulary and the similarity values a new vocabulary with merged elements, that will use the NameSpace definined by the marco nameSpace, selected by the value of the macro comparisonThreshold&lt;br&gt;The new vocabulary will be stored as given in input by the macro mergedFileDestination</description>
      <description align="center" color="blue" colored="true" height="141" resized="true" width="183" x="1939" y="242">Store the merged version of the originale vocabulary</description>
      <description align="center" color="blue" colored="true" height="142" resized="true" width="126" x="1968" y="398">Store the Filtered output of Analysis</description>
      <description align="center" color="blue" colored="true" height="151" resized="true" width="133" x="1973" y="546">Store the Cue output of Analysis</description>
      <description align="center" color="blue" colored="true" height="137" resized="true" width="141" x="1966" y="705">Store the Cross output of Analysis</description>
      <description align="center" color="blue" colored="true" height="194" resized="true" width="270" x="1890" y="847">Store the upSetFormat output of Analysis&lt;br&gt;To use it, go to https://asntech.shinyapps.io/intervene/ and upload it as a Binary Data (0&amp;amp;1), with the separator as chosen in this module</description>
    </process>
  </operator>
</process>
